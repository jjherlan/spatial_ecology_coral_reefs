[
  {
    "objectID": "rpn_sppa.html",
    "href": "rpn_sppa.html",
    "title": "SPPA",
    "section": "",
    "text": "vhu_csv <- read.csv(\"vhu_centroids.csv\")\n\n\nvhu_csv2 <-as_tibble(vhu_csv)\n\n\nvhu_csv2 %>%\n  select(x, y)\n\n# A tibble: 3,131 × 2\n         x     y\n     <dbl> <dbl>\n 1 0.0775   24.7\n 2 0.0588   24.0\n 3 0.0368   23.7\n 4 0.0318   23.4\n 5 0.0501   23.1\n 6 0.0125   22.8\n 7 0.0794   21.7\n 8 0.00269  21.0\n 9 0.0485   20.8\n10 0.0824   16.7\n# … with 3,121 more rows\n\n\n\nvhu_window <- owin(c(0, 10), c(0, 25))\n\n\nattach(vhu_window)\n\n\nvhu_corals <- ppp(vhu_csv2$x, vhu_csv2$y, window = vhu_window)\n\nSummary information\n\nsummary(vhu_corals)\n\nPlanar point pattern:  3131 points\nAverage intensity 12.524 points per square unit\n\nCoordinates are given to 7 decimal places\n\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\n\n\nThis summary shows there are 3131 points (pocilloporid coral colony) and provides the observed, lambda. The density plot can be a helpful visualization of intensity of points across the plot. By plotting the spatial intensity this way, spatial trends in the point occurrences that may violate the assumption of homogeneous point process.\n\ndensity plots\n\nplot(\n  density(vhu_corals)\n)\n\n\n\n\n#alter smoothing parameter\n\nplot(\n  density(vhu_corals, 1)\n)  \n\n\n\n\n#contour plot\ncontour( density(vhu_corals, 1) )\n\n\nWe can also make tallies of counts of point locations based on quadrats overlaid\n\n\non the plot. To determine whether these quadrat counts conform to CSR\n\n\n(i.e., a homogeneous Poisson process), use a simple Chi-Square test statistic.\n\n\nquadrat counts\nvhu_Q <- quadratcount(vhu_corals, nx = 10, ny = 25) #counts in 10 x 25 m quadrats\n#plot\nplot(vhu_corals, cex = 1)\nplot(vhu_Q, add = TRUE, cex = 1)\n#chi-sq test for complete spatial randomness, CSR\nquadrat.test(vhu_corals, nx = 10, ny = 25, method = “Chisq”)\n\n\nThe test statistic suggests highly a non-random point pattern at the\n\n\nscale of the quadrat defined. Note that this test is more akin to a first-order\n\n\npoint pattern analysis because it is based on the dispersion of points among\n\n\nsampling quadrats."
  },
  {
    "objectID": "content/percent_cover.html",
    "href": "content/percent_cover.html",
    "title": "Percent Cover",
    "section": "",
    "text": "From Damgaard & Irvine (2019) Using the beta distribution to analyse plant cover data Journal of Ecology. 107:2747-2759\nWrote a function to create a table from betreg object that can be used in xtable()"
  },
  {
    "objectID": "content/percent_cover.html#methods-for-analyzing-percent-cover-of-pocilloporid-coral",
    "href": "content/percent_cover.html#methods-for-analyzing-percent-cover-of-pocilloporid-coral",
    "title": "Percent Cover",
    "section": "Methods for analyzing percent cover of pocilloporid coral",
    "text": "Methods for analyzing percent cover of pocilloporid coral\nOne option for dealing with the 0 and 1 values is to transform them to be slightly less than one or more than zero. This approach assumes that the data are consistent with a common beta distribution. We fit five models to the data: three variations on the beta model and two linear model approaches.\nA beta regression assuming a common spatial aggregation \\(\\delta\\) or precision parameter (\\(\\phi\\)) (object named: mod.beta1). Notice that \\(\\delta= \\frac{1}{1+\\phi}\\) and \\(\\phi=\\frac{(1-\\delta)}{\\delta}\\). A beta regression assuming each year had a different \\(\\phi\\) parameter (object named: mod.beta2).\nAnother option for modeling the data is to use a zero-one augmented beta model. Currently, the betareg package does not implement this model directly. Therefore, we follow the theoretical results shown in Ospina and Ferrari (2010) that suggest a three-part model can be fit to the data. Basically, we use logistic regression with response an indicator variable for whether or not the plot had zero recorded cover, another logistic regression with response an indicator for whether or not the plot had 100\\(\\%\\) recorded percent cover, and then the beta regression is used to model the continuous percent cover observations ranging from greater than 0 and less than 1.\nOther options based on assuming that the residuals are normally distributed is to use a linear model with a logit-transformed response (object named: mod.lmlogit) or a linear model with response untransformed proportions (object named: mod.lmraw).\nFor comparison this applies a logit-transformation to the empirical proportions and then uses a standard linear regression model.\nRaw data - no transformation\n\np_cover_mod.aov1 <- aov(pland_decimal ~ Site, data = p_cover)\nsummary(p_cover_mod.aov1)\n\n             Df Sum Sq Mean Sq F value Pr(>F)    \nSite          2 12.699   6.350    2203 <2e-16 ***\nResiduals   747  2.153   0.003                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\np_cover_mod.lm1 <- lm(pland_decimal ~ Site, data = p_cover)\nsummary(p_cover_mod.lm1)\n\n\nCall:\nlm(formula = pland_decimal ~ Site, data = p_cover)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.267087 -0.022444 -0.006021  0.015304  0.242376 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.006021   0.003395   1.773  0.07656 .  \nSiteman     0.016423   0.004802   3.420  0.00066 ***\nSitevhu     0.283878   0.004802  59.122  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05368 on 747 degrees of freedom\nMultiple R-squared:  0.8551,    Adjusted R-squared:  0.8547 \nF-statistic:  2203 on 2 and 747 DF,  p-value: < 2.2e-16\n\n\n\npar(mfrow = c(2, 2))\nplot(p_cover_mod.lm1)\n\n\n\n\n\nAnova(p_cover_mod.lm1, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: pland_decimal\n             Sum Sq  Df   F value  Pr(>F)    \n(Intercept)  0.0091   1    3.1451 0.07656 .  \nSite        12.6991   2 2203.2548 < 2e-16 ***\nResiduals    2.1528 747                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOne option to deal with 0 and 100 percent cover is to add and subtract a small amount to those values\n\ntransform01 <- function(x) {\n  (x * (length(x) - 1) + 0.5) / (length(x))\n}\n\n\np_cover$pland_decimal_scaled <- transform01(p_cover$pland_decimal)\n\nLogit-transformation\n\np_cover_mod.lm2 <- lm(logit(pland_decimal_scaled) ~ Site, data = p_cover) \n\n\nsummary(p_cover_mod.lm2)\n\n\nCall:\nlm(formula = logit(pland_decimal_scaled) ~ Site, data = p_cover)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7875 -1.0925 -0.1617  0.7543  4.5854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6.22005    0.09621 -64.654  < 2e-16 ***\nSiteman      0.61521    0.13606   4.522 7.13e-06 ***\nSitevhu      5.27828    0.13606  38.795  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.521 on 747 degrees of freedom\nMultiple R-squared:  0.7067,    Adjusted R-squared:  0.7059 \nF-statistic: 900.1 on 2 and 747 DF,  p-value: < 2.2e-16\n\n\n\npar(mfrow = c(2, 2))\nplot(p_cover_mod.lm2)\n\n\n\n\n\nAnova(p_cover_mod.lm2, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: logit(pland_decimal_scaled)\n            Sum Sq  Df F value    Pr(>F)    \n(Intercept) 9672.3   1 4180.11 < 2.2e-16 ***\nSite        4165.2   2  900.06 < 2.2e-16 ***\nResiduals   1728.5 747                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nGeneralized-linear model\n\np_cover_mod.glm1 <- glm(pland_decimal ~ Site, family = binomial, data = p_cover)\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nsummary(p_cover_mod.glm1)\n\n\nCall:\nglm(formula = pland_decimal ~ Site, family = binomial, data = p_cover)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-0.71274  -0.15856  -0.10518   0.07292   0.94245  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -5.1064     0.8175  -6.246 4.21e-10 ***\nSiteman       1.3324     0.9223   1.445    0.149    \nSitevhu       4.2106     0.8293   5.077 3.83e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 161.289  on 749  degrees of freedom\nResidual deviance:  26.922  on 747  degrees of freedom\nAIC: 191.54\n\nNumber of Fisher Scoring iterations: 8\n\n\n\npar(mfrow = c(2, 2))\nplot(p_cover_mod.glm1)\n\n\n\n\n\nAnova(p_cover_mod.glm1, type = \"III\")\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: pland_decimal\n     LR Chisq Df Pr(>Chisq)    \nSite   134.37  2  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBeta Regression I: \\(\\phi\\) does not vary\n\np_cover_mod.beta1 <- betareg(pland_decimal_scaled ~ Site, data = p_cover, link = c(\"logit\"), link.phi = NULL, type = c(\"ML\"))\n\nBeta Regression II: \\(\\phi\\) does vary (by Site)\n\np_cover_mod.beta2 <- betareg(pland_decimal_scaled ~ Site | Site, data = p_cover, link = c(\"logit\"), link.phi = NULL, type = c(\"ML\"))\n\nExtract AIC from beta regression models\n\np_cover_mod.beta1_aic <- AIC(p_cover_mod.beta1)\np_cover_mod.beta2_aic <- AIC(p_cover_mod.beta2)\n\n\np_cover_mod.beta1_aic\n\n[1] -4179.365\n\np_cover_mod.beta2_aic\n\n[1] -4259.399"
  },
  {
    "objectID": "content/percent_cover.html#interpreting-results-for-pocilloporid-corals",
    "href": "content/percent_cover.html#interpreting-results-for-pocilloporid-corals",
    "title": "Percent Cover",
    "section": "Interpreting results for pocilloporid corals",
    "text": "Interpreting results for pocilloporid corals\nIn order to choose between the beta regression model with a common \\(\\phi\\) versus different \\(\\phi\\), I used AIC but a likelihood ratio or wald test could be used. Using AIC, the model with varying \\(\\phi\\) values had a lower AIC (-1518.568 compared to -1494.524) and therefore more support. We interpret the output from beta regression with the following:\nThe model we fit assumes \\[logit(\\mu_j)=\\beta_0+\\beta_1 Ind_{grp2},\\] where \\(Ind_{grp2}\\) is an indicator for group 2 and \\(j\\) denotes the group membership so \\(j=1\\) or \\(j=2\\). We have \\(logit(\\mu_2)-logit(\\mu_1)=\\beta_1\\), which is equivalent to \\[log(\\frac{\\mu_2}{1-\\mu_2})-log(\\frac{\\mu_1}{1-\\mu_1})= \\beta_1.\\]\nThe \\(\\frac{\\mu_j}{1-\\mu_j}\\) is interpreted as the odds of proportion cover in group \\(j\\). Therefore,\n\\[log(\\frac{\\mu_2}{1-\\mu_2}/\\frac{\\mu_1}{1-\\mu_1})=\\beta_1\\] is the log- odds ratio of cover in group 2 compared to group 1, \\[(\\frac{\\mu_2}{1-\\mu_2}/\\frac{\\mu_1}{1-\\mu_1})=exp(\\beta_1).\\]\nThen \\(exp(\\beta_1)\\) is the factor increase/decrease in odds of proportion cover for group 2 compared to group 1, where \\(exp(\\beta_1)>1\\) is an increase and \\(exp(\\beta_1)<1\\) is a decrease, and \\(exp(\\beta_1) \\approx 1\\) means essentially no change.\nBeta Regression I: \\(\\phi\\) does not vary\n\n\n\n\n\n\nResults from using betareg package in R by transforming the 0 and 1’s. These are on the logit-scale for \\(\\mu\\) with a common \\(\\phi\\) parameter\n\n\n\n\n\nEstimate\n\n\nStd. Error\n\n\nz value\n\n\nPr(>|z|)\n\n\n\n\n(Intercept)\n\n\n-4.283\n\n\n0.074\n\n\n-57.953\n\n\n0.000\n\n\n\n\nSiteman\n\n\n0.205\n\n\n0.084\n\n\n2.433\n\n\n0.015\n\n\n\n\nSitevhu\n\n\n3.384\n\n\n0.077\n\n\n43.754\n\n\n0.000\n\n\n\n\n(phi)\n\n\n25.321\n\n\n1.561\n\n\n16.221\n\n\n0.000\n\n\n\n\nBeta Regression II: \\(\\phi\\) does vary (by Site)\n\np_cover_mod.beta2 <- betareg(pland_decimal_scaled ~ Site | Site, data = p_cover, link = c(\"logit\"), link.phi = NULL, type = c(\"ML\"))\n\n\nsummary(p_cover_mod.beta2)\n\n\nCall:\nbetareg(formula = pland_decimal_scaled ~ Site | Site, data = p_cover, \n    link = c(\"logit\"), link.phi = NULL, type = c(\"ML\"))\n\nStandardized weighted residuals 2:\n    Min      1Q  Median      3Q     Max \n-6.1960 -0.5649 -0.3417  0.7115  1.8425 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -4.99928    0.08784 -56.913   <2e-16 ***\nSiteman      1.26338    0.13630   9.269   <2e-16 ***\nSitevhu      4.09928    0.09194  44.584   <2e-16 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   4.3427     0.1141  38.052  < 2e-16 ***\nSiteman      -1.6249     0.1679  -9.680  < 2e-16 ***\nSitevhu      -1.1097     0.1441  -7.699 1.37e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  2136 on 6 Df\nPseudo R-squared: 0.6791\nNumber of iterations: 22 (BFGS) + 1 (Fisher scoring) \n\n\nThe function, Fcn.CreateSummary.betareg, doesn’t work with phi varying, so just fixed labels manually.\n\np_cover_mod.glm1 <- glm(pland_decimal ~ Site, family = binomial, data = p_cover)\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nprint(xtable(summary(p_cover_mod.glm1), digits = 3, caption = \" . \"), type = \"html\")\n\n<!-- html table generated in R 4.2.2 by xtable 1.8-4 package -->\n<!-- Sun Mar 12 17:38:01 2023 -->\n<table border=1>\n<caption align=\"bottom\">  .  </caption>\n<tr> <th>  </th> <th> Estimate </th> <th> Std. Error </th> <th> z value </th> <th> Pr(&gt;|z|) </th>  </tr>\n  <tr> <td align=\"right\"> (Intercept) </td> <td align=\"right\"> -5.106 </td> <td align=\"right\"> 0.818 </td> <td align=\"right\"> -6.246 </td> <td align=\"right\"> 0.000 </td> </tr>\n  <tr> <td align=\"right\"> Siteman </td> <td align=\"right\"> 1.332 </td> <td align=\"right\"> 0.922 </td> <td align=\"right\"> 1.445 </td> <td align=\"right\"> 0.149 </td> </tr>\n  <tr> <td align=\"right\"> Sitevhu </td> <td align=\"right\"> 4.211 </td> <td align=\"right\"> 0.829 </td> <td align=\"right\"> 5.077 </td> <td align=\"right\"> 0.000 </td> </tr>\n   </table>\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nz value\nPr(>|z|)\n\n\n\n\n(Intercept)\n-4.129\n0.822\n-5.025\n0.000\n\n\nSiteman\n1.225\n0.929\n1.318\n0.187\n\n\nSitevhu\n3.233\n0.833\n3.880\n0.000\n\n\n\n\n\n\n\n\nExample diagnostic plots for beta regression)\n\n\n\n\n\n\n\n\n\nExample diagnostic plots for beta regression, variable phi)\n\n\n\n\nBuilt-in diagnostic plots for linear regression models\nLogit-transformed\n\npar(mfrow = c(3, 2), pty = 'm')\nplot(p_cover_mod.lm2)\nplot(cooks.distance(p_cover_mod.lm2))\n\n\n\n\nUntransformed\n\npar(mfrow = c(3, 2), pty = 'm')\nplot(p_cover_mod.lm1)\nplot(cooks.distance(p_cover_mod.lm1))\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of default qqplots for beta and linear models. Beta models use weighted residuals 2 recommended in and linear model are the response residuals.\n\n\n\n\n\npar(mfrow = c(2,1), pty = 'm', cex = 1)\n\nplot(p_cover_mod.beta2, which = 5, type = \"sweighted2\", main = \"\")\nplot(p_cover_mod.lm2, which = 2, main = \"\")\n\n\n\n\n\nClassical Analysis\nFrom Douma & Weedon (2019)\n\n\np_cover.aov1 <- aov(pland_decimal ~ Site, data = p_cover)\nsummary(p_cover.aov1)\n##              Df Sum Sq Mean Sq F value Pr(>F)    \n## Site          2 12.699   6.350    2203 <2e-16 ***\n## Residuals   747  2.153   0.003                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\np_cover.aov2 <- aov(pland_decimal ~ Site + Error(plot_id), data = p_cover)\nsummary(p_cover.aov2)\n## \n## Error: plot_id\n##            Df Sum Sq Mean Sq F value Pr(>F)\n## Residuals 249 0.7794 0.00313               \n## \n## Error: Within\n##            Df Sum Sq Mean Sq F value Pr(>F)    \n## Site        2 12.699   6.350    2302 <2e-16 ***\n## Residuals 498  1.373   0.003                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA very similar analysis can be conducted using the library nlme for mixed effects modeling.\n\np_cover.lme1 <- lme(pland_decimal ~ Site, random = ~ 1 | plot_id, data = p_cover)\n\n\nanova(p_cover.lme1)\n\n            numDF denDF  F-value p-value\n(Intercept)     1   498 2698.541  <.0001\nSite            2   498 2302.345  <.0001\n\n\n\np_cover.lme_null <- lme(pland_decimal ~ 1, random = ~ 1 | plot_id, data = p_cover)\n\n\nlmtest::lrtest(p_cover.lme1, p_cover.lme_null)\n\nLikelihood ratio test\n\nModel 1: pland_decimal ~ Site\nModel 2: pland_decimal ~ 1\n  #Df  LogLik Df Chisq Pr(>Chisq)    \n1   5 1117.16                        \n2   3  402.18 -2  1430  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nBeta regression with no variable precision \\(\\phi\\)\nWe now turn to the beta regression model, that models the response variable as being generated from a beta distribution (i.e. that is bounded at 0 and 1).\nThe observations of percent cover are based on replicate quadrats replicated within three experimental plots.\nWe begin by attempting to fit a beta regression model, with pland_decimal as the response, and Site as the categorical predictor.\n\np_cover.bm1 <- betareg(pland_decimal_scaled ~ Site, data = p_cover)\n\nbetareg will not accept values of 0 and 1 in the response variable.\nThere are two possible solutions here, rescaling the data to remove 0s and 1s, or fitting zero-inflated models. We start here with the rescaling solution. A suggested rescaling equation is:\n\\[ x^*_{i} = \\frac{x_i(n-1)+0.5}{n} \\]\nWhere \\(x^*_i\\) is the transformation of \\(x_i\\) and \\(n\\) is the total number of observations in the dataset.\nFor convenience we define this as a custom function tranform01 and apply it to the dataset:\n\ntransform01 <- function(x) {\n  (x * (length(x) - 1) + 0.5) / (length(x))\n}\n\nWith this scaled data we can now successfully fit the model. And test its significance relative to a null model that assumes no effect of wave power on percent cover of pocilloporid corals. For reference we also fit a classical ANOVA model assuming normally distributed errors using lm.\n\np_cover.bmnull <- betareg(pland_decimal_scaled ~ 1, data = p_cover)\nsummary(p_cover.bmnull)\n\n\nCall:\nbetareg(formula = pland_decimal_scaled ~ 1, data = p_cover)\n\nStandardized weighted residuals 2:\n    Min      1Q  Median      3Q     Max \n-0.8695 -0.8695  0.0151  0.8905  1.1783 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -2.18925    0.05919  -36.99   <2e-16 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(>|z|)    \n(phi)   3.0204     0.1916   15.76   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  1511 on 2 Df\nNumber of iterations: 15 (BFGS) + 1 (Fisher scoring) \n\n\n\np_cover.lm1 <- lm(pland_decimal_scaled ~ Site, data = p_cover)\nsummary(p_cover.lm1)\n\n\nCall:\nlm(formula = pland_decimal_scaled ~ Site, data = p_cover)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.266731 -0.022414 -0.006013  0.015284  0.242053 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.006680   0.003391    1.97  0.04920 *  \nSiteman     0.016401   0.004795    3.42  0.00066 ***\nSitevhu     0.283499   0.004795   59.12  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05361 on 747 degrees of freedom\nMultiple R-squared:  0.8551,    Adjusted R-squared:  0.8547 \nF-statistic:  2203 on 2 and 747 DF,  p-value: < 2.2e-16\n\n\n\nlmtest::lrtest(p_cover.bm1, p_cover.bmnull)\n\nLikelihood ratio test\n\nModel 1: pland_decimal_scaled ~ Site\nModel 2: pland_decimal_scaled ~ 1\n  #Df LogLik Df  Chisq Pr(>Chisq)    \n1   4 2093.7                         \n2   2 1511.0 -2 1165.3  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nAIC(p_cover.bm1, p_cover.lm1, p_cover.bmnull)\n\n               df       AIC\np_cover.bm1     4 -4179.365\np_cover.lm1     4 -2255.582\np_cover.bmnull  2 -3018.031\n\n\nAccording to the likelihood-ratio test there is a significant difference betwen the null model and the treatment model. The AIC analysis supports this conclusion, but also highlights the improved model fit with beta regression relative to normal ANOVA (lm1). From this initial analysis we would tentatively conclude that using beta regresson improves our ability to model the algal cover, but that there is no effect of grazer manipulation treatment.\nIt is useful to plot the predictions derived from the model and compare them to the observed data. First we define two new functions to allow us to use the dbeta and rbeta functions with the \\(\\mu\\) and \\(\\phi\\) parameterization.\n\n\ndbeta2 <- function(X, mu, phi, ...) {\n  dbeta(X, shape1 = mu * phi, shape2 = (1 - mu) * phi, ...)\n}\n\nrbeta2 <- function(N, mu, phi, ...) {\n  rbeta(N, shape1 = mu * phi, shape2 = (1 - mu) * phi, ...)\n}\n\n\nWith this function we can plot the distributions corresponding to the MLE parameters for each treatment :\n\n\n# extract coefficients of beta regression model\ncoefs.bm1 <- coef(p_cover.bm1)\n\n# create vector spanning the transformed 0-1 interval\n\nn.bm2 <- length(fitted(p_cover.bm1))\nx.range <- seq(0.5/n.bm2 , 1-0.5/n.bm2 , length.out = 200)\nx.range.bt <- (x.range*n.bm2 - 0.5)/(n.bm2-1)\n\n\n# Anakena\nplot(x.range.bt, dbeta2(x.range, inv.logit(coefs.bm1[\"(Intercept)\"]), coefs.bm1[\"(phi)\"]),\n     type = \"l\", lty = 2, lwd = 2,\n     ylab = \"Probability density\", xlab = \"Proportion cover\",\n     ylim=c(0, 10)\n)\n\n# Manavai\nlines(x.range.bt, dbeta2(x.range, inv.logit(coefs.bm1[\"(Intercept)\"] + coefs.bm1[2]), coefs.bm1[\"(phi)\"]),lwd = 2, col = \"red\")\n\n# Vaihu\nlines(x.range.bt, dbeta2(x.range, inv.logit(coefs.bm1[\"(Intercept)\"] + coefs.bm1[3]), coefs.bm1[\"(phi)\"]), col = \"blue\", lwd = 2)\n\nrug(p_cover$pland_decimal[p_cover$Site == \"ana\"], lwd = 1.5, pos = 10)\n\nrug(p_cover$pland_decimal[p_cover$Site == \"man\"],col=\"red\", pos = 9.75, side = 3,lwd=1.5)\n\nrug(p_cover$pland_decimal[p_cover$Site == \"vhu\"], col=\"blue\", pos = 9.5, side = 3, lwd=1.5)\n\nlegend(\"topright\", lwd = 2, lty = c(2, 1, 1, 1), col = c(\"black\", \"red\", \"blue\"), legend = c(\"Anakena\", \"Manavai\", \"Vaihu\"), bty = \"n\")\n\n\n\n\n\nWe have added the original observations as ticks of the appropriate colour, and back-transformed the densities to allow fair visual comparisons between the fitted distributions and the original data using:\n\\[ x_{i} = \\frac{x^*_in-0.5}{(n-1)} \\]\nNote that the vertical positioning of the dots is merely to prevent overplotting.\nFrom this plot we can see that the model does a reasonable job of fitting beta distributions to each of the treatment levels… And that likewise, the variance of the groups …. This can be confirmed with a residual plot, using residuals calculated relative to their predicted variance.\n\nplot(resid(p_cover.bm1) ~ fitted(p_cover.bm1))\n\n\n\n\nIs this statement accurate?\nThe spread of the standardized residuals is strongly related to the fitted values, suggesting that variance is not being adequately modeled. This observation suggests the possible utility of allowing for the precision parameter \\(\\phi\\) to vary between treatment groups. The following section will show extension of the beta regression model to allow for this.\n\n\nVariable precision \\(\\phi\\)\nWe can repeat the above analysis using a model that allows \\(\\phi\\) to vary with predictors. This is achieved by adding a second part to the right hand side of the formula, separated with the | symbol. All covariates to the right of this | symbol will be used to model \\(\\phi\\). Note that they do not have to be the same covariates used to model \\(\\mu\\) (specified to the left of the |).\n\np_cover.bm2 <- betareg(pland_decimal_scaled ~ Site | Site, data = p_cover)\n\nsummary(p_cover.bm2)\n\n\nCall:\nbetareg(formula = pland_decimal_scaled ~ Site | Site, data = p_cover)\n\nStandardized weighted residuals 2:\n    Min      1Q  Median      3Q     Max \n-6.1960 -0.5649 -0.3417  0.7115  1.8425 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -4.99928    0.08784 -56.913   <2e-16 ***\nSiteman      1.26338    0.13630   9.269   <2e-16 ***\nSitevhu      4.09928    0.09194  44.584   <2e-16 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   4.3427     0.1141  38.052  < 2e-16 ***\nSiteman      -1.6249     0.1679  -9.680  < 2e-16 ***\nSitevhu      -1.1097     0.1441  -7.699 1.37e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  2136 on 6 Df\nPseudo R-squared: 0.6791\nNumber of iterations: 22 (BFGS) + 1 (Fisher scoring) \n\n\nFrom the Coefficients table we see that the estimate for \\(\\mu\\) in the Control treatment is inv.logit(-4.1313) = X = X% coral cover. Moreover, the estimates of \\(\\mu\\) for the other two Sites (treatments) are each significantly higher.\nFrom the Phi coefficients table we can see that the maximum likelihood estimate of the precision is highest in the Control treatment and is reduced significantly relative to this baseline in each of other treatment groups. In other words, the model fit confirms our impression from the previous two graphs that a fixed \\(\\phi\\) model overestimates variance in the Control treatment, and underestimates it in the other three treatments.\nWe can use likelihood-ratio tests to compare the new model to the fixed-\\(\\phi\\) and null models.\n\nlmtest::lrtest(p_cover.bm1, p_cover.bm2)\n\nLikelihood ratio test\n\nModel 1: pland_decimal_scaled ~ Site\nModel 2: pland_decimal_scaled ~ Site | Site\n  #Df LogLik Df  Chisq Pr(>Chisq)    \n1   4 2093.7                         \n2   6 2135.7  2 84.034  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlmtest::lrtest(p_cover.bmnull, p_cover.bm1, p_cover.bm2)\n\nLikelihood ratio test\n\nModel 1: pland_decimal_scaled ~ 1\nModel 2: pland_decimal_scaled ~ Site\nModel 3: pland_decimal_scaled ~ Site | Site\n  #Df LogLik Df    Chisq Pr(>Chisq)    \n1   2 1511.0                           \n2   4 2093.7  2 1165.334  < 2.2e-16 ***\n3   6 2135.7  2   84.034  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe likelihood ratio tests indicate that the model with varying \\(\\phi\\) is significantly better than both the previous fixed \\(\\phi\\) model, and the null model. In this case the conclusion is that including the model for \\(\\phi\\) led to a better fitting model than both the fixed \\(\\phi\\) model and the null model.\nIt is possible to apply post-hoc tests to identify which pariwise contrasts of treatments levels are significant.\n\ntest(pairs(emmeans(p_cover.bm2, ~ Site, mode = \"link\")))\n\n contrast  estimate     SE  df z.ratio p.value\n ana - man    -1.26 0.1363 Inf  -9.269  <.0001\n ana - vhu    -4.10 0.0919 Inf -44.584  <.0001\n man - vhu    -2.84 0.1077 Inf -26.331  <.0001\n\nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nWe conclude from this analysis that each Site treatment is significantly different to each other. This is an important point: correct modeling of \\(\\phi\\) can often be important for accurate inference on \\(\\mu\\).\nResidual plots confirm our conclusion that p_cover.bm2 provides a better fit to the observed data than p_cover.bm1. This is seen by the more even spread of residuals in the second plot below.\n\n\npar(mfrow = c(2, 1), oma = c(0, 0, 0, 0), mar = c(4, 4, 0.2, 0.2))\nplot(residuals(p_cover.bm1) ~ fitted(p_cover.bm1))\nplot(residuals(p_cover.bm2) ~ fitted(p_cover.bm2))\n\n\n\n\n\nAs above we can plot the MLE distributions for each of the treatments, based on the variable \\(\\phi\\) model:\n\n\n# plot distributions\nmuphi.bm2 <- unique(data.frame(\n  mu = fitted(p_cover.bm2),\n  phi = predict(p_cover.bm2, type = \"precision\"),\n  treatment = p_cover$Site\n))\n\n\n\n\nplot(x.range.bt , dbeta2(x.range, muphi.bm2[1, 1], muphi.bm2[1, 2]),\n     type=\"l\",\n     xlab = \"Proportion cover\", ylab = \"Probability density\",\n     lty = 2, lwd = 2)\n\nfor (i in 2:3) {\n  lines(x.range.bt, dbeta2(x.range, muphi.bm2[i, 1], muphi.bm2[i, 2]), col = c(\"black\", \"red\", \"blue\")[i], lty = 1, lwd = 2)\n}\n\nlegend(\"topright\", lwd = 2, lty = c(2, 1, 1, 1), col = c(\"black\", \"red\", \"blue\"), legend = c(\"Anakena\", \"Manavai\", \"Vaihu\"), bty = \"n\")\n\n\n\n\n\nDue to the much narrower variance of the Control treatment group in this model, the probability density plots of the other treatments are rather distorted. The graph below rescales the Y axis for comparison to the fixed \\(\\phi\\) model above.\n\n\nplot(x.range.bt , dbeta2(x.range, muphi.bm2[1, \"mu\"], muphi.bm2[1, \"phi\"]),\n     type=\"l\",\n     xlab = \"Proportion cover\", ylab = \"Probability density\",\n     lty = 2, lwd = 2, ylim = c(0,10))\n\nfor (i in 2:3) {\n  lines(x.range.bt, dbeta2(x.range, muphi.bm2[i, \"mu\"], muphi.bm2[i, \"phi\"]), col = c(\"black\", \"red\", \"blue\")[i], lty = 1, lwd = 2)\n}\n  \nlegend(\"topright\", lwd = 2, lty = c(2, 1, 1, 1), col = c(\"black\", \"red\", \"blue\"), legend = c(\"Anakena\", \"Manavai\", \"Vaihu\"), bty = \"n\")\n\nrug(p_cover$pland_decimal[p_cover$Site == \"ana\"], lwd = 1.5, pos = 10)\n\nrug(p_cover$pland_decimal[p_cover$Site == \"man\"], col = \"red\", pos = 9.75, side = 3,lwd = 1.5)\n\nrug(p_cover$pland_decimal[p_cover$Site == \"vhu\"], col = \"blue\", pos = 9.5, side = 3, lwd = 1.5)\n\n\n\n\n\nThese plots support the conclusion from the likelihood ratio tests above. The best-fit distributions from the variable \\(\\phi\\) model better match the observed differences in dispersion between the different groups.\n\npoci_cover <-\np_cover %>%\n  as_tibble() %>%\n#  mutate(size_cm = area*10000) %>%\n  group_by(Site) %>%\n  dplyr::summarize(mean = mean(pland_decimal), \n                   sd = sd(pland_decimal), \n                   n = n(),\n                   se = sd/sqrt(n)\n  ) %>%\n  mutate(se = sd / sqrt(n),\n         lower.ci = mean - qt(1 - (0.05 / 2), n - 1) * se,\n         upper.ci = mean + qt(1 - (0.05 / 2), n - 1) * se) %>%\n  mutate_at(vars(Site), factor) %>%\n  add_column(\n          location = c('Anakena', 'Manavai', 'Southeast')\n          ) %>%\n  mutate_at(vars(location), factor)\n\n\npoci_cover2 <- \npoci_cover %>%\n  add_column(\n          cld = c('a', 'b', 'c')\n          ) %>%\nmutate_at(vars(cld), factor)\n\n\npoci_cover2\n\n# A tibble: 3 × 9\n  Site     mean     sd     n       se lower.ci upper.ci location  cld  \n  <fct>   <dbl>  <dbl> <int>    <dbl>    <dbl>    <dbl> <fct>     <fct>\n1 ana   0.00602 0.0110   250 0.000696  0.00465  0.00739 Anakena   a    \n2 man   0.0224  0.0423   250 0.00268   0.0172   0.0277  Manavai   b    \n3 vhu   0.290   0.0821   250 0.00519   0.280    0.300   Southeast c    \n\n\n\nx_labels = c(\"Anakena\", \"Manavai\", \"Vaihu\")\n# label_names = c(\"8 m\" = \"8 m\", \"15 m\" = \"15 m\", \"25 m\" = \"25 m\")\n\n\npoci_cover.ggbarplot <- ggplot(poci_cover2, aes(x = location, y = mean)) +   \n  geom_bar(stat = \"identity\", width = 0.75, color = \"black\", fill = \"#333399\", size = 0.50, alpha = 0.6) +\n  geom_linerange(aes(ymin = lower.ci, ymax = upper.ci), size = 0.75) +\n  scale_y_continuous(expression(paste(\"Mean Percent Cover (%)\")), limits = c(0, 1)) + \n  scale_x_discrete(expand = c(0, 1), labels = x_labels) + \n#  scale_fill_manual(breaks = c(\"North\", \"West\",\n#                               \"Southeast\"),\n#                    values = c(\"red\", \"blue\", \n#                                    \"green\"), \n#                                    labels = c(\"North\", \"West\",\n#                                               \"Southeast\")) +\n#  facet_wrap( ~ depth2, labeller = as_labeller(label_names), dir = \"v\", ncol = 1) + \n  ggtitle(expression(paste(italic(\" Pocillopora \"), \"spp.\"))) +\n  geom_text(aes(label = cld, y = upper.ci), vjust = -0.5) +\n  #scale_y_log10(expression(paste(\"Colony Size (\", cm^2, \")\"), limits = c(0, 100000))) +\n  labs(x = NULL) +\n  theme(strip.text = element_text(size = 10, color = \"black\", hjust = 0.50),\n        strip.background = element_rect(fill = \"#FFFFFF\", color = NA),    \n        panel.background = element_rect(fill = \"#FFFFFF\", color = NA),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_line(color = \"#b2b2b2\"),\n        panel.spacing.x = unit(1, \"cm\"),\n        panel.spacing.y = unit(0.5, \"cm\"),\n        panel.spacing = unit(1, \"lines\"),\n        axis.ticks = element_blank(),\n        legend.position = \"top\",\n        plot.title = element_text(size = 11),\n        axis.title.y = element_text(size = 11),\n        legend.title = element_blank())\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\npoci_cover.ggbarplot"
  },
  {
    "objectID": "content/publishing.html",
    "href": "content/publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "To get your Quarto webpage to show up with the url\nyou have a few steps."
  },
  {
    "objectID": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "href": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "title": "Publishing",
    "section": "Turn on GitHub Pages for your repo",
    "text": "Turn on GitHub Pages for your repo\n\nTurn on GitHub Pages under Settings > Pages . You will set pages to be made from the gh-pages branch and the root directory.\nTurn on GitHub Actions under Settings > Actions > General\n\nThe GitHub Action will automatically recreate your website when you push to GitHub after you do the initial gh-pages set-up"
  },
  {
    "objectID": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "href": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "title": "Publishing",
    "section": "Do your first publish to gh-pages",
    "text": "Do your first publish to gh-pages\nThe first time you publish to gh-pages, you need to do so locally.\n\nOn your local computer, open a terminal window and cd to your repo directory. Here is what that cd command looks like for me. You command will look different because your local repo will be somewhere else on your computer.\n\ncd ~/Documents/GitHub/NOAA-quarto-simple\n\nPublish to the gh-pages. In the terminal type\n\nquarto publish gh-pages\nThis is going to render your webpage and then push the _site contents to the gh-pages branch."
  },
  {
    "objectID": "content/publishing.html#dont-like-using-gh-pages",
    "href": "content/publishing.html#dont-like-using-gh-pages",
    "title": "Publishing",
    "section": "Don’t like using gh-pages?",
    "text": "Don’t like using gh-pages?\nIn some cases, you don’t want your website on the gh-pages branch. For example, if you are creating releases and you want the website pages archived in that release, then you won’t want your website pages on the gh-pages branch.\nHere are the changes you need to make if you to avoid gh-pages branch.\n\nAt the top of _quarto.yml add the following:\n\nproject: \n  type: website\n  output-dir: docs\n\nOn GitHub under Settings > Pages set pages to be made from the main branch and the docs directory.\nMake sure docs is not listed in .gitignore\nPublish the site the first time locally using quarto publish from the terminal\nChange the GitHub Action because you can’t use quarto publish gh-pages. You’ll need to push to the main branch yourself (in the GitHub Action)\n\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2 \n        \n      - name: Set up R (needed for Rmd)\n        uses: r-lib/actions/setup-r@v2\n\n      - name: Install packages (needed for Rmd)\n        run: Rscript -e 'install.packages(c(\"rmarkdown\", \"knitr\", \"jsonlite\"))'\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          # tinytex: true \n          # uncomment below and fill to pin a version\n          # version: 0.9.600\n      \n      - name: Render Quarto Project\n        uses: quarto-dev/quarto-actions/render@v2\n        with:\n          to: html\n\n      - name: Set up Git\n        run: |\n          git config --local user.email \"actions@github.com\"\n          git config --local user.name \"GitHub Actions\"\n\n      - name: Commit all changes and push\n        run: |\n          git add -A && git commit -m 'Build site' || echo \"No changes to commit\"\n          git push origin || echo \"No changes to commit\""
  },
  {
    "objectID": "content/rpn_sppa.html",
    "href": "content/rpn_sppa.html",
    "title": "SPPA",
    "section": "",
    "text": "vhu_csv <- read.csv(\"vhu_centroids.csv\")\n\n\nvhu_csv2 <- as_tibble(vhu_csv)\n\n\nvhu_csv2 \n\n# A tibble: 3,131 × 6\n   layer level class    id       x     y\n   <int> <chr> <int> <int>   <dbl> <dbl>\n 1     1 patch     0     1 0.0775   24.7\n 2     1 patch     0     2 0.0588   24.0\n 3     1 patch     0     3 0.0368   23.7\n 4     1 patch     0     4 0.0318   23.4\n 5     1 patch     0     5 0.0501   23.1\n 6     1 patch     0     6 0.0125   22.8\n 7     1 patch     0     7 0.0794   21.7\n 8     1 patch     0     8 0.00269  21.0\n 9     1 patch     0     9 0.0485   20.8\n10     1 patch     0    10 0.0824   16.7\n# … with 3,121 more rows\n\n\nDefine window for spatial pattern analyses based on the extent defined above\n\nvhu_window <- owin(c(0, 10), c(0, 25))\n\n\nattach(vhu_window)\n\n\nvhu_corals <- ppp(vhu_csv2$x, vhu_csv2$y, window = vhu_window)\n\nSummary information\n\nsummary(vhu_corals)\n\nPlanar point pattern:  3131 points\nAverage intensity 12.524 points per square unit\n\nCoordinates are given to 7 decimal places\n\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\n\n\nThis summary shows there are 3131 points (pocilloporid coral colony) and provides the observed, lambda. The density plot can be a helpful visualization of intensity of points across the plot. By plotting the spatial intensity this way, spatial trends in the point occurrences that may violate the assumption of homogeneous point process.\nDensity plots\n\nplot(\n  density(vhu_corals)\n)\n\n\n\n\nAlter smoothing parameter\n\nplot(\n  density(vhu_corals, 1)\n)  \n\n\n\n\nContour plot\n\ncontour(\n  density(vhu_corals, 1)\n)\n\n\n\n\nWe can also make tallies of counts of point locations based on quadrats overlaid on the plot. To determine whether these quadrat counts conform to CSR (i.e., a homogeneous Poisson process), use a simple Chi-Square test statistic.\nQuadrat counts\nCounts in 10 x 25 m quadrats\n\nvhu_Q <- quadratcount(vhu_corals, nx = 10, ny = 25)\n\nPlot\n\nplot(vhu_corals, cex = 1)\nplot(vhu_Q, add = TRUE, cex = 1)\n\n\n\n\nChi-sq test for complete spatial randomness, CSR\n\nquadrat.test(vhu_corals, nx = 10, ny = 25, method = \"Chisq\")\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  vhu_corals\nX2 = 308.08, df = 249, p-value = 0.0127\nalternative hypothesis: two.sided\n\nQuadrats: 10 by 25 grid of tiles\n\n\nThe test statistic suggests highly a non-random point pattern at the scale of the quadrat defined. Note that this test is more akin to a first-order point pattern analysis because it is based on the dispersion of points among sampling quadrats.\n\n\n\n\nman_csv <- read.csv(\"man_centroids.csv\")\n\n\nman_csv2 <- as_tibble(man_csv)\n\n\nman_csv2 \n\n# A tibble: 372 × 6\n   layer level class    id       x     y\n   <int> <chr> <int> <int>   <dbl> <dbl>\n 1     1 patch     0     1 0.0579  20.6 \n 2     1 patch     0     2 0.0812  20.5 \n 3     1 patch     0     3 0.00173 19.4 \n 4     1 patch     0     4 0.0281  12.7 \n 5     1 patch     0     5 0.0164  12.6 \n 6     1 patch     0     6 0.0550  10.8 \n 7     1 patch     0     7 0.0111   8.88\n 8     1 patch     0     8 0.0366   8.51\n 9     1 patch     0     9 0.0589  13.3 \n10     1 patch     0    10 0.0825  13.0 \n# … with 362 more rows\n\n\nDefine window for spatial pattern analyses based on the extent defined above\n\nman_window <- owin(c(0, 10), c(0, 25))\n\n\nattach(man_window)\n\nThe following objects are masked from vhu_window:\n\n    type, units, xrange, yrange\n\n\n\nman_corals <- ppp(man_csv2$x, man_csv2$y, window = man_window)\n\nSummary information\n\nsummary(man_corals)\n\nPlanar point pattern:  372 points\nAverage intensity 1.488 points per square unit\n\nCoordinates are given to 7 decimal places\n\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\n\n\nThis summary shows there are 3131 points (pocilloporid coral colony) and provides the observed, lambda. The density plot can be a helpful visualization of intensity of points across the plot. By plotting the spatial intensity this way, spatial trends in the point occurrences that may violate the assumption of homogeneous point process.\nDensity plots\n\nplot(\n  density(man_corals)\n)\n\n\n\n\nAlter smoothing parameter\n\nplot(\n  density(man_corals, 1)\n)  \n\n\n\n\nContour plot\n\ncontour(\n  density(man_corals, 1)\n)\n\n\n\n\nWe can also make tallies of counts of point locations based on quadrats overlaid on the plot. To determine whether these quadrat counts conform to CSR (i.e., a homogeneous Poisson process), use a simple Chi-Square test statistic.\nQuadrat counts\nCounts in 10 x 25 m quadrats\n\nman_Q <- quadratcount(man_corals, nx = 10, ny = 25)\n\nPlot\n\nplot(man_corals, cex = 1)\nplot(man_Q, add = TRUE, cex = 1)\n\n\n\n\nChi-sq test for complete spatial randomness, CSR\n\nquadrat.test(man_corals, nx = 10, ny = 25, method = \"Chisq\")\n\nWarning: Some expected counts are small; chi^2 approximation may be inaccurate\n\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  man_corals\nX2 = 1394.1, df = 249, p-value < 2.2e-16\nalternative hypothesis: two.sided\n\nQuadrats: 10 by 25 grid of tiles\n\n\n\n\n\n\nana_csv <- read.csv(\"ana_centroids.csv\")\n\n\nana_csv2 <- as_tibble(ana_csv)\n\n\nana_csv2 \n\n# A tibble: 136 × 6\n   layer level class    id     x      y\n   <int> <chr> <int> <int> <dbl>  <dbl>\n 1     1 patch     1     1 0.330 23.4  \n 2     1 patch     1     2 0.343 23.7  \n 3     1 patch     1     3 0.360  1.54 \n 4     1 patch     1     4 0.477 11.2  \n 5     1 patch     1     5 0.494 12.6  \n 6     1 patch     1     6 0.547 17.4  \n 7     1 patch     1     7 0.557  1.52 \n 8     1 patch     1     8 0.587 16.3  \n 9     1 patch     1     9 0.779  1.81 \n10     1 patch     1    10 0.749  0.270\n# … with 126 more rows\n\n\nDefine window for spatial pattern analyses based on the extent defined above\n\nana_window <- owin(c(0, 10), c(0, 25))\n\n\nattach(ana_window)\n\nThe following objects are masked from man_window:\n\n    type, units, xrange, yrange\n\n\nThe following objects are masked from vhu_window:\n\n    type, units, xrange, yrange\n\n\n\nana_corals <- ppp(ana_csv2$x, ana_csv2$y, window = ana_window)\n\nSummary information\n\nsummary(ana_corals)\n\nPlanar point pattern:  136 points\nAverage intensity 0.544 points per square unit\n\nCoordinates are given to 7 decimal places\n\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\n\n\nThis summary shows there are 3131 points (pocilloporid coral colony) and provides the observed, lambda. The density plot can be a helpful visualization of intensity of points across the plot. By plotting the spatial intensity this way, spatial trends in the point occurrences that may violate the assumption of homogeneous point process.\nDensity plots\n\nplot(\n  density(ana_corals)\n)\n\n\n\n\nAlter smoothing parameter\n\nplot(\n  density(ana_corals, 1)\n)  \n\n\n\n\nContour plot\n\ncontour(\n  density(ana_corals, 1)\n)\n\n\n\n\nWe can also make tallies of counts of point locations based on quadrats overlaid on the plot. To determine whether these quadrat counts conform to CSR (i.e., a homogeneous Poisson process), use a simple Chi-Square test statistic.\nQuadrat counts\nCounts in 10 x 25 m quadrats\n\nana_Q <- quadratcount(ana_corals, nx = 10, ny = 25)\n\nPlot\n\nplot(ana_corals, cex = 1)\nplot(ana_Q, add = TRUE, cex = 1)\n\n\n\n\nChi-sq test for complete spatial randomness, CSR\n\nquadrat.test(ana_corals, nx = 10, ny = 25, method = \"Chisq\")\n\nWarning: Some expected counts are small; chi^2 approximation may be inaccurate\n\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  ana_corals\nX2 = 397.09, df = 249, p-value = 1.389e-08\nalternative hypothesis: two.sided\n\nQuadrats: 10 by 25 grid of tiles\n\n\nThe test statistic suggests highly a non-random point pattern at the scale of the quadrat defined. Note that this test is more akin to a first-order point pattern analysis because it is based on the dispersion of points among sampling quadrats."
  },
  {
    "objectID": "content/rpn_sppa.html#ripleys-k-function",
    "href": "content/rpn_sppa.html#ripleys-k-function",
    "title": "SPPA",
    "section": "Ripley’s K function:",
    "text": "Ripley’s K function:\n\nSecond-order point pattern analyses can readily be implemented in ‘spatstat’.\nRipley’s K and the standard L functions\nIgnore edge effects with ‘(correction = “none”)’\n\n\nSoutheast - Vaihu\n\nK_none_vhu <- Kest(vhu_corals, rmax = 5.0, correction = \"none\")\n\n\nPlot K\n\n\nplot(K_none_vhu, legend = F, main = \"Southeast: Ripley's K\")\n\n\n\n\n\nPlot L with 1:1 expectation\n\n\nL_none_vhu <- Lest(vhu_corals, rmax = 5.0, correction = \"none\")\n\n\nplot(L_none_vhu, legend = F, main = \"Southeast: standardized L function (standardized 1:1)\")\n\n\n\n\n\nPlot L with 0 expectation\n\n\nplot(L_none_vhu, . - r ~ r, legend = F, main = \"Southeast: standardized L function (standardized 0)\")\n\n\n\n\nThe above analysis ignores the problem of edge effects. spatstat provides a variety of edge corrections. Contrast an (1) isotropic and (2) translate correction for adjusting for boundary effects. The isotropic correction uses a simple weighting for the area sampled near the plot boundary (Ripley 1988), the translate correction uses a toroidal shift. We adjust for potential boundary effects by typing:\n\nIsotropic edge correction\n\n\nL_iso_vhu <- Lest(vhu_corals, rmax = 5.0, correction = \"isotropic\")\n\n\nplot(L_iso_vhu, . - r ~ r, legend = F, main = \"Southeast: standardzied L (isotropic correction)\")\n\n\n\n\n\nTranslate (toroidal) edge correction\n\n\nL_trans_vhu <- Lest(vhu_corals, rmax = 5.0, correction = \"trans\")\n\n\nplot(L_trans_vhu, . - r ~ r, legend = F, main = \"Southeast: standardzied L (translate correction)\")"
  },
  {
    "objectID": "content/rpn_sppa.html#monte-carlo-simulations-to-calculate-a-global-and-pointwise-confidence-envelope-under-csr",
    "href": "content/rpn_sppa.html#monte-carlo-simulations-to-calculate-a-global-and-pointwise-confidence-envelope-under-csr",
    "title": "SPPA",
    "section": "Monte Carlo simulations to calculate a global and pointwise confidence envelope under CSR",
    "text": "Monte Carlo simulations to calculate a global and pointwise confidence envelope under CSR\n\nL_csr_vhu <- envelope(vhu_corals, rmax = 5.0, Lest, nsim = 99, rank = 1, correction = \"trans\", global = F)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nL_csr.g_vhu <- envelope(vhu_corals, rmax = 5.0, Lest, nsim = 99, rank = 1, correction = \"trans\", global = T)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nPlot point-wise envelope\n\n\nplot(L_csr_vhu, . - r ~ r, shade = c(\"hi\", \"lo\"), legend = F, main = \"Southeast: stand. L (Monte Carlo, CSR env)\")\n\n\n\n\n\nPlot global envelope\n\n\nplot(L_csr.g_vhu, . - r ~ r, shade = c(\"hi\", \"lo\"), legend = F, main = \"Southeast: stand. L (Monte Carlo, CSR env)\")\n\n\n\n\n\nWest - Manavai\n\nK_none_man <- Kest(man_corals, rmax = 5.0, correction = \"none\")\n\n\nPlot K\n\n\nplot(K_none_man, legend = F, main = \"West: Ripley's K\")\n\n\n\n\n\nPlot L with 1:1 expectation\n\n\nL_none_man <- Lest(man_corals, rmax = 5.0, correction = \"none\")\n\n\nplot(L_none_man, legend = F, main = \"West: standardized L function (standardized 1:1)\")\n\n\n\n\n\nPlot L with 0 expectation\n\n\nplot(L_none_man, . - r ~ r, legend = F, main = \"West: standardized L function (standardized 0)\")\n\n\n\n\nThe above analysis ignores the problem of edge effects. spatstat provides a variety of edge corrections. Contrast an (1) isotropic and (2) translate correction for adjusting for boundary effects. The isotropic correction uses a simple weighting for the area sampled near the plot boundary (Ripley 1988), the translate correction uses a toroidal shift. We adjust for potential boundary effects by typing:\n\nIsotropic edge correction\n\n\nL_iso_man <- Lest(man_corals, rmax = 5.0, correction = \"isotropic\")\n\n\nplot(L_iso_man, . - r ~ r, legend = F, main = \"West: standardzied L (isotropic correction)\")\n\n\n\n\n\nTranslate (toroidal) edge correction\n\n\nL_trans_man <- Lest(man_corals, rmax = 5.0, correction = \"trans\")\n\n\nplot(L_trans_man, . - r ~ r, legend = F, main = \"West: standardzied L (translate correction)\")\n\n\n\n\n\n\nNorth - Anakena\n\nK_none_ana <- Kest(ana_corals, rmax = 5.0, correction = \"none\")\n\n\nPlot K\n\n\nplot(K_none_ana, legend = F, main = \"North: Ripley's K\")\n\n\n\n\n\nPlot L with 1:1 expectation\n\n\nL_none_ana <- Lest(ana_corals, rmax = 5.0, correction = \"none\")\n\n\nplot(L_none_ana, legend = F, main = \"North: standardized L function (standardized 1:1)\")\n\n\n\n\n\nPlot L with 0 expectation\n\n\nplot(L_none_ana, . - r ~ r, legend = F, main = \"North: standardized L function (standardized 0)\")\n\n\n\n\nThe above analysis ignores the problem of edge effects. spatstat provides a variety of edge corrections. Contrast an (1) isotropic and (2) translate correction for adjusting for boundary effects. The isotropic correction uses a simple weighting for the area sampled near the plot boundary (Ripley 1988), the translate correction uses a toroidal shift. We adjust for potential boundary effects by typing:\n\nIsotropic edge correction\n\n\nL_iso_ana <- Lest(ana_corals, rmax = 5.0, correction = \"isotropic\")\n\n\nplot(L_iso_ana, . - r ~ r, legend = F, main = \"North: standardzied L (isotropic correction)\")\n\n\n\n\n\nTranslate (toroidal) edge correction\n\n\nL_trans_ana <- Lest(ana_corals, rmax = 5.0, correction = \"trans\")\n\n\nplot(L_trans_ana, . - r ~ r, legend = F, main = \"North: standardzied L (translate correction)\")\n\n\n\n\nFor the functions above, two lines are drawn. The \\(L_{pois}\\) line is a dashed line that represents the expected (theoretical) value based on a Poisson process (CSR). The way that spatstat calculates \\(L\\) is to linearize \\(K\\) such that the expected value is \\(r\\) (or the radius). The other solid line represents the estimated \\(L\\) (linearized \\(K\\)), when the edges are ignored.\nWhen comparing the \\(L\\) function that ignores boundaries to those above that account for boundaries, notice that patterns change at larger distances - we expect that the \\(L\\) function at larger distances should potentially be more biased than at smaller distances because larger radii will naturally overlap more with the boundary of the study area.\nWhen edge effects are ignored, the effect in the of counting fewer points within the radius \\(r\\) near the boundary, so the observed value for \\(L\\) or \\(K\\) should have an artifact of decreasing as \\(r\\) increases.\nThe analyses so far are exploratory. While the observed statistics (\\(K\\), \\(L\\)) appear different than the expectation, it is unclear if these are substantially (or significantly) different.\nTo conduct formal inference regarding if the point pattern follows CSR, we can use Monte Carlo simulations ro calculate a confidence envelope under CSR with the envelope function.\nIn the envelope function, rank specifies the alpha for the simulations. For a rank = 1, the max an min are used as the envelopes, such that for 99 simulations, alpha = 0.01 while for 19 simulations, alpha = 0.05.\nAlso not that we used global = FALSE. This means that these are pointwise envelopes.\nThese envelopes work better for \\(L\\) than \\(K\\) because of variance stabilizing properties.\nPlots of pointwise envelopes show the stated upper and lower quantiles of simulated patterns for any distance r. Because such analyses are calculating envelopes for vhuy distances, pointwise envelopes with a specified alpha should not be used to reject a null model at that level (because of the multiple tests). Consequently, there are alternative global tests that can be used in this way. While global tests are under active development (Baddeley et al. 2014; Wiegand et al. 2016), spatstat does provide one option for a global test (using global = T).\nThis approach estimates the maximum deviation from the Poisson point process across all r (i.e., \\(D = max|K_{(r)} - K_{pois(r)}|)\\). This approach is referred to as a simultaneous envelope (or critical band) rather than a pointwise envelope.\nIf the observed line falls outside the simultaneous envelope at any point on \\(r\\), we would reject the null hypothesis."
  },
  {
    "objectID": "content/dispersion_presentation.html#different-patterns-of-dispersion",
    "href": "content/dispersion_presentation.html#different-patterns-of-dispersion",
    "title": "Pocilloporid Dispersion at Rapa Nui",
    "section": "Different patterns of dispersion",
    "text": "Different patterns of dispersion\n\nEnvironmental heterogeneity"
  },
  {
    "objectID": "content/rendering.html",
    "href": "content/rendering.html",
    "title": "Rendering",
    "section": "",
    "text": "The repo includes a GitHub Action that will render (build) the website automatically when you make changes to the files. It will be pushed to the gh-pages branch.\nBut when you are developing your content, you will want to render it locally."
  },
  {
    "objectID": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "href": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "title": "Rendering",
    "section": "Step 1. Make sure you have a recent RStudio",
    "text": "Step 1. Make sure you have a recent RStudio\nHave you updated RStudio since about August 2022? No? Then update to a newer version of RStudio. In general, you want to keep RStudio updated and it is required to have a recent version to use Quarto."
  },
  {
    "objectID": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "href": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "title": "Rendering",
    "section": "Step 2. Clone and create RStudio project",
    "text": "Step 2. Clone and create RStudio project\nFirst, clone the repo onto your local computer. How? You can click File > New Project and then select “Version Control”. Paste in the url of the repository. That will clone the repo on to your local computer. When you make changes, you will need to push those up."
  },
  {
    "objectID": "content/rendering.html#step-3.-render-within-rstudio",
    "href": "content/rendering.html#step-3.-render-within-rstudio",
    "title": "Rendering",
    "section": "Step 3. Render within RStudio",
    "text": "Step 3. Render within RStudio\nRStudio will recognize that this is a Quarto project by the presence of the _quarto.yml file and will see the “Build” tab. Click the “Render website” button to render to the _site folder.\nPreviewing: You can either click index.html in the _site folder and specify “preview in browser” or set up RStudio to preview to the viewer panel. To do the latter, go to Tools > Global Options > R Markdown. Then select “Show output preview in: Viewer panel”."
  },
  {
    "objectID": "content/code.html",
    "href": "content/code.html",
    "title": "Rendering with Code",
    "section": "",
    "text": "You can have code (R, Python or Julia) in your qmd file. You will need to have these installed on your local computer, but presumably you do already if you are adding code to your qmd files."
  },
  {
    "objectID": "content/code.html#modify-the-github-action",
    "href": "content/code.html#modify-the-github-action",
    "title": "Rendering with Code",
    "section": "Modify the GitHub Action",
    "text": "Modify the GitHub Action\nYou will need to change the GitHub Action in .github/workflows to install these and any needed packages in order for GitHub to be able to render your webpage. The GitHub Action install R since I used that in code.qmd. If you use Python or Julia instead, then you will need to update the GitHub Action to install those.\nIf getting the GitHub Action to work is too much hassle (and that definitely happens), you can alway render locally and publish to the gh-pages branch. If you do this, make sure to delete or rename the GitHub Action to something like\nrender-and-publish.old_yml\nso GitHub does not keep trying to run it. Nothing bad will happen if you don’t do this, but if you are not using the action (because it keeps failing), then you don’t need GitHub to run it."
  },
  {
    "objectID": "content/code.html#render-locally-and-publish-to-gh-pages-branch",
    "href": "content/code.html#render-locally-and-publish-to-gh-pages-branch",
    "title": "Rendering with Code",
    "section": "Render locally and publish to gh-pages branch",
    "text": "Render locally and publish to gh-pages branch\nTo render locally and push up to the gh-pages branch, open a terminal window and then cd to the directory with the Quarto project. Type this in the terminal:\nquarto render gh-pages"
  },
  {
    "objectID": "content/acknowledgements.html",
    "href": "content/acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "This repo and GitHub Action was based on the tutorial by Openscapes quarto-website-tutorial by Julia Lowndes and Stefanie Butland."
  },
  {
    "objectID": "content/rmarkdown.html",
    "href": "content/rmarkdown.html",
    "title": "R Markdown",
    "section": "",
    "text": "You can include R Markdown files in your project."
  },
  {
    "objectID": "content/rmarkdown.html#r-markdown",
    "href": "content/rmarkdown.html#r-markdown",
    "title": "R Markdown",
    "section": "R Markdown",
    "text": "R Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "content/rmarkdown.html#including-plots",
    "href": "content/rmarkdown.html#including-plots",
    "title": "R Markdown",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "content/customizing.html",
    "href": "content/customizing.html",
    "title": "Customization",
    "section": "",
    "text": "Quarto allow many bells and whistles to make nice output. Read the documentation here Quarto documentation."
  },
  {
    "objectID": "content/customizing.html#examples",
    "href": "content/customizing.html#examples",
    "title": "Customization",
    "section": "Examples",
    "text": "Examples\nLooking at other people’s Quarto code is a great way to figure out how to do stuff. Most will have a link to a GitHub repo where you can see the raw code. Look for a link to edit page or see source code. This will usually be on the right. Or look for the GitHub icon somewhere.\n\nQuarto gallery\nnmfs-openscapes\nFaye lab manual\nquarto-titlepages Note the link to edit is broken. Go to repo and look in documentation directory."
  },
  {
    "objectID": "content/fuck_this.html",
    "href": "content/fuck_this.html",
    "title": "SPPA",
    "section": "",
    "text": "vhu_csv <- read.csv(\"vhu_centroids.csv\")\n\n\nvhu_csv2 <-as_tibble(vhu_csv)\n\n\nvhu_csv2 \n\n# A tibble: 3,131 × 6\n   layer level class    id       x     y\n   <int> <chr> <int> <int>   <dbl> <dbl>\n 1     1 patch     0     1 0.0775   24.7\n 2     1 patch     0     2 0.0588   24.0\n 3     1 patch     0     3 0.0368   23.7\n 4     1 patch     0     4 0.0318   23.4\n 5     1 patch     0     5 0.0501   23.1\n 6     1 patch     0     6 0.0125   22.8\n 7     1 patch     0     7 0.0794   21.7\n 8     1 patch     0     8 0.00269  21.0\n 9     1 patch     0     9 0.0485   20.8\n10     1 patch     0    10 0.0824   16.7\n# … with 3,121 more rows\n\n\nDefine window for spatial pattern analyses based on the extent defined above\n\nvhu_window <- owin(c(0, 10), c(0, 25))\n\n\nvhu_window\n\nwindow: rectangle = [0, 10] x [0, 25] units"
  },
  {
    "objectID": "content/coral_size.html",
    "href": "content/coral_size.html",
    "title": "Coral Size",
    "section": "",
    "text": "poci_size <- read.csv('coral_size.csv')\n\n\npoci_size.gg <- read_csv(\"poci_size_main.csv\")\n\nRows: 3639 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): level, Site, cover\ndbl (7): layer, class, id, area, enn, para, size_cm\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nggplot(poci_size.gg, aes(x = size_cm)) +\n  geom_histogram(fill = \"#333399\") + \n  #below here is ylabel, xlabel, and main title\n  ylab(\"Frequency\") +\n  xlab(NULL) +\n  ggtitle(expression(\"Coral size \" (cm**2))) +\n  theme_bw() +\n  facet_wrap(~ Site, ncol = 1) +\n  #theme sets sizes, text, etc\n  theme(axis.title.x = element_text(size = 14), \n        axis.title.y = element_text(size = 14), \n        axis.text.y  = element_text(size= 10),\n        axis.text.x  = element_text(size = 12), \n        legend.text = element_text(size = 12),\n        legend.title = element_text(size = 12),\n        plot.title = element_text(hjust = 0.5, size = 14),\n        # change plot background, grid lines, etc (just examples so you can see)\n        panel.background = element_rect(fill = \"white\"),\n        panel.grid.minor.y = element_blank(),\n        panel.grid.major = element_blank(),\n        plot.background = element_rect(fill = \"white\"),\n        legend.background = element_rect(fill = \"white\"),\n        strip.text.x = element_text(size = 12, colour = \"#FFFFFF\"),\n        strip.background = element_rect(fill = '#000066')\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\npoci_size2 <-\npoci_size %>%\n  as_tibble() %>%\n  mutate(size_cm = area*10000) %>%\n  group_by(Site) %>%\n  dplyr::summarize(mean = mean(size_cm), \n                   sd = sd(size_cm), \n                   n = n(),\n                   se = sd/sqrt(n)\n  ) %>%\n  mutate(se = sd / sqrt(n),\n         lower.ci = mean - qt(1 - (0.05 / 2), n - 1) * se,\n         upper.ci = mean + qt(1 - (0.05 / 2), n - 1) * se) %>%\n  mutate_at(vars(Site), factor) %>%\n  add_column(\n          location = c('Anakena', 'Manavai', 'Southeast')\n          ) %>%\n  mutate_at(vars(location), factor)\n\n\npoci_size.gg <-\n  poci_size %>%\n  mutate(size_cm = area*10000) %>%\n  as_tibble() %>%\n  mutate_at(vars(Site), factor)\n\n\nmodel_1.lm <- lm(size_cm ~ Site, data = poci_size.gg)\n\n\nmodel_1.lm\n\n\nCall:\nlm(formula = size_cm ~ Site, data = poci_size.gg)\n\nCoefficients:\n(Intercept)      Siteman      Sitevhu  \n     110.67        40.18       120.81  \n\n\n\nsummary(model_1.lm)\n\n\nCall:\nlm(formula = size_cm ~ Site, data = poci_size.gg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-231.46  -91.09  -16.44   70.33 1160.47 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   110.67      11.18   9.896  < 2e-16 ***\nSiteman        40.18      13.07   3.074  0.00212 ** \nSitevhu       120.81      11.42  10.575  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 130.4 on 3636 degrees of freedom\nMultiple R-squared:  0.05804,   Adjusted R-squared:  0.05752 \nF-statistic:   112 on 2 and 3636 DF,  p-value: < 2.2e-16\n\n\n\npar(mfrow = c(2, 2))\nplot(model_1.lm)\n\n\n\n\n\nAnova(model_1.lm, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: size_cm\n              Sum Sq   Df F value    Pr(>F)    \n(Intercept)  1665764    1  97.933 < 2.2e-16 ***\nSite         3810826    2 112.022 < 2.2e-16 ***\nResiduals   61845547 3636                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\npost_hoc.model_1.lm <- glht(model_1.lm, linfct = mcp(Site = 'Tukey'))\n\n\nmodel_1.aov <- aov(size_cm ~ Site, data = poci_size.gg)\n\n\n# Tukey's test\ntukey <- TukeyHSD(model_1.aov)\n\n\n# compact letter display\ncld <- multcompLetters4(model_1.aov, tukey)\n\n\ncld\n\n$Site\nvhu man ana \n\"a\" \"b\" \"c\" \n\n\n\npoci_size3 <- \npoci_size2 %>%\n  add_column(\n          cld = c('a', 'b', 'c')\n          ) %>%\nmutate_at(vars(cld), factor)\n\n\npoci_size3\n\n# A tibble: 3 × 9\n  Site   mean    sd     n    se lower.ci upper.ci location  cld  \n  <fct> <dbl> <dbl> <int> <dbl>    <dbl>    <dbl> <fct>     <fct>\n1 ana    111.  73.8   136  6.33     98.2     123. Anakena   a    \n2 man    151.  82.7   372  4.29    142.      159. Manavai   b    \n3 vhu    231. 137.   3131  2.44    227.      236. Southeast c    \n\n\n\nx_labels = c(\"Anakena\", \"Manavai\", \"Vaihu\")\n# label_names = c(\"8 m\" = \"8 m\", \"15 m\" = \"15 m\", \"25 m\" = \"25 m\")\n\n\npoci_size.gg.barplot <- ggplot(poci_size3, aes(x = location, y = mean)) +   \n  geom_bar(stat = \"identity\", width = 0.75, color = \"black\", fill = \"#333399\", size = 0.50, alpha = 0.6) +\n  geom_linerange(aes(ymin = lower.ci, ymax = upper.ci), size = 0.75) +\n  scale_y_continuous(expression(paste(\"Mean Colony Size (\",\" \", cm^2, \")\")), limits = c(0, 300)) + \n  scale_x_discrete(expand = c(0, 1), labels = x_labels) + \n#  scale_fill_manual(breaks = c(\"North\", \"West\",\n#                               \"Southeast\"),\n#                    values = c(\"red\", \"blue\", \n#                                    \"green\"), \n#                                    labels = c(\"North\", \"West\",\n#                                               \"Southeast\")) +\n#  facet_wrap( ~ depth2, labeller = as_labeller(label_names), dir = \"v\", ncol = 1) + \n  ggtitle(expression(paste(italic(\" Pocillopora \"), \"spp.\"))) +\n  geom_text(aes(label = cld, y = upper.ci), vjust = -0.5) +\n  #scale_y_log10(expression(paste(\"Colony Size (\", cm^2, \")\"), limits = c(0, 100000))) +\n  labs(x = NULL) +\n  theme(strip.text = element_text(size = 10, color = \"black\", hjust = 0.50),\n        strip.background = element_rect(fill = \"#FFFFFF\", color = NA),    \n        panel.background = element_rect(fill = \"#FFFFFF\", color = NA),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_line(color = \"#b2b2b2\"),\n        panel.spacing.x = unit(1, \"cm\"),\n        panel.spacing.y = unit(0.5, \"cm\"),\n        panel.spacing = unit(1, \"lines\"),\n        axis.ticks = element_blank(),\n        legend.position = \"top\",\n        plot.title = element_text(size = 11),\n        axis.title.y = element_text(size = 11),\n        legend.title = element_blank())\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\npoci_size.gg.barplot"
  },
  {
    "objectID": "content/add-content.html",
    "href": "content/add-content.html",
    "title": "Customize",
    "section": "",
    "text": "Edit the qmd or md files in the content folder. qmd files can include code (R, Python, Julia) and lots of Quarto markdown bells and whistles (like call-outs, cross-references, auto-citations and much more).\nEach page should start with\n---\ntitle: your title\n---\nand the first header will be the 2nd level, so ##. Note, there are situations where you leave off\n---\ntitle: your title\n---\nand start the qmd file with a level header #, but if using the default title yaml (in the --- fence) is a good habit since it makes it easy for Quarto convert your qmd file to other formats (like into a presentation)."
  },
  {
    "objectID": "content/add-content.html#add-your-pages-the-project",
    "href": "content/add-content.html#add-your-pages-the-project",
    "title": "Customize",
    "section": "Add your pages the project",
    "text": "Add your pages the project\n\nAdd the files to _quarto.yml"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NOAA quarto simple with R",
    "section": "",
    "text": "This is a template for a simple Quarto website that looks like a “book”. This is a common format for documentation. It includes a GitHub Action that will build the website automatically when you make changes to the files. The NOAA palette and fonts has been added to theme.scss. The webpage will be on the gh-pages branch. Serving the website files from this branch is a common way to keep all the website files from cluttering your main branch.\nThe GitHub Action installs R so you can have R code in your qmd or Rmd files. Note, you do not need to make changes to your Rmd files unless your need Quarto features like cross-references."
  },
  {
    "objectID": "index.html#github-set-up",
    "href": "index.html#github-set-up",
    "title": "NOAA quarto simple with R",
    "section": "GitHub Set-up",
    "text": "GitHub Set-up\n\nClick the green “use template” button to make a repository with this content. Make sure to make your repo public (since GitHub Pages doesn’t work on private repos unless you have a paid account) and check box to include all the branches (so that you get the gh-pages branch). \nTurn on GitHub Pages under Settings > Pages . You will set pages to be made from the gh-pages branch and root directory. \nTurn on GitHub Actions under Settings > Actions > General \nEdit the repo description and Readme to add a link to the webpage. When you edit the description, you will see the link url in the url box or you can click on the Actions tab or the Settings > Pages page to find the url."
  },
  {
    "objectID": "fuck_this.html",
    "href": "fuck_this.html",
    "title": "SPPA",
    "section": "",
    "text": "vhu_csv <- read.csv(\"vhu_centroids.csv\")\n\n\nvhu_csv2 <-as_tibble(vhu_csv)\n\n\nvhu_csv2 \n\n# A tibble: 3,131 × 6\n   layer level class    id       x     y\n   <int> <chr> <int> <int>   <dbl> <dbl>\n 1     1 patch     0     1 0.0775   24.7\n 2     1 patch     0     2 0.0588   24.0\n 3     1 patch     0     3 0.0368   23.7\n 4     1 patch     0     4 0.0318   23.4\n 5     1 patch     0     5 0.0501   23.1\n 6     1 patch     0     6 0.0125   22.8\n 7     1 patch     0     7 0.0794   21.7\n 8     1 patch     0     8 0.00269  21.0\n 9     1 patch     0     9 0.0485   20.8\n10     1 patch     0    10 0.0824   16.7\n# … with 3,121 more rows\n\n\nDefine window for spatial pattern analyses based on the extent defined above\n\nvhu_window <- owin(c(0, 10), c(0, 25))\n\n\nvhu_window\n\nwindow: rectangle = [0, 10] x [0, 25] units"
  }
]