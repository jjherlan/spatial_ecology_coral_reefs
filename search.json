[
  {
    "objectID": "rpn_sppa.html",
    "href": "rpn_sppa.html",
    "title": "SPPA",
    "section": "",
    "text": "vhu_csv <- read.csv(\"vhu_centroids.csv\")\n\n\nvhu_csv2 <-as_tibble(vhu_csv)\n\n\nvhu_csv2 %>%\n  select(x, y)\n\n# A tibble: 3,131 × 2\n         x     y\n     <dbl> <dbl>\n 1 0.0775   24.7\n 2 0.0588   24.0\n 3 0.0368   23.7\n 4 0.0318   23.4\n 5 0.0501   23.1\n 6 0.0125   22.8\n 7 0.0794   21.7\n 8 0.00269  21.0\n 9 0.0485   20.8\n10 0.0824   16.7\n# ℹ 3,121 more rows\n\n\n\nvhu_window <- owin(c(0, 10), c(0, 25))\n\n\nattach(vhu_window)\n\n\nvhu_corals <- ppp(vhu_csv2$x, vhu_csv2$y, window = vhu_window)\n\nSummary information\n\nsummary(vhu_corals)\n\nPlanar point pattern:  3131 points\nAverage intensity 12.524 points per square unit\n\nCoordinates are given to 7 decimal places\n\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\n\n\nThis summary shows there are 3131 points (pocilloporid coral colony) and provides the observed, lambda. The density plot can be a helpful visualization of intensity of points across the plot. By plotting the spatial intensity this way, spatial trends in the point occurrences that may violate the assumption of homogeneous point process.\n\ndensity plots\n\nplot(\n  density(vhu_corals)\n)\n\n\n\n\n#alter smoothing parameter\n\nplot(\n  density(vhu_corals, 1)\n)  \n\n\n\n\n#contour plot\ncontour( density(vhu_corals, 1) )\n\n\nWe can also make tallies of counts of point locations based on quadrats overlaid\n\n\non the plot. To determine whether these quadrat counts conform to CSR\n\n\n(i.e., a homogeneous Poisson process), use a simple Chi-Square test statistic.\n\n\nquadrat counts\nvhu_Q <- quadratcount(vhu_corals, nx = 10, ny = 25) #counts in 10 x 25 m quadrats\n#plot\nplot(vhu_corals, cex = 1)\nplot(vhu_Q, add = TRUE, cex = 1)\n#chi-sq test for complete spatial randomness, CSR\nquadrat.test(vhu_corals, nx = 10, ny = 25, method = “Chisq”)\n\n\nThe test statistic suggests highly a non-random point pattern at the\n\n\nscale of the quadrat defined. Note that this test is more akin to a first-order\n\n\npoint pattern analysis because it is based on the dispersion of points among\n\n\nsampling quadrats."
  },
  {
    "objectID": "content/percent_cover_fieldwork2016.html",
    "href": "content/percent_cover_fieldwork2016.html",
    "title": "Percent Cover Transects & Photos 2016",
    "section": "",
    "text": "rpn_ana_sh <- read.csv('ana_sh.csv') %>%\n  as_tibble() %>%\n  mutate_at(vars(location, depth, transect, photo, group), factor)\n\n\nrpn_ana_sh\n\n# A tibble: 420 × 7\n   location depth transect photo group    successes failures\n   <fct>    <fct> <fct>    <fct> <fct>        <int>    <int>\n 1 north    sh    t1       t1_1  abiotic          0      100\n 2 north    sh    t1       t1_1  biotic           0      100\n 3 north    sh    t1       t1_1  macro            2       98\n 4 north    sh    t1       t1_1  non-reef        12       88\n 5 north    sh    t1       t1_1  plob            85       15\n 6 north    sh    t1       t1_1  poci             0      100\n 7 north    sh    t1       t1_1  turf             1       99\n 8 north    sh    t1       t1_10 abiotic          0      100\n 9 north    sh    t1       t1_10 biotic           0      100\n10 north    sh    t1       t1_10 macro           13       87\n# ℹ 410 more rows\n\n\n\nrpn_ana_md <- read.csv('ana_md.csv') %>%\n  as_tibble() %>%\n  mutate_at(vars(location, depth, transect, photo, group), factor)\n\n\nrpn_ana_md\n\n# A tibble: 420 × 7\n   location depth transect photo group    successes failures\n   <fct>    <fct> <fct>    <fct> <fct>        <int>    <int>\n 1 north    md    t1       t1_1  abiotic          0      100\n 2 north    md    t1       t1_1  biotic           0      100\n 3 north    md    t1       t1_1  macro            8       92\n 4 north    md    t1       t1_1  non-reef        27       73\n 5 north    md    t1       t1_1  plob            42       58\n 6 north    md    t1       t1_1  poci             5       95\n 7 north    md    t1       t1_1  turf            18       82\n 8 north    md    t1       t1_10 abiotic          0      100\n 9 north    md    t1       t1_10 biotic           0      100\n10 north    md    t1       t1_10 macro            8       92\n# ℹ 410 more rows\n\n\n\nrpn_ana_dp <- read.csv('ana_dp.csv') %>%\n  as_tibble() %>%\n  mutate_at(vars(location, depth, transect, photo, group), factor)\n\n\nrpn_ana_dp\n\n# A tibble: 420 × 7\n   location depth transect photo group    successes failures\n   <fct>    <fct> <fct>    <fct> <fct>        <int>    <int>\n 1 north    dp    t1       t1_1  abiotic          1       99\n 2 north    dp    t1       t1_1  biotic           0      100\n 3 north    dp    t1       t1_1  macro           17       83\n 4 north    dp    t1       t1_1  non-reef        15       85\n 5 north    dp    t1       t1_1  plob            62       38\n 6 north    dp    t1       t1_1  poci             5       95\n 7 north    dp    t1       t1_1  turf             0      100\n 8 north    dp    t1       t1_10 abiotic          0      100\n 9 north    dp    t1       t1_10 biotic           0      100\n10 north    dp    t1       t1_10 macro           25       75\n# ℹ 410 more rows\n\n\n\nrpn_north <- bind_rows(rpn_ana_sh, rpn_ana_md, rpn_ana_dp) %>%\n  as_tibble() %>%\n  mutate_at(vars(location, depth, transect, photo, group), factor)\n\n\nrpn_north\n\n# A tibble: 1,260 × 7\n   location depth transect photo group    successes failures\n   <fct>    <fct> <fct>    <fct> <fct>        <int>    <int>\n 1 north    sh    t1       t1_1  abiotic          0      100\n 2 north    sh    t1       t1_1  biotic           0      100\n 3 north    sh    t1       t1_1  macro            2       98\n 4 north    sh    t1       t1_1  non-reef        12       88\n 5 north    sh    t1       t1_1  plob            85       15\n 6 north    sh    t1       t1_1  poci             0      100\n 7 north    sh    t1       t1_1  turf             1       99\n 8 north    sh    t1       t1_10 abiotic          0      100\n 9 north    sh    t1       t1_10 biotic           0      100\n10 north    sh    t1       t1_10 macro           13       87\n# ℹ 1,250 more rows\n\n\n\nrpn_north_plob <- rpn_north %>%  \n  filter(group == \"plob\") %>%\n  group_by(depth, transect, photo)\n\n\nrpn_north_plob\n\n# A tibble: 180 × 7\n# Groups:   depth, transect, photo [179]\n   location depth transect photo group successes failures\n   <fct>    <fct> <fct>    <fct> <fct>     <int>    <int>\n 1 north    sh    t1       t1_1  plob         85       15\n 2 north    sh    t1       t1_10 plob         81       19\n 3 north    sh    t1       t1_11 plob         81       19\n 4 north    sh    t1       t1_12 plob         56       44\n 5 north    sh    t1       t1_13 plob         68       32\n 6 north    sh    t1       t1_14 plob         93        7\n 7 north    sh    t1       t1_15 plob         57       43\n 8 north    sh    t1       t1_2  plob         81       19\n 9 north    sh    t1       t1_3  plob         82       18\n10 north    sh    t1       t1_4  plob         84       16\n# ℹ 170 more rows\n\n\n\nrpn_north.glm <- glm(cbind(successes, failures) ~ depth, \n                  family = binomial(link = \"logit\"), data = rpn_north_plob)\n\n\nsummary(rpn_north.glm)\n\n\nCall:\nglm(formula = cbind(successes, failures) ~ depth, family = binomial(link = \"logit\"), \n    data = rpn_north_plob)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-16.671   -1.141    0.883    2.608    6.449  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.10306    0.02985  36.957   <2e-16 ***\ndepthmd     -0.10082    0.04171  -2.417   0.0156 *  \ndepthdp     -0.55221    0.04012 -13.765   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2859.7  on 179  degrees of freedom\nResidual deviance: 2638.2  on 177  degrees of freedom\nAIC: 3470.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nAnova function from the car package\n\nAnova(rpn_north.glm, type = \"III\") # Type III because... \n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: cbind(successes, failures)\n      LR Chisq Df Pr(>Chisq)    \ndepth   221.54  2  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nrpn_north_poci <- rpn_north %>%  \n  filter(group == \"poci\") %>%\n  group_by(depth, transect, photo)\n\n\nrpn_north_poci\n\n# A tibble: 180 × 7\n# Groups:   depth, transect, photo [179]\n   location depth transect photo group successes failures\n   <fct>    <fct> <fct>    <fct> <fct>     <int>    <int>\n 1 north    sh    t1       t1_1  poci          0      100\n 2 north    sh    t1       t1_10 poci          0      100\n 3 north    sh    t1       t1_11 poci          0      100\n 4 north    sh    t1       t1_12 poci          0      100\n 5 north    sh    t1       t1_13 poci          0      100\n 6 north    sh    t1       t1_14 poci          0      100\n 7 north    sh    t1       t1_15 poci          0      100\n 8 north    sh    t1       t1_2  poci          0      100\n 9 north    sh    t1       t1_3  poci          0      100\n10 north    sh    t1       t1_4  poci          0      100\n# ℹ 170 more rows\n\n\n\nrpn_north_poci.glm <- glm(cbind(successes, failures) ~ depth, \n                  family = binomial(link = \"logit\"), data = rpn_north_poci)\n\n\nsummary(rpn_north_poci.glm)\n\n\nCall:\nglm(formula = cbind(successes, failures) ~ depth, family = binomial(link = \"logit\"), \n    data = rpn_north_poci)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.487  -1.127  -1.122   0.000   6.473  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -5.064235   0.193055 -26.232   <2e-16 ***\ndepthmd      0.008659   0.252491   0.034   0.9726    \ndepthdp      0.565436   0.229324   2.466   0.0137 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 391.76  on 162  degrees of freedom\nResidual deviance: 381.68  on 160  degrees of freedom\nAIC: 515.35\n\nNumber of Fisher Scoring iterations: 5\n\n\nAnova function from the car package\n\nAnova(rpn_north_poci.glm, type = \"III\") # Type III because... \n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: cbind(successes, failures)\n      LR Chisq Df Pr(>Chisq)   \ndepth   10.083  2   0.006465 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "content/percent_cover.html",
    "href": "content/percent_cover.html",
    "title": "Percent Cover",
    "section": "",
    "text": "From Damgaard & Irvine (2019) Using the beta distribution to analyse plant cover data Journal of Ecology. 107:2747-2759\nWrote a function to create a table from betreg object that can be used in xtable()"
  },
  {
    "objectID": "content/percent_cover.html#methods-for-analyzing-percent-cover-of-pocilloporid-coral",
    "href": "content/percent_cover.html#methods-for-analyzing-percent-cover-of-pocilloporid-coral",
    "title": "Percent Cover",
    "section": "Methods for analyzing percent cover of pocilloporid coral",
    "text": "Methods for analyzing percent cover of pocilloporid coral\nOne option for dealing with the 0 and 1 values is to transform them to be slightly less than one or more than zero. This approach assumes that the data are consistent with a common beta distribution. We fit five models to the data: three variations on the beta model and two linear model approaches.\nA beta regression assuming a common spatial aggregation \\(\\delta\\) or precision parameter (\\(\\phi\\)) (object named: mod.beta1). Notice that \\(\\delta= \\frac{1}{1+\\phi}\\) and \\(\\phi=\\frac{(1-\\delta)}{\\delta}\\). A beta regression assuming each year had a different \\(\\phi\\) parameter (object named: mod.beta2).\nAnother option for modeling the data is to use a zero-one augmented beta model. Currently, the betareg package does not implement this model directly. Therefore, we follow the theoretical results shown in Ospina and Ferrari (2010) that suggest a three-part model can be fit to the data. Basically, we use logistic regression with response an indicator variable for whether or not the plot had zero recorded cover, another logistic regression with response an indicator for whether or not the plot had 100\\(\\%\\) recorded percent cover, and then the beta regression is used to model the continuous percent cover observations ranging from greater than 0 and less than 1.\nOther options based on assuming that the residuals are normally distributed is to use a linear model with a logit-transformed response (object named: mod.lmlogit) or a linear model with response untransformed proportions (object named: mod.lmraw).\nFor comparison this applies a logit-transformation to the empirical proportions and then uses a standard linear regression model.\nRaw data - no transformation\n\np_cover_mod.aov1 <- aov(pland_decimal ~ Site, data = p_cover)\nsummary(p_cover_mod.aov1)\n\n             Df Sum Sq Mean Sq F value Pr(>F)    \nSite          2 12.699   6.350    2203 <2e-16 ***\nResiduals   747  2.153   0.003                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\np_cover_mod.lm1 <- lm(pland_decimal ~ Site, data = p_cover)\nsummary(p_cover_mod.lm1)\n\n\nCall:\nlm(formula = pland_decimal ~ Site, data = p_cover)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.267087 -0.022444 -0.006021  0.015304  0.242376 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.006021   0.003395   1.773  0.07656 .  \nSiteman     0.016423   0.004802   3.420  0.00066 ***\nSitevhu     0.283878   0.004802  59.122  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05368 on 747 degrees of freedom\nMultiple R-squared:  0.8551,    Adjusted R-squared:  0.8547 \nF-statistic:  2203 on 2 and 747 DF,  p-value: < 2.2e-16\n\n\n\npar(mfrow = c(2, 2))\nplot(p_cover_mod.lm1)\n\n\n\n\n\nAnova(p_cover_mod.lm1, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: pland_decimal\n             Sum Sq  Df   F value  Pr(>F)    \n(Intercept)  0.0091   1    3.1451 0.07656 .  \nSite        12.6991   2 2203.2548 < 2e-16 ***\nResiduals    2.1528 747                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOne option to deal with 0 and 100 percent cover is to add and subtract a small amount to those values\n\ntransform01 <- function(x) {\n  (x * (length(x) - 1) + 0.5) / (length(x))\n}\n\n\np_cover$pland_decimal_scaled <- transform01(p_cover$pland_decimal)\n\nLogit-transformation\n\np_cover_mod.lm2 <- lm(logit(pland_decimal_scaled) ~ Site, data = p_cover) \n\n\nsummary(p_cover_mod.lm2)\n\n\nCall:\nlm(formula = logit(pland_decimal_scaled) ~ Site, data = p_cover)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7875 -1.0925 -0.1617  0.7543  4.5854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6.22005    0.09621 -64.654  < 2e-16 ***\nSiteman      0.61521    0.13606   4.522 7.13e-06 ***\nSitevhu      5.27828    0.13606  38.795  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.521 on 747 degrees of freedom\nMultiple R-squared:  0.7067,    Adjusted R-squared:  0.7059 \nF-statistic: 900.1 on 2 and 747 DF,  p-value: < 2.2e-16\n\n\n\npar(mfrow = c(2, 2))\nplot(p_cover_mod.lm2)\n\n\n\n\n\nAnova(p_cover_mod.lm2, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: logit(pland_decimal_scaled)\n            Sum Sq  Df F value    Pr(>F)    \n(Intercept) 9672.3   1 4180.11 < 2.2e-16 ***\nSite        4165.2   2  900.06 < 2.2e-16 ***\nResiduals   1728.5 747                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nGeneralized-linear model\n\np_cover_mod.glm1 <- glm(pland_decimal ~ Site, family = binomial, data = p_cover)\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nsummary(p_cover_mod.glm1)\n\n\nCall:\nglm(formula = pland_decimal ~ Site, family = binomial, data = p_cover)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-0.71274  -0.15856  -0.10518   0.07292   0.94245  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -5.1064     0.8175  -6.246 4.21e-10 ***\nSiteman       1.3324     0.9223   1.445    0.149    \nSitevhu       4.2106     0.8293   5.077 3.83e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 161.289  on 749  degrees of freedom\nResidual deviance:  26.922  on 747  degrees of freedom\nAIC: 191.54\n\nNumber of Fisher Scoring iterations: 8\n\n\n\npar(mfrow = c(2, 2))\nplot(p_cover_mod.glm1)\n\n\n\n\n\nAnova(p_cover_mod.glm1, type = \"III\")\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: pland_decimal\n     LR Chisq Df Pr(>Chisq)    \nSite   134.37  2  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBeta Regression I: \\(\\phi\\) does not vary\n\np_cover_mod.beta1 <- betareg(pland_decimal_scaled ~ Site, data = p_cover, link = c(\"logit\"), link.phi = NULL, type = c(\"ML\"))\n\nBeta Regression II: \\(\\phi\\) does vary (by Site)\n\np_cover_mod.beta2 <- betareg(pland_decimal_scaled ~ Site | Site, data = p_cover, link = c(\"logit\"), link.phi = NULL, type = c(\"ML\"))\n\nExtract AIC from beta regression models\n\np_cover_mod.beta1_aic <- AIC(p_cover_mod.beta1)\np_cover_mod.beta2_aic <- AIC(p_cover_mod.beta2)\n\n\np_cover_mod.beta1_aic\n\n[1] -4179.365\n\np_cover_mod.beta2_aic\n\n[1] -4259.399"
  },
  {
    "objectID": "content/percent_cover.html#interpreting-results-for-pocilloporid-corals",
    "href": "content/percent_cover.html#interpreting-results-for-pocilloporid-corals",
    "title": "Percent Cover",
    "section": "Interpreting results for pocilloporid corals",
    "text": "Interpreting results for pocilloporid corals\nIn order to choose between the beta regression model with a common \\(\\phi\\) versus different \\(\\phi\\), I used AIC but a likelihood ratio or wald test could be used. Using AIC, the model with varying \\(\\phi\\) values had a lower AIC (-1518.568 compared to -1494.524) and therefore more support. We interpret the output from beta regression with the following:\nThe model we fit assumes \\[logit(\\mu_j)=\\beta_0+\\beta_1 Ind_{grp2},\\] where \\(Ind_{grp2}\\) is an indicator for group 2 and \\(j\\) denotes the group membership so \\(j=1\\) or \\(j=2\\). We have \\(logit(\\mu_2)-logit(\\mu_1)=\\beta_1\\), which is equivalent to \\[log(\\frac{\\mu_2}{1-\\mu_2})-log(\\frac{\\mu_1}{1-\\mu_1})= \\beta_1.\\]\nThe \\(\\frac{\\mu_j}{1-\\mu_j}\\) is interpreted as the odds of proportion cover in group \\(j\\). Therefore,\n\\[log(\\frac{\\mu_2}{1-\\mu_2}/\\frac{\\mu_1}{1-\\mu_1})=\\beta_1\\] is the log- odds ratio of cover in group 2 compared to group 1, \\[(\\frac{\\mu_2}{1-\\mu_2}/\\frac{\\mu_1}{1-\\mu_1})=exp(\\beta_1).\\]\nThen \\(exp(\\beta_1)\\) is the factor increase/decrease in odds of proportion cover for group 2 compared to group 1, where \\(exp(\\beta_1)>1\\) is an increase and \\(exp(\\beta_1)<1\\) is a decrease, and \\(exp(\\beta_1) \\approx 1\\) means essentially no change.\nBeta Regression I: \\(\\phi\\) does not vary\n\n\n\n\n\n\nResults from using betareg package in R by transforming the 0 and 1’s. These are on the logit-scale for \\(\\mu\\) with a common \\(\\phi\\) parameter\n\n\n\n\n\nEstimate\n\n\nStd. Error\n\n\nz value\n\n\nPr(>|z|)\n\n\n\n\n(Intercept)\n\n\n-4.283\n\n\n0.074\n\n\n-57.953\n\n\n0.000\n\n\n\n\nSiteman\n\n\n0.205\n\n\n0.084\n\n\n2.433\n\n\n0.015\n\n\n\n\nSitevhu\n\n\n3.384\n\n\n0.077\n\n\n43.754\n\n\n0.000\n\n\n\n\n(phi)\n\n\n25.321\n\n\n1.561\n\n\n16.221\n\n\n0.000\n\n\n\n\nBeta Regression II: \\(\\phi\\) does vary (by Site)\n\np_cover_mod.beta2 <- betareg(pland_decimal_scaled ~ Site | Site, data = p_cover, link = c(\"logit\"), link.phi = NULL, type = c(\"ML\"))\n\n\nsummary(p_cover_mod.beta2)\n\n\nCall:\nbetareg(formula = pland_decimal_scaled ~ Site | Site, data = p_cover, \n    link = c(\"logit\"), link.phi = NULL, type = c(\"ML\"))\n\nStandardized weighted residuals 2:\n    Min      1Q  Median      3Q     Max \n-6.1960 -0.5649 -0.3417  0.7115  1.8425 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -4.99928    0.08784 -56.913   <2e-16 ***\nSiteman      1.26338    0.13630   9.269   <2e-16 ***\nSitevhu      4.09928    0.09194  44.584   <2e-16 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   4.3427     0.1141  38.052  < 2e-16 ***\nSiteman      -1.6249     0.1679  -9.680  < 2e-16 ***\nSitevhu      -1.1097     0.1441  -7.699 1.37e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  2136 on 6 Df\nPseudo R-squared: 0.6791\nNumber of iterations: 22 (BFGS) + 1 (Fisher scoring) \n\n\nThe function, Fcn.CreateSummary.betareg, doesn’t work with phi varying, so just fixed labels manually.\n\np_cover_mod.glm1 <- glm(pland_decimal ~ Site, family = binomial, data = p_cover)\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nprint(xtable(summary(p_cover_mod.glm1), digits = 3, caption = \" . \"), type = \"html\")\n\n<!-- html table generated in R 4.2.3 by xtable 1.8-4 package -->\n<!-- Tue Mar 28 23:08:32 2023 -->\n<table border=1>\n<caption align=\"bottom\">  .  </caption>\n<tr> <th>  </th> <th> Estimate </th> <th> Std. Error </th> <th> z value </th> <th> Pr(&gt;|z|) </th>  </tr>\n  <tr> <td align=\"right\"> (Intercept) </td> <td align=\"right\"> -5.106 </td> <td align=\"right\"> 0.818 </td> <td align=\"right\"> -6.246 </td> <td align=\"right\"> 0.000 </td> </tr>\n  <tr> <td align=\"right\"> Siteman </td> <td align=\"right\"> 1.332 </td> <td align=\"right\"> 0.922 </td> <td align=\"right\"> 1.445 </td> <td align=\"right\"> 0.149 </td> </tr>\n  <tr> <td align=\"right\"> Sitevhu </td> <td align=\"right\"> 4.211 </td> <td align=\"right\"> 0.829 </td> <td align=\"right\"> 5.077 </td> <td align=\"right\"> 0.000 </td> </tr>\n   </table>\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nz value\nPr(>|z|)\n\n\n\n\n(Intercept)\n-4.129\n0.822\n-5.025\n0.000\n\n\nSiteman\n1.225\n0.929\n1.318\n0.187\n\n\nSitevhu\n3.233\n0.833\n3.880\n0.000\n\n\n\n\n\n\n\n\nExample diagnostic plots for beta regression)\n\n\n\n\n\n\n\n\n\nExample diagnostic plots for beta regression, variable phi)\n\n\n\n\nBuilt-in diagnostic plots for linear regression models\nLogit-transformed\n\npar(mfrow = c(3, 2), pty = 'm')\nplot(p_cover_mod.lm2)\nplot(cooks.distance(p_cover_mod.lm2))\n\n\n\n\nUntransformed\n\npar(mfrow = c(3, 2), pty = 'm')\nplot(p_cover_mod.lm1)\nplot(cooks.distance(p_cover_mod.lm1))\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of default qqplots for beta and linear models. Beta models use weighted residuals 2 recommended in and linear model are the response residuals.\n\n\n\n\n\npar(mfrow = c(2,1), pty = 'm', cex = 1)\n\nplot(p_cover_mod.beta2, which = 5, type = \"sweighted2\", main = \"\")\nplot(p_cover_mod.lm2, which = 2, main = \"\")\n\n\n\n\n\nClassical Analysis\nFrom Douma & Weedon (2019)\n\n\np_cover.aov1 <- aov(pland_decimal ~ Site, data = p_cover)\nsummary(p_cover.aov1)\n##              Df Sum Sq Mean Sq F value Pr(>F)    \n## Site          2 12.699   6.350    2203 <2e-16 ***\n## Residuals   747  2.153   0.003                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\np_cover.aov2 <- aov(pland_decimal ~ Site + Error(plot_id), data = p_cover)\nsummary(p_cover.aov2)\n## \n## Error: plot_id\n##            Df Sum Sq Mean Sq F value Pr(>F)\n## Residuals 249 0.7794 0.00313               \n## \n## Error: Within\n##            Df Sum Sq Mean Sq F value Pr(>F)    \n## Site        2 12.699   6.350    2302 <2e-16 ***\n## Residuals 498  1.373   0.003                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA very similar analysis can be conducted using the library nlme for mixed effects modeling.\n\np_cover.lme1 <- lme(pland_decimal ~ Site, random = ~ 1 | plot_id, data = p_cover)\n\n\nanova(p_cover.lme1)\n\n            numDF denDF  F-value p-value\n(Intercept)     1   498 2698.541  <.0001\nSite            2   498 2302.345  <.0001\n\n\n\np_cover.lme_null <- lme(pland_decimal ~ 1, random = ~ 1 | plot_id, data = p_cover)\n\n\nlmtest::lrtest(p_cover.lme1, p_cover.lme_null)\n\nLikelihood ratio test\n\nModel 1: pland_decimal ~ Site\nModel 2: pland_decimal ~ 1\n  #Df  LogLik Df Chisq Pr(>Chisq)    \n1   5 1117.16                        \n2   3  402.18 -2  1430  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nBeta regression with no variable precision \\(\\phi\\)\nWe now turn to the beta regression model, that models the response variable as being generated from a beta distribution (i.e. that is bounded at 0 and 1).\nThe observations of percent cover are based on replicate quadrats replicated within three experimental plots.\nWe begin by attempting to fit a beta regression model, with pland_decimal as the response, and Site as the categorical predictor.\n\np_cover.bm1 <- betareg(pland_decimal_scaled ~ Site, data = p_cover)\n\nbetareg will not accept values of 0 and 1 in the response variable.\nThere are two possible solutions here, rescaling the data to remove 0s and 1s, or fitting zero-inflated models. We start here with the rescaling solution. A suggested rescaling equation is:\n\\[ x^*_{i} = \\frac{x_i(n-1)+0.5}{n} \\]\nWhere \\(x^*_i\\) is the transformation of \\(x_i\\) and \\(n\\) is the total number of observations in the dataset.\nFor convenience we define this as a custom function tranform01 and apply it to the dataset:\n\ntransform01 <- function(x) {\n  (x * (length(x) - 1) + 0.5) / (length(x))\n}\n\nWith this scaled data we can now successfully fit the model. And test its significance relative to a null model that assumes no effect of wave power on percent cover of pocilloporid corals. For reference we also fit a classical ANOVA model assuming normally distributed errors using lm.\n\np_cover.bmnull <- betareg(pland_decimal_scaled ~ 1, data = p_cover)\nsummary(p_cover.bmnull)\n\n\nCall:\nbetareg(formula = pland_decimal_scaled ~ 1, data = p_cover)\n\nStandardized weighted residuals 2:\n    Min      1Q  Median      3Q     Max \n-0.8695 -0.8695  0.0151  0.8905  1.1783 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -2.18925    0.05919  -36.99   <2e-16 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(>|z|)    \n(phi)   3.0204     0.1916   15.76   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  1511 on 2 Df\nNumber of iterations: 15 (BFGS) + 1 (Fisher scoring) \n\n\n\np_cover.lm1 <- lm(pland_decimal_scaled ~ Site, data = p_cover)\nsummary(p_cover.lm1)\n\n\nCall:\nlm(formula = pland_decimal_scaled ~ Site, data = p_cover)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.266731 -0.022414 -0.006013  0.015284  0.242053 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.006680   0.003391    1.97  0.04920 *  \nSiteman     0.016401   0.004795    3.42  0.00066 ***\nSitevhu     0.283499   0.004795   59.12  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05361 on 747 degrees of freedom\nMultiple R-squared:  0.8551,    Adjusted R-squared:  0.8547 \nF-statistic:  2203 on 2 and 747 DF,  p-value: < 2.2e-16\n\n\n\nlmtest::lrtest(p_cover.bm1, p_cover.bmnull)\n\nLikelihood ratio test\n\nModel 1: pland_decimal_scaled ~ Site\nModel 2: pland_decimal_scaled ~ 1\n  #Df LogLik Df  Chisq Pr(>Chisq)    \n1   4 2093.7                         \n2   2 1511.0 -2 1165.3  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nAIC(p_cover.bm1, p_cover.lm1, p_cover.bmnull)\n\n               df       AIC\np_cover.bm1     4 -4179.365\np_cover.lm1     4 -2255.582\np_cover.bmnull  2 -3018.031\n\n\nAccording to the likelihood-ratio test there is a significant difference betwen the null model and the treatment model. The AIC analysis supports this conclusion, but also highlights the improved model fit with beta regression relative to normal ANOVA (lm1). From this initial analysis we would tentatively conclude that using beta regresson improves our ability to model the algal cover, but that there is no effect of grazer manipulation treatment.\nIt is useful to plot the predictions derived from the model and compare them to the observed data. First we define two new functions to allow us to use the dbeta and rbeta functions with the \\(\\mu\\) and \\(\\phi\\) parameterization.\n\n\ndbeta2 <- function(X, mu, phi, ...) {\n  dbeta(X, shape1 = mu * phi, shape2 = (1 - mu) * phi, ...)\n}\n\nrbeta2 <- function(N, mu, phi, ...) {\n  rbeta(N, shape1 = mu * phi, shape2 = (1 - mu) * phi, ...)\n}\n\n\nWith this function we can plot the distributions corresponding to the MLE parameters for each treatment :\n\n\n# extract coefficients of beta regression model\ncoefs.bm1 <- coef(p_cover.bm1)\n\n# create vector spanning the transformed 0-1 interval\n\nn.bm2 <- length(fitted(p_cover.bm1))\nx.range <- seq(0.5/n.bm2 , 1-0.5/n.bm2 , length.out = 200)\nx.range.bt <- (x.range*n.bm2 - 0.5)/(n.bm2-1)\n\n\n# Anakena\nplot(x.range.bt, dbeta2(x.range, inv.logit(coefs.bm1[\"(Intercept)\"]), coefs.bm1[\"(phi)\"]),\n     type = \"l\", lty = 2, lwd = 2,\n     ylab = \"Probability density\", xlab = \"Proportion cover\",\n     ylim=c(0, 10)\n)\n\n# Manavai\nlines(x.range.bt, dbeta2(x.range, inv.logit(coefs.bm1[\"(Intercept)\"] + coefs.bm1[2]), coefs.bm1[\"(phi)\"]),lwd = 2, col = \"red\")\n\n# Vaihu\nlines(x.range.bt, dbeta2(x.range, inv.logit(coefs.bm1[\"(Intercept)\"] + coefs.bm1[3]), coefs.bm1[\"(phi)\"]), col = \"blue\", lwd = 2)\n\nrug(p_cover$pland_decimal[p_cover$Site == \"ana\"], lwd = 1.5, pos = 10)\n\nrug(p_cover$pland_decimal[p_cover$Site == \"man\"],col=\"red\", pos = 9.75, side = 3,lwd=1.5)\n\nrug(p_cover$pland_decimal[p_cover$Site == \"vhu\"], col=\"blue\", pos = 9.5, side = 3, lwd=1.5)\n\nlegend(\"topright\", lwd = 2, lty = c(2, 1, 1, 1), col = c(\"black\", \"red\", \"blue\"), legend = c(\"Anakena\", \"Manavai\", \"Vaihu\"), bty = \"n\")\n\n\n\n\n\nWe have added the original observations as ticks of the appropriate colour, and back-transformed the densities to allow fair visual comparisons between the fitted distributions and the original data using:\n\\[ x_{i} = \\frac{x^*_in-0.5}{(n-1)} \\]\nNote that the vertical positioning of the dots is merely to prevent overplotting.\nFrom this plot we can see that the model does a reasonable job of fitting beta distributions to each of the treatment levels… And that likewise, the variance of the groups …. This can be confirmed with a residual plot, using residuals calculated relative to their predicted variance.\n\nplot(resid(p_cover.bm1) ~ fitted(p_cover.bm1))\n\n\n\n\nIs this statement accurate?\nThe spread of the standardized residuals is strongly related to the fitted values, suggesting that variance is not being adequately modeled. This observation suggests the possible utility of allowing for the precision parameter \\(\\phi\\) to vary between treatment groups. The following section will show extension of the beta regression model to allow for this.\n\n\nVariable precision \\(\\phi\\)\nWe can repeat the above analysis using a model that allows \\(\\phi\\) to vary with predictors. This is achieved by adding a second part to the right hand side of the formula, separated with the | symbol. All covariates to the right of this | symbol will be used to model \\(\\phi\\). Note that they do not have to be the same covariates used to model \\(\\mu\\) (specified to the left of the |).\n\np_cover.bm2 <- betareg(pland_decimal_scaled ~ Site | Site, data = p_cover)\n\nsummary(p_cover.bm2)\n\n\nCall:\nbetareg(formula = pland_decimal_scaled ~ Site | Site, data = p_cover)\n\nStandardized weighted residuals 2:\n    Min      1Q  Median      3Q     Max \n-6.1960 -0.5649 -0.3417  0.7115  1.8425 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -4.99928    0.08784 -56.913   <2e-16 ***\nSiteman      1.26338    0.13630   9.269   <2e-16 ***\nSitevhu      4.09928    0.09194  44.584   <2e-16 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   4.3427     0.1141  38.052  < 2e-16 ***\nSiteman      -1.6249     0.1679  -9.680  < 2e-16 ***\nSitevhu      -1.1097     0.1441  -7.699 1.37e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  2136 on 6 Df\nPseudo R-squared: 0.6791\nNumber of iterations: 22 (BFGS) + 1 (Fisher scoring) \n\n\nFrom the Coefficients table we see that the estimate for \\(\\mu\\) in the Control treatment is inv.logit(-4.1313) = X = X% coral cover. Moreover, the estimates of \\(\\mu\\) for the other two Sites (treatments) are each significantly higher.\nFrom the Phi coefficients table we can see that the maximum likelihood estimate of the precision is highest in the Control treatment and is reduced significantly relative to this baseline in each of other treatment groups. In other words, the model fit confirms our impression from the previous two graphs that a fixed \\(\\phi\\) model overestimates variance in the Control treatment, and underestimates it in the other three treatments.\nWe can use likelihood-ratio tests to compare the new model to the fixed-\\(\\phi\\) and null models.\n\nlmtest::lrtest(p_cover.bm1, p_cover.bm2)\n\nLikelihood ratio test\n\nModel 1: pland_decimal_scaled ~ Site\nModel 2: pland_decimal_scaled ~ Site | Site\n  #Df LogLik Df  Chisq Pr(>Chisq)    \n1   4 2093.7                         \n2   6 2135.7  2 84.034  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlmtest::lrtest(p_cover.bmnull, p_cover.bm1, p_cover.bm2)\n\nLikelihood ratio test\n\nModel 1: pland_decimal_scaled ~ 1\nModel 2: pland_decimal_scaled ~ Site\nModel 3: pland_decimal_scaled ~ Site | Site\n  #Df LogLik Df    Chisq Pr(>Chisq)    \n1   2 1511.0                           \n2   4 2093.7  2 1165.334  < 2.2e-16 ***\n3   6 2135.7  2   84.034  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe likelihood ratio tests indicate that the model with varying \\(\\phi\\) is significantly better than both the previous fixed \\(\\phi\\) model, and the null model. In this case the conclusion is that including the model for \\(\\phi\\) led to a better fitting model than both the fixed \\(\\phi\\) model and the null model.\nIt is possible to apply post-hoc tests to identify which pariwise contrasts of treatments levels are significant.\n\ntest(pairs(emmeans(p_cover.bm2, ~ Site, mode = \"link\")))\n\n contrast  estimate     SE  df z.ratio p.value\n ana - man    -1.26 0.1363 Inf  -9.269  <.0001\n ana - vhu    -4.10 0.0919 Inf -44.584  <.0001\n man - vhu    -2.84 0.1077 Inf -26.331  <.0001\n\nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nWe conclude from this analysis that each Site treatment is significantly different to each other. This is an important point: correct modeling of \\(\\phi\\) can often be important for accurate inference on \\(\\mu\\).\nResidual plots confirm our conclusion that p_cover.bm2 provides a better fit to the observed data than p_cover.bm1. This is seen by the more even spread of residuals in the second plot below.\n\n\npar(mfrow = c(2, 1), oma = c(0, 0, 0, 0), mar = c(4, 4, 0.2, 0.2))\nplot(residuals(p_cover.bm1) ~ fitted(p_cover.bm1))\nplot(residuals(p_cover.bm2) ~ fitted(p_cover.bm2))\n\n\n\n\n\nAs above we can plot the MLE distributions for each of the treatments, based on the variable \\(\\phi\\) model:\n\n\n# plot distributions\nmuphi.bm2 <- unique(data.frame(\n  mu = fitted(p_cover.bm2),\n  phi = predict(p_cover.bm2, type = \"precision\"),\n  treatment = p_cover$Site\n))\n\n\n\n\nplot(x.range.bt , dbeta2(x.range, muphi.bm2[1, 1], muphi.bm2[1, 2]),\n     type=\"l\",\n     xlab = \"Proportion cover\", ylab = \"Probability density\",\n     lty = 2, lwd = 2)\n\nfor (i in 2:3) {\n  lines(x.range.bt, dbeta2(x.range, muphi.bm2[i, 1], muphi.bm2[i, 2]), col = c(\"black\", \"red\", \"blue\")[i], lty = 1, lwd = 2)\n}\n\nlegend(\"topright\", lwd = 2, lty = c(2, 1, 1, 1), col = c(\"black\", \"red\", \"blue\"), legend = c(\"Anakena\", \"Manavai\", \"Vaihu\"), bty = \"n\")\n\n\n\n\n\nDue to the much narrower variance of the Control treatment group in this model, the probability density plots of the other treatments are rather distorted. The graph below rescales the Y axis for comparison to the fixed \\(\\phi\\) model above.\n\n\nplot(x.range.bt , dbeta2(x.range, muphi.bm2[1, \"mu\"], muphi.bm2[1, \"phi\"]),\n     type=\"l\",\n     xlab = \"Proportion cover\", ylab = \"Probability density\",\n     lty = 2, lwd = 2, ylim = c(0,10))\n\nfor (i in 2:3) {\n  lines(x.range.bt, dbeta2(x.range, muphi.bm2[i, \"mu\"], muphi.bm2[i, \"phi\"]), col = c(\"black\", \"red\", \"blue\")[i], lty = 1, lwd = 2)\n}\n  \nlegend(\"topright\", lwd = 2, lty = c(2, 1, 1, 1), col = c(\"black\", \"red\", \"blue\"), legend = c(\"Anakena\", \"Manavai\", \"Vaihu\"), bty = \"n\")\n\nrug(p_cover$pland_decimal[p_cover$Site == \"ana\"], lwd = 1.5, pos = 10)\n\nrug(p_cover$pland_decimal[p_cover$Site == \"man\"], col = \"red\", pos = 9.75, side = 3,lwd = 1.5)\n\nrug(p_cover$pland_decimal[p_cover$Site == \"vhu\"], col = \"blue\", pos = 9.5, side = 3, lwd = 1.5)\n\n\n\n\n\nThese plots support the conclusion from the likelihood ratio tests above. The best-fit distributions from the variable \\(\\phi\\) model better match the observed differences in dispersion between the different groups.\n\npoci_cover <-\np_cover %>%\n  as_tibble() %>%\n#  mutate(size_cm = area*10000) %>%\n  group_by(Site) %>%\n  dplyr::summarize(mean = mean(pland_decimal), \n                   sd = sd(pland_decimal), \n                   n = n(),\n                   se = sd/sqrt(n)\n  ) %>%\n  mutate(se = sd / sqrt(n),\n         lower.ci = mean - qt(1 - (0.05 / 2), n - 1) * se,\n         upper.ci = mean + qt(1 - (0.05 / 2), n - 1) * se) %>%\n  mutate_at(vars(Site), factor) %>%\n  add_column(\n          location = c('Anakena', 'Manavai', 'Southeast')\n          ) %>%\n  mutate_at(vars(location), factor)\n\n\npoci_cover2 <- \npoci_cover %>%\n  add_column(\n          cld = c('a', 'b', 'c')\n          ) %>%\nmutate_at(vars(cld), factor)\n\n\npoci_cover2\n\n# A tibble: 3 × 9\n  Site     mean     sd     n       se lower.ci upper.ci location  cld  \n  <fct>   <dbl>  <dbl> <int>    <dbl>    <dbl>    <dbl> <fct>     <fct>\n1 ana   0.00602 0.0110   250 0.000696  0.00465  0.00739 Anakena   a    \n2 man   0.0224  0.0423   250 0.00268   0.0172   0.0277  Manavai   b    \n3 vhu   0.290   0.0821   250 0.00519   0.280    0.300   Southeast c    \n\n\n\nx_labels = c(\"North\", \"West\", \"Southeast\")\n# label_names = c(\"8 m\" = \"8 m\", \"15 m\" = \"15 m\", \"25 m\" = \"25 m\")\n\n\npoci_cover.ggbarplot <- ggplot(poci_cover2, aes(x = location, y = mean, fill = x_labels)) +   \n  geom_bar(stat = \"identity\", width = 0.75, color = \"black\", linewidth = 0.50, alpha = 0.6) +\n  geom_linerange(aes(ymin = lower.ci, ymax = upper.ci), size = 0.75) +\n  scale_y_continuous(expression(paste(\"Mean Percent Cover (%)\")), limits = c(0, 1)) + \n  scale_x_discrete(expand = c(0, 1), labels = x_labels) + \n  scale_fill_manual(values = c(\"#FFC74E\", \"#82A5C0\", \"#ABC178\")) +\n#  facet_wrap( ~ depth2, labeller = as_labeller(label_names), dir = \"v\", ncol = 1) + \n  ggtitle(expression(paste(italic(\" Pocillopora \"), \"spp.\"))) +\n  geom_text(aes(label = cld, y = upper.ci), vjust = -0.5) +\n  #scale_y_log10(expression(paste(\"Colony Size (\", cm^2, \")\"), limits = c(0, 100000))) +\n  labs(x = NULL) +\n  theme(strip.text = element_text(size = 10, color = \"black\", hjust = 0.50),\n        strip.background = element_rect(fill = \"#FFFFFF\", color = NA),    \n        panel.background = element_rect(fill = \"#FFFFFF\", color = NA),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_line(color = \"#b2b2b2\"),\n        panel.spacing.x = unit(1, \"cm\"),\n        panel.spacing.y = unit(0.5, \"cm\"),\n        panel.spacing = unit(1, \"lines\"),\n        axis.ticks = element_blank(),\n        legend.position = 'none',\n        plot.title = element_text(size = 11),\n        axis.title.y = element_text(size = 11),\n        legend.title = element_blank())\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\npoci_cover.ggbarplot"
  },
  {
    "objectID": "content/publishing.html",
    "href": "content/publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "To get your Quarto webpage to show up with the url\nyou have a few steps."
  },
  {
    "objectID": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "href": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "title": "Publishing",
    "section": "Turn on GitHub Pages for your repo",
    "text": "Turn on GitHub Pages for your repo\n\nTurn on GitHub Pages under Settings > Pages . You will set pages to be made from the gh-pages branch and the root directory.\nTurn on GitHub Actions under Settings > Actions > General\n\nThe GitHub Action will automatically recreate your website when you push to GitHub after you do the initial gh-pages set-up"
  },
  {
    "objectID": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "href": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "title": "Publishing",
    "section": "Do your first publish to gh-pages",
    "text": "Do your first publish to gh-pages\nThe first time you publish to gh-pages, you need to do so locally.\n\nOn your local computer, open a terminal window and cd to your repo directory. Here is what that cd command looks like for me. You command will look different because your local repo will be somewhere else on your computer.\n\ncd ~/Documents/GitHub/NOAA-quarto-simple\n\nPublish to the gh-pages. In the terminal type\n\nquarto publish gh-pages\nThis is going to render your webpage and then push the _site contents to the gh-pages branch."
  },
  {
    "objectID": "content/publishing.html#dont-like-using-gh-pages",
    "href": "content/publishing.html#dont-like-using-gh-pages",
    "title": "Publishing",
    "section": "Don’t like using gh-pages?",
    "text": "Don’t like using gh-pages?\nIn some cases, you don’t want your website on the gh-pages branch. For example, if you are creating releases and you want the website pages archived in that release, then you won’t want your website pages on the gh-pages branch.\nHere are the changes you need to make if you to avoid gh-pages branch.\n\nAt the top of _quarto.yml add the following:\n\nproject: \n  type: website\n  output-dir: docs\n\nOn GitHub under Settings > Pages set pages to be made from the main branch and the docs directory.\nMake sure docs is not listed in .gitignore\nPublish the site the first time locally using quarto publish from the terminal\nChange the GitHub Action because you can’t use quarto publish gh-pages. You’ll need to push to the main branch yourself (in the GitHub Action)\n\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2 \n        \n      - name: Set up R (needed for Rmd)\n        uses: r-lib/actions/setup-r@v2\n\n      - name: Install packages (needed for Rmd)\n        run: Rscript -e 'install.packages(c(\"rmarkdown\", \"knitr\", \"jsonlite\"))'\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          # tinytex: true \n          # uncomment below and fill to pin a version\n          # version: 0.9.600\n      \n      - name: Render Quarto Project\n        uses: quarto-dev/quarto-actions/render@v2\n        with:\n          to: html\n\n      - name: Set up Git\n        run: |\n          git config --local user.email \"actions@github.com\"\n          git config --local user.name \"GitHub Actions\"\n\n      - name: Commit all changes and push\n        run: |\n          git add -A && git commit -m 'Build site' || echo \"No changes to commit\"\n          git push origin || echo \"No changes to commit\""
  },
  {
    "objectID": "content/rpn_sppa.html",
    "href": "content/rpn_sppa.html",
    "title": "SPPA",
    "section": "",
    "text": "vhu_csv <- read.csv(\"vhu_centroids.csv\")\n\n\nvhu_csv2 <- as_tibble(vhu_csv)\n\n\nvhu_csv2 \n\n# A tibble: 3,131 × 6\n   layer level class    id       x     y\n   <int> <chr> <int> <int>   <dbl> <dbl>\n 1     1 patch     0     1 0.0775   24.7\n 2     1 patch     0     2 0.0588   24.0\n 3     1 patch     0     3 0.0368   23.7\n 4     1 patch     0     4 0.0318   23.4\n 5     1 patch     0     5 0.0501   23.1\n 6     1 patch     0     6 0.0125   22.8\n 7     1 patch     0     7 0.0794   21.7\n 8     1 patch     0     8 0.00269  21.0\n 9     1 patch     0     9 0.0485   20.8\n10     1 patch     0    10 0.0824   16.7\n# ℹ 3,121 more rows\n\n\nDefine window for spatial pattern analyses based on the extent defined above\n\nvhu_window <- owin(c(0, 10), c(0, 25))\n\n\nattach(vhu_window)\n\n\nvhu_corals <- ppp(vhu_csv2$x, vhu_csv2$y, window = vhu_window)\n\nSummary information\n\nsummary(vhu_corals)\n\nPlanar point pattern:  3131 points\nAverage intensity 12.524 points per square unit\n\nCoordinates are given to 7 decimal places\n\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\n\n\nThis summary shows there are 3131 points (pocilloporid coral colony) and provides the observed, lambda. The density plot can be a helpful visualization of intensity of points across the plot. By plotting the spatial intensity this way, spatial trends in the point occurrences that may violate the assumption of homogeneous point process.\nDensity plots\n\nplot(\n  density(vhu_corals)\n)\n\n\n\n\nAlter smoothing parameter\n\nplot(\n  density(vhu_corals, 1)\n)  \n\n\n\n\nContour plot\n\ncontour(\n  density(vhu_corals, 1)\n)\n\n\n\n\nWe can also make tallies of counts of point locations based on quadrats overlaid on the plot. To determine whether these quadrat counts conform to CSR (i.e., a homogeneous Poisson process), use a simple Chi-Square test statistic.\nQuadrat counts\nCounts in 10 x 25 m quadrats\n\nvhu_Q <- quadratcount(vhu_corals, nx = 10, ny = 25)\n\nPlot\n\nplot(vhu_corals, cex = 1)\nplot(vhu_Q, add = TRUE, cex = 1)\n\n\n\n\nChi-sq test for complete spatial randomness, CSR\n\nquadrat.test(vhu_corals, nx = 10, ny = 25, method = \"Chisq\")\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  vhu_corals\nX2 = 308.08, df = 249, p-value = 0.0127\nalternative hypothesis: two.sided\n\nQuadrats: 10 by 25 grid of tiles\n\n\nThe test statistic suggests highly a non-random point pattern at the scale of the quadrat defined. Note that this test is more akin to a first-order point pattern analysis because it is based on the dispersion of points among sampling quadrats.\n\n\n\n\nman_csv <- read.csv(\"man_centroids.csv\")\n\n\nman_csv2 <- as_tibble(man_csv)\n\n\nman_csv2 \n\n# A tibble: 372 × 6\n   layer level class    id       x     y\n   <int> <chr> <int> <int>   <dbl> <dbl>\n 1     1 patch     0     1 0.0579  20.6 \n 2     1 patch     0     2 0.0812  20.5 \n 3     1 patch     0     3 0.00173 19.4 \n 4     1 patch     0     4 0.0281  12.7 \n 5     1 patch     0     5 0.0164  12.6 \n 6     1 patch     0     6 0.0550  10.8 \n 7     1 patch     0     7 0.0111   8.88\n 8     1 patch     0     8 0.0366   8.51\n 9     1 patch     0     9 0.0589  13.3 \n10     1 patch     0    10 0.0825  13.0 \n# ℹ 362 more rows\n\n\nDefine window for spatial pattern analyses based on the extent defined above\n\nman_window <- owin(c(0, 10), c(0, 25))\n\n\nattach(man_window)\n\nThe following objects are masked from vhu_window:\n\n    type, units, xrange, yrange\n\n\n\nman_corals <- ppp(man_csv2$x, man_csv2$y, window = man_window)\n\nSummary information\n\nsummary(man_corals)\n\nPlanar point pattern:  372 points\nAverage intensity 1.488 points per square unit\n\nCoordinates are given to 7 decimal places\n\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\n\n\nThis summary shows there are 3131 points (pocilloporid coral colony) and provides the observed, lambda. The density plot can be a helpful visualization of intensity of points across the plot. By plotting the spatial intensity this way, spatial trends in the point occurrences that may violate the assumption of homogeneous point process.\nDensity plots\n\nplot(\n  density(man_corals)\n)\n\n\n\n\nAlter smoothing parameter\n\nplot(\n  density(man_corals, 1)\n)  \n\n\n\n\nContour plot\n\ncontour(\n  density(man_corals, 1)\n)\n\n\n\n\nWe can also make tallies of counts of point locations based on quadrats overlaid on the plot. To determine whether these quadrat counts conform to CSR (i.e., a homogeneous Poisson process), use a simple Chi-Square test statistic.\nQuadrat counts\nCounts in 10 x 25 m quadrats\n\nman_Q <- quadratcount(man_corals, nx = 10, ny = 25)\n\nPlot\n\nplot(man_corals, cex = 1)\nplot(man_Q, add = TRUE, cex = 1)\n\n\n\n\nChi-sq test for complete spatial randomness, CSR\n\nquadrat.test(man_corals, nx = 10, ny = 25, method = \"Chisq\")\n\nWarning: Some expected counts are small; chi^2 approximation may be inaccurate\n\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  man_corals\nX2 = 1394.1, df = 249, p-value < 2.2e-16\nalternative hypothesis: two.sided\n\nQuadrats: 10 by 25 grid of tiles\n\n\n\n\n\n\nana_csv <- read.csv(\"ana_centroids.csv\")\n\n\nana_csv2 <- as_tibble(ana_csv)\n\n\nana_csv2 \n\n# A tibble: 136 × 6\n   layer level class    id     x      y\n   <int> <chr> <int> <int> <dbl>  <dbl>\n 1     1 patch     1     1 0.330 23.4  \n 2     1 patch     1     2 0.343 23.7  \n 3     1 patch     1     3 0.360  1.54 \n 4     1 patch     1     4 0.477 11.2  \n 5     1 patch     1     5 0.494 12.6  \n 6     1 patch     1     6 0.547 17.4  \n 7     1 patch     1     7 0.557  1.52 \n 8     1 patch     1     8 0.587 16.3  \n 9     1 patch     1     9 0.779  1.81 \n10     1 patch     1    10 0.749  0.270\n# ℹ 126 more rows\n\n\nDefine window for spatial pattern analyses based on the extent defined above\n\nana_window <- owin(c(0, 10), c(0, 25))\n\n\nattach(ana_window)\n\nThe following objects are masked from man_window:\n\n    type, units, xrange, yrange\n\n\nThe following objects are masked from vhu_window:\n\n    type, units, xrange, yrange\n\n\n\nana_corals <- ppp(ana_csv2$x, ana_csv2$y, window = ana_window)\n\nSummary information\n\nsummary(ana_corals)\n\nPlanar point pattern:  136 points\nAverage intensity 0.544 points per square unit\n\nCoordinates are given to 7 decimal places\n\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\n\n\nThis summary shows there are 3131 points (pocilloporid coral colony) and provides the observed, lambda. The density plot can be a helpful visualization of intensity of points across the plot. By plotting the spatial intensity this way, spatial trends in the point occurrences that may violate the assumption of homogeneous point process.\nDensity plots\n\nplot(\n  density(ana_corals)\n)\n\n\n\n\nAlter smoothing parameter\n\nplot(\n  density(ana_corals, 1)\n)  \n\n\n\n\nContour plot\n\ncontour(\n  density(ana_corals, 1)\n)\n\n\n\n\nWe can also make tallies of counts of point locations based on quadrats overlaid on the plot. To determine whether these quadrat counts conform to CSR (i.e., a homogeneous Poisson process), use a simple Chi-Square test statistic.\nQuadrat counts\nCounts in 10 x 25 m quadrats\n\nana_Q <- quadratcount(ana_corals, nx = 10, ny = 25)\n\nPlot\n\nplot(ana_corals, cex = 1)\nplot(ana_Q, add = TRUE, cex = 1)\n\n\n\n\nChi-sq test for complete spatial randomness, CSR\n\nquadrat.test(ana_corals, nx = 10, ny = 25, method = \"Chisq\")\n\nWarning: Some expected counts are small; chi^2 approximation may be inaccurate\n\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  ana_corals\nX2 = 397.09, df = 249, p-value = 1.389e-08\nalternative hypothesis: two.sided\n\nQuadrats: 10 by 25 grid of tiles\n\n\nThe test statistic suggests highly a non-random point pattern at the scale of the quadrat defined. Note that this test is more akin to a first-order point pattern analysis because it is based on the dispersion of points among sampling quadrats."
  },
  {
    "objectID": "content/rpn_sppa.html#ripleys-k-function",
    "href": "content/rpn_sppa.html#ripleys-k-function",
    "title": "SPPA",
    "section": "Ripley’s K function:",
    "text": "Ripley’s K function:\n\nSecond-order point pattern analyses can readily be implemented in ‘spatstat’.\nRipley’s K and the standard L functions\nIgnore edge effects with ‘(correction = “none”)’\n\n\nSoutheast - Vaihu\n\nK_none_vhu <- Kest(vhu_corals, rmax = 5.0, correction = \"none\")\n\n\nPlot K\n\n\nplot(K_none_vhu, legend = F, main = \"Southeast: Ripley's K\")\n\n\n\n\n\nPlot L with 1:1 expectation\n\n\nL_none_vhu <- Lest(vhu_corals, rmax = 5.0, correction = \"none\")\n\n\nplot(L_none_vhu, legend = F, main = \"Southeast: standardized L function (standardized 1:1)\")\n\n\n\n\n\nPlot L with 0 expectation\n\n\nplot(L_none_vhu, . - r ~ r, legend = F, main = \"Southeast: standardized L function (standardized 0)\")\n\n\n\n\nThe above analysis ignores the problem of edge effects. spatstat provides a variety of edge corrections. Contrast an (1) isotropic and (2) translate correction for adjusting for boundary effects. The isotropic correction uses a simple weighting for the area sampled near the plot boundary (Ripley 1988), the translate correction uses a toroidal shift. We adjust for potential boundary effects by typing:\n\nIsotropic edge correction\n\n\nL_iso_vhu <- Lest(vhu_corals, rmax = 5.0, correction = \"isotropic\")\n\n\nplot(L_iso_vhu, . - r ~ r, legend = F, main = \"Southeast: standardzied L (isotropic correction)\")\n\n\n\n\n\nTranslate (toroidal) edge correction\n\n\nL_trans_vhu <- Lest(vhu_corals, rmax = 5.0, correction = \"trans\")\n\n\nplot(L_trans_vhu, . - r ~ r, legend = F, main = \"Southeast: standardzied L (translate correction)\")"
  },
  {
    "objectID": "content/rpn_sppa.html#monte-carlo-simulations-to-calculate-a-global-and-pointwise-confidence-envelope-under-csr",
    "href": "content/rpn_sppa.html#monte-carlo-simulations-to-calculate-a-global-and-pointwise-confidence-envelope-under-csr",
    "title": "SPPA",
    "section": "Monte Carlo simulations to calculate a global and pointwise confidence envelope under CSR",
    "text": "Monte Carlo simulations to calculate a global and pointwise confidence envelope under CSR\n\nL_csr_vhu <- envelope(vhu_corals, rmax = 5.0, Lest, nsim = 99, rank = 1, correction = \"trans\", global = F)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nL_csr.g_vhu <- envelope(vhu_corals, rmax = 5.0, Lest, nsim = 99, rank = 1, correction = \"trans\", global = T)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nPlot point-wise envelope\n\n\nplot(L_csr_vhu, . - r ~ r, shade = c(\"hi\", \"lo\"), legend = F, main = \"Southeast: stand. L (Monte Carlo, CSR env)\")\n\n\n\n\n\nPlot global envelope\n\n\nplot(L_csr.g_vhu, . - r ~ r, shade = c(\"hi\", \"lo\"), legend = F, main = \"Southeast: stand. L (Monte Carlo, CSR env)\")\n\n\n\n\n\nWest - Manavai\n\nK_none_man <- Kest(man_corals, rmax = 5.0, correction = \"none\")\n\n\nPlot K\n\n\nplot(K_none_man, legend = F, main = \"West: Ripley's K\")\n\n\n\n\n\nPlot L with 1:1 expectation\n\n\nL_none_man <- Lest(man_corals, rmax = 5.0, correction = \"none\")\n\n\nplot(L_none_man, legend = F, main = \"West: standardized L function (standardized 1:1)\")\n\n\n\n\n\nPlot L with 0 expectation\n\n\nplot(L_none_man, . - r ~ r, legend = F, main = \"West: standardized L function (standardized 0)\")\n\n\n\n\nThe above analysis ignores the problem of edge effects. spatstat provides a variety of edge corrections. Contrast an (1) isotropic and (2) translate correction for adjusting for boundary effects. The isotropic correction uses a simple weighting for the area sampled near the plot boundary (Ripley 1988), the translate correction uses a toroidal shift. We adjust for potential boundary effects by typing:\n\nIsotropic edge correction\n\n\nL_iso_man <- Lest(man_corals, rmax = 5.0, correction = \"isotropic\")\n\n\nplot(L_iso_man, . - r ~ r, legend = F, main = \"West: standardzied L (isotropic correction)\")\n\n\n\n\n\nTranslate (toroidal) edge correction\n\n\nL_trans_man <- Lest(man_corals, rmax = 5.0, correction = \"trans\")\n\n\nplot(L_trans_man, . - r ~ r, legend = F, main = \"West: standardzied L (translate correction)\")\n\n\n\n\n\n\nNorth - Anakena\n\nK_none_ana <- Kest(ana_corals, rmax = 5.0, correction = \"none\")\n\n\nPlot K\n\n\nplot(K_none_ana, legend = F, main = \"North: Ripley's K\")\n\n\n\n\n\nPlot L with 1:1 expectation\n\n\nL_none_ana <- Lest(ana_corals, rmax = 5.0, correction = \"none\")\n\n\nplot(L_none_ana, legend = F, main = \"North: standardized L function (standardized 1:1)\")\n\n\n\n\n\nPlot L with 0 expectation\n\n\nplot(L_none_ana, . - r ~ r, legend = F, main = \"North: standardized L function (standardized 0)\")\n\n\n\n\nThe above analysis ignores the problem of edge effects. spatstat provides a variety of edge corrections. Contrast an (1) isotropic and (2) translate correction for adjusting for boundary effects. The isotropic correction uses a simple weighting for the area sampled near the plot boundary (Ripley 1988), the translate correction uses a toroidal shift. We adjust for potential boundary effects by typing:\n\nIsotropic edge correction\n\n\nL_iso_ana <- Lest(ana_corals, rmax = 5.0, correction = \"isotropic\")\n\n\nplot(L_iso_ana, . - r ~ r, legend = F, main = \"North: standardzied L (isotropic correction)\")\n\n\n\n\n\nTranslate (toroidal) edge correction\n\n\nL_trans_ana <- Lest(ana_corals, rmax = 5.0, correction = \"trans\")\n\n\nplot(L_trans_ana, . - r ~ r, legend = F, main = \"North: standardzied L (translate correction)\")\n\n\n\n\nFor the functions above, two lines are drawn. The \\(L_{pois}\\) line is a dashed line that represents the expected (theoretical) value based on a Poisson process (CSR). The way that spatstat calculates \\(L\\) is to linearize \\(K\\) such that the expected value is \\(r\\) (or the radius). The other solid line represents the estimated \\(L\\) (linearized \\(K\\)), when the edges are ignored.\nWhen comparing the \\(L\\) function that ignores boundaries to those above that account for boundaries, notice that patterns change at larger distances - we expect that the \\(L\\) function at larger distances should potentially be more biased than at smaller distances because larger radii will naturally overlap more with the boundary of the study area.\nWhen edge effects are ignored, the effect in the of counting fewer points within the radius \\(r\\) near the boundary, so the observed value for \\(L\\) or \\(K\\) should have an artifact of decreasing as \\(r\\) increases.\nThe analyses so far are exploratory. While the observed statistics (\\(K\\), \\(L\\)) appear different than the expectation, it is unclear if these are substantially (or significantly) different.\nTo conduct formal inference regarding if the point pattern follows CSR, we can use Monte Carlo simulations ro calculate a confidence envelope under CSR with the envelope function.\nIn the envelope function, rank specifies the alpha for the simulations. For a rank = 1, the max an min are used as the envelopes, such that for 99 simulations, alpha = 0.01 while for 19 simulations, alpha = 0.05.\nAlso not that we used global = FALSE. This means that these are pointwise envelopes.\nThese envelopes work better for \\(L\\) than \\(K\\) because of variance stabilizing properties.\nPlots of pointwise envelopes show the stated upper and lower quantiles of simulated patterns for any distance r. Because such analyses are calculating envelopes for vhuy distances, pointwise envelopes with a specified alpha should not be used to reject a null model at that level (because of the multiple tests). Consequently, there are alternative global tests that can be used in this way. While global tests are under active development (Baddeley et al. 2014; Wiegand et al. 2016), spatstat does provide one option for a global test (using global = T).\nThis approach estimates the maximum deviation from the Poisson point process across all r (i.e., \\(D = max|K_{(r)} - K_{pois(r)}|)\\). This approach is referred to as a simultaneous envelope (or critical band) rather than a pointwise envelope.\nIf the observed line falls outside the simultaneous envelope at any point on \\(r\\), we would reject the null hypothesis."
  },
  {
    "objectID": "content/dispersion_presentation.html#different-patterns-of-dispersion",
    "href": "content/dispersion_presentation.html#different-patterns-of-dispersion",
    "title": "Pocilloporid Dispersion at Rapa Nui",
    "section": "Different patterns of dispersion",
    "text": "Different patterns of dispersion\n\nEnvironmental heterogeneity"
  },
  {
    "objectID": "content/rendering.html",
    "href": "content/rendering.html",
    "title": "Rendering",
    "section": "",
    "text": "The repo includes a GitHub Action that will render (build) the website automatically when you make changes to the files. It will be pushed to the gh-pages branch.\nBut when you are developing your content, you will want to render it locally."
  },
  {
    "objectID": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "href": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "title": "Rendering",
    "section": "Step 1. Make sure you have a recent RStudio",
    "text": "Step 1. Make sure you have a recent RStudio\nHave you updated RStudio since about August 2022? No? Then update to a newer version of RStudio. In general, you want to keep RStudio updated and it is required to have a recent version to use Quarto."
  },
  {
    "objectID": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "href": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "title": "Rendering",
    "section": "Step 2. Clone and create RStudio project",
    "text": "Step 2. Clone and create RStudio project\nFirst, clone the repo onto your local computer. How? You can click File > New Project and then select “Version Control”. Paste in the url of the repository. That will clone the repo on to your local computer. When you make changes, you will need to push those up."
  },
  {
    "objectID": "content/rendering.html#step-3.-render-within-rstudio",
    "href": "content/rendering.html#step-3.-render-within-rstudio",
    "title": "Rendering",
    "section": "Step 3. Render within RStudio",
    "text": "Step 3. Render within RStudio\nRStudio will recognize that this is a Quarto project by the presence of the _quarto.yml file and will see the “Build” tab. Click the “Render website” button to render to the _site folder.\nPreviewing: You can either click index.html in the _site folder and specify “preview in browser” or set up RStudio to preview to the viewer panel. To do the latter, go to Tools > Global Options > R Markdown. Then select “Show output preview in: Viewer panel”."
  },
  {
    "objectID": "content/code.html",
    "href": "content/code.html",
    "title": "Rendering with Code",
    "section": "",
    "text": "You can have code (R, Python or Julia) in your qmd file. You will need to have these installed on your local computer, but presumably you do already if you are adding code to your qmd files."
  },
  {
    "objectID": "content/code.html#modify-the-github-action",
    "href": "content/code.html#modify-the-github-action",
    "title": "Rendering with Code",
    "section": "Modify the GitHub Action",
    "text": "Modify the GitHub Action\nYou will need to change the GitHub Action in .github/workflows to install these and any needed packages in order for GitHub to be able to render your webpage. The GitHub Action install R since I used that in code.qmd. If you use Python or Julia instead, then you will need to update the GitHub Action to install those.\nIf getting the GitHub Action to work is too much hassle (and that definitely happens), you can alway render locally and publish to the gh-pages branch. If you do this, make sure to delete or rename the GitHub Action to something like\nrender-and-publish.old_yml\nso GitHub does not keep trying to run it. Nothing bad will happen if you don’t do this, but if you are not using the action (because it keeps failing), then you don’t need GitHub to run it."
  },
  {
    "objectID": "content/code.html#render-locally-and-publish-to-gh-pages-branch",
    "href": "content/code.html#render-locally-and-publish-to-gh-pages-branch",
    "title": "Rendering with Code",
    "section": "Render locally and publish to gh-pages branch",
    "text": "Render locally and publish to gh-pages branch\nTo render locally and push up to the gh-pages branch, open a terminal window and then cd to the directory with the Quarto project. Type this in the terminal:\nquarto render gh-pages"
  },
  {
    "objectID": "content/acknowledgements.html",
    "href": "content/acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "This repo and GitHub Action was based on the tutorial by Openscapes quarto-website-tutorial by Julia Lowndes and Stefanie Butland."
  },
  {
    "objectID": "content/rmarkdown.html",
    "href": "content/rmarkdown.html",
    "title": "R Markdown",
    "section": "",
    "text": "You can include R Markdown files in your project."
  },
  {
    "objectID": "content/rmarkdown.html#r-markdown",
    "href": "content/rmarkdown.html#r-markdown",
    "title": "R Markdown",
    "section": "R Markdown",
    "text": "R Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "content/rmarkdown.html#including-plots",
    "href": "content/rmarkdown.html#including-plots",
    "title": "R Markdown",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "content/rpn_sppa2.html",
    "href": "content/rpn_sppa2.html",
    "title": "SPPA",
    "section": "",
    "text": "se_csv <- read.csv(\"vhu_centroids.csv\")\n\n\nse_csv2 <- as_tibble(se_csv)\n\n\nse_csv2 \n\n# A tibble: 3,131 × 6\n   layer level class    id       x     y\n   <int> <chr> <int> <int>   <dbl> <dbl>\n 1     1 patch     0     1 0.0775   24.7\n 2     1 patch     0     2 0.0588   24.0\n 3     1 patch     0     3 0.0368   23.7\n 4     1 patch     0     4 0.0318   23.4\n 5     1 patch     0     5 0.0501   23.1\n 6     1 patch     0     6 0.0125   22.8\n 7     1 patch     0     7 0.0794   21.7\n 8     1 patch     0     8 0.00269  21.0\n 9     1 patch     0     9 0.0485   20.8\n10     1 patch     0    10 0.0824   16.7\n# ℹ 3,121 more rows\n\n\n\n\n\n\nwest_csv <- read.csv(\"man_centroids.csv\")\n\n\nwest_csv2 <- as_tibble(west_csv)\n\n\nwest_csv2 \n\n# A tibble: 372 × 6\n   layer level class    id       x     y\n   <int> <chr> <int> <int>   <dbl> <dbl>\n 1     1 patch     0     1 0.0579  20.6 \n 2     1 patch     0     2 0.0812  20.5 \n 3     1 patch     0     3 0.00173 19.4 \n 4     1 patch     0     4 0.0281  12.7 \n 5     1 patch     0     5 0.0164  12.6 \n 6     1 patch     0     6 0.0550  10.8 \n 7     1 patch     0     7 0.0111   8.88\n 8     1 patch     0     8 0.0366   8.51\n 9     1 patch     0     9 0.0589  13.3 \n10     1 patch     0    10 0.0825  13.0 \n# ℹ 362 more rows\n\n\n\n\n\n\nnorth_csv <- read.csv(\"ana_centroids.csv\")\n\n\nnorth_csv2 <- as_tibble(north_csv)\n\n\nnorth_csv2 \n\n# A tibble: 136 × 6\n   layer level class    id     x      y\n   <int> <chr> <int> <int> <dbl>  <dbl>\n 1     1 patch     1     1 0.330 23.4  \n 2     1 patch     1     2 0.343 23.7  \n 3     1 patch     1     3 0.360  1.54 \n 4     1 patch     1     4 0.477 11.2  \n 5     1 patch     1     5 0.494 12.6  \n 6     1 patch     1     6 0.547 17.4  \n 7     1 patch     1     7 0.557  1.52 \n 8     1 patch     1     8 0.587 16.3  \n 9     1 patch     1     9 0.779  1.81 \n10     1 patch     1    10 0.749  0.270\n# ℹ 126 more rows\n\n\nSoutheast Define window for spatial pattern analyses based on the extent defined above\n\nse_window <- owin(c(0, 10), c(0, 25))\n\n\nattach(se_window)\n\n\nse_corals <- ppp(se_csv2$x, se_csv2$y, window = se_window)\n\nSummary information\n\nsummary(se_corals)\n\nPlanar point pattern:  3131 points\nAverage intensity 12.524 points per square unit\n\nCoordinates are given to 7 decimal places\n\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\n\n\nThis summary shows there are 3131 points (pocilloporid coral colony) and provides the observed, lambda. The density plot can be a helpful visualization of intensity of points across the plot. By plotting the spatial intensity this way, spatial trends in the point occurrences that may violate the assumption of homogeneous point process.\nWest Define window for spatial pattern analyses based on the extent defined above\n\nwest_window <- owin(c(0, 10), c(0, 25))\n\n\nattach(west_window)\n\nThe following objects are masked from se_window:\n\n    type, units, xrange, yrange\n\n\n\nwest_corals <- ppp(west_csv2$x, west_csv2$y, window = west_window)\n\nSummary information\n\nsummary(west_corals)\n\nPlanar point pattern:  372 points\nAverage intensity 1.488 points per square unit\n\nCoordinates are given to 7 decimal places\n\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\n\n\nThis summary shows there are XXX points (pocilloporid coral colony) and provides the observed, lambda. The density plot can be a helpful visualization of intensity of points across the plot. By plotting the spatial intensity this way, spatial trends in the point occurrences that may violate the assumption of homogeneous point process.\nNorth Define window for spatial pattern analyses based on the extent defined above\n\nnorth_window <- owin(c(0, 10), c(0, 25))\n\n\nattach(north_window)\n\nThe following objects are masked from west_window:\n\n    type, units, xrange, yrange\n\n\nThe following objects are masked from se_window:\n\n    type, units, xrange, yrange\n\n\n\nnorth_corals <- ppp(north_csv2$x, north_csv2$y, window = north_window)\n\nSummary information\n\nsummary(north_corals)\n\nPlanar point pattern:  136 points\nAverage intensity 0.544 points per square unit\n\nCoordinates are given to 7 decimal places\n\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\n\n\nThis summary shows there are XXXX points (pocilloporid coral colony) and provides the observed, lambda. The density plot can be a helpful visualization of intensity of points across the plot. By plotting the spatial intensity this way, spatial trends in the point occurrences that may violate the assumption of homogeneous point process."
  },
  {
    "objectID": "content/rpn_sppa2.html#density-plots",
    "href": "content/rpn_sppa2.html#density-plots",
    "title": "SPPA",
    "section": "Density Plots",
    "text": "Density Plots\n\nplot(\n  density(se_corals)\n)\n\n\n\n\n\nplot(\n  density(west_corals)\n)\n\n\n\n\n\nplot(\n  density(north_corals)\n)\n\n\n\n\nAlter smoothing parameter\n\nplot(\n  density(se_corals, 1)\n)  \n\n\n\n\n\nplot(\n  density(west_corals, 1)\n)  \n\n\n\n\n\nplot(\n  density(north_corals, 1)\n)  \n\n\n\n\nContour plot\n\ncontour(\n  density(se_corals, 1)\n)\n\n\n\n\n\ncontour(\n  density(west_corals, 1)\n)\n\n\n\n\n\ncontour(\n  density(north_corals, 1)\n)\n\n\n\n\nWe can also make tallies of counts of point locations based on quadrats overlaid on the plot. To determine whether these quadrat counts conform to CSR (i.e., a homogeneous Poisson process), use a simple Chi-Square test statistic.\nQuadrat counts\nCounts in 10 x 25 m quadrats\n\nse_Q <- quadratcount(se_corals, nx = 10, ny = 25)\n\nPlot\n\nplot(se_corals, cex = 1)\nplot(se_Q, add = TRUE, cex = 1)\n\n\n\n\nChi-sq test for complete spatial randomness, CSR\n\nquadrat.test(se_corals, nx = 10, ny = 25, method = \"Chisq\")\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  se_corals\nX2 = 308.08, df = 249, p-value = 0.0127\nalternative hypothesis: two.sided\n\nQuadrats: 10 by 25 grid of tiles\n\n\nQuadrat counts\nCounts in 10 x 25 m quadrats\n\nwest_Q <- quadratcount(west_corals, nx = 10, ny = 25)\n\nPlot\n\nplot(west_corals, cex = 1)\nplot(west_Q, add = TRUE, cex = 1)\n\n\n\n\nChi-sq test for complete spatial randomness, CSR\n\nquadrat.test(west_corals, nx = 10, ny = 25, method = \"Chisq\")\n\nWarning: Some expected counts are small; chi^2 approximation may be inaccurate\n\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  west_corals\nX2 = 1394.1, df = 249, p-value < 2.2e-16\nalternative hypothesis: two.sided\n\nQuadrats: 10 by 25 grid of tiles\n\n\nThe test statistic suggests highly a non-random point pattern at the scale of the quadrat defined. Note that this test is more akin to a first-order point pattern analysis because it is based on the dispersion of points among sampling quadrats.\nQuadrat counts\nCounts in 10 x 25 m quadrats\n\nnorth_Q <- quadratcount(north_corals, nx = 10, ny = 25)\n\nPlot\n\nplot(north_corals, cex = 1)\nplot(north_Q, add = TRUE, cex = 1)\n\n\n\n\nChi-sq test for complete spatial randomness, CSR\n\nquadrat.test(north_corals, nx = 10, ny = 25, method = \"Chisq\")\n\nWarning: Some expected counts are small; chi^2 approximation may be inaccurate\n\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  north_corals\nX2 = 397.09, df = 249, p-value = 1.389e-08\nalternative hypothesis: two.sided\n\nQuadrats: 10 by 25 grid of tiles\n\n\nThe test statistic suggests highly a non-random point pattern at the scale of the quadrat defined. Note that this test is more akin to a first-order point pattern analysis because it is based on the dispersion of points among sampling quadrats."
  },
  {
    "objectID": "content/rpn_sppa2.html#ripleys-k-function",
    "href": "content/rpn_sppa2.html#ripleys-k-function",
    "title": "SPPA",
    "section": "Ripley’s K function:",
    "text": "Ripley’s K function:\n\nSecond-order point pattern analyses can readily be implemented in ‘spatstat’.\nRipley’s K and the standard L functions\nIgnore edge effects with ‘(correction = “none”)’\n\n\nSoutheast - Vaihu\n\nK_none_se <- Kest(se_corals, rmax = 5.0, correction = \"none\")\n\n\nPlot K\n\n\nplot(K_none_se, legend = F, main = \"Southeast: Ripley's K\")\n\n\n\n\n\nPlot L with 1:1 expectation\n\n\nL_none_se <- Lest(se_corals, rmax = 5.0, correction = \"none\")\n\n\nplot(L_none_se, legend = F, main = \"Southeast: standardized L function (standardized 1:1)\")\n\n\n\n\n\nPlot L with 0 expectation\n\n\nplot(L_none_se, . - r ~ r, legend = F, main = \"Southeast: standardized L function (standardized 0)\")\n\n\n\n\n\n\nWest - Manavai\n\nK_none_west <- Kest(west_corals, rmax = 5.0, correction = \"none\")\n\n\nPlot K\n\n\nplot(K_none_west, legend = F, main = \"West: Ripley's K\")\n\n\n\n\n\nPlot L with 1:1 expectation\n\n\nL_none_west <- Lest(west_corals, rmax = 5.0, correction = \"none\")\n\n\nplot(L_none_west, legend = F, main = \"West: standardized L function (standardized 1:1)\")\n\n\n\n\n\nPlot L with 0 expectation\n\n\nplot(L_none_west, . - r ~ r, legend = F, main = \"West: standardized L function (standardized 0)\")\n\n\n\n\n\n\nNorth - Anakena\n\nK_none_north <- Kest(north_corals, rmax = 5.0, correction = \"none\")\n\n\nPlot K\n\n\nplot(K_none_north, legend = F, main = \"North: Ripley's K\")\n\n\n\n\n\nPlot L with 1:1 expectation\n\n\nL_none_north <- Lest(north_corals, rmax = 5.0, correction = \"none\")\n\n\nplot(L_none_north, legend = F, main = \"North: standardized L function (standardized 1:1)\")\n\n\n\n\n\nPlot L with 0 expectation\n\n\nplot(L_none_north, . - r ~ r, legend = F, main = \"North: standardized L function (standardized 0)\")\n\n\n\n\nThe above analysis ignores the problem of edge effects. spatstat provides a variety of edge corrections. Contrast an (1) isotropic and (2) translate correction for adjusting for boundary effects. The isotropic correction uses a simple weighting for the area sampled near the plot boundary (Ripley 1988), the translate correction uses a toroidal shift. We adjust for potential boundary effects by typing:\n\n\nSoutheast – Vaihu\n\nIsotropic edge correction\n\n\nL_iso_se <- Lest(se_corals, rmax = 5.0, correction = \"isotropic\")\n\n\nplot(L_iso_se, . - r ~ r, legend = F, main = \"Southeast: standardzied L (isotropic correction)\")\n\n\n\n\n\nTranslate (toroidal) edge correction\n\n\nL_trans_se <- Lest(se_corals, rmax = 5.0, correction = \"trans\")\n\n\nplot(L_trans_se, . - r ~ r, legend = F, main = \"Southeast: standardzied L (translate correction)\")"
  },
  {
    "objectID": "content/rpn_sppa2.html#monte-carlo-simulations-to-calculate-a-global-and-pointwise-confidence-envelope-under-csr",
    "href": "content/rpn_sppa2.html#monte-carlo-simulations-to-calculate-a-global-and-pointwise-confidence-envelope-under-csr",
    "title": "SPPA",
    "section": "Monte Carlo simulations to calculate a global and pointwise confidence envelope under CSR",
    "text": "Monte Carlo simulations to calculate a global and pointwise confidence envelope under CSR\n\nL_csr_se <- envelope(se_corals, rmax = 5.0, Lest, nsim = 99, rank = 1, correction = \"trans\", global = F)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nL_csr.g_se <- envelope(se_corals, rmax = 5.0, Lest, nsim = 99, rank = 1, correction = \"trans\", global = T)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nPlot point-wise envelope\n\n\nplot(L_csr_se, . - r ~ r, shade = c(\"hi\", \"lo\"), legend = F, main = \"Southeast: stand. L (Monte Carlo, CSR env)\")\n\n\n\n\n\nPlot global envelope\n\n\nplot(L_csr.g_se, . - r ~ r, shade = c(\"hi\", \"lo\"), legend = F, main = \"Southeast: stand. L (Monte Carlo, CSR env)\")\n\n\n\n\n\nWest - Manavai\n\nIsotropic edge correction\n\n\nL_iso_west <- Lest(west_corals, rmax = 5.0, correction = \"isotropic\")\n\n\nplot(L_iso_west, . - r ~ r, legend = F, main = \"West: standardzied L (isotropic correction)\")\n\n\n\n\n\nTranslate (toroidal) edge correction\n\n\nL_trans_west <- Lest(west_corals, rmax = 5.0, correction = \"trans\")\n\n\nplot(L_trans_west, . - r ~ r, legend = F, main = \"West: standardzied L (translate correction)\")"
  },
  {
    "objectID": "content/rpn_sppa2.html#monte-carlo-simulations-to-calculate-a-global-and-pointwise-confidence-envelope-under-csr-1",
    "href": "content/rpn_sppa2.html#monte-carlo-simulations-to-calculate-a-global-and-pointwise-confidence-envelope-under-csr-1",
    "title": "SPPA",
    "section": "Monte Carlo simulations to calculate a global and pointwise confidence envelope under CSR",
    "text": "Monte Carlo simulations to calculate a global and pointwise confidence envelope under CSR\n\nL_csr_se <- envelope(se_corals, rmax = 5.0, Lest, nsim = 99, rank = 1, correction = \"trans\", global = F)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nL_csr.g_se <- envelope(se_corals, rmax = 5.0, Lest, nsim = 99, rank = 1, correction = \"trans\", global = T)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nPlot point-wise envelope\n\n\nplot(L_csr_se, . - r ~ r, shade = c(\"hi\", \"lo\"), legend = F, main = \"Southeast: stand. L (Monte Carlo, CSR env)\")\n\n\n\n\n\nPlot global envelope\n\n\nplot(L_csr.g_se, . - r ~ r, shade = c(\"hi\", \"lo\"), legend = F, main = \"Southeast: stand. L (Monte Carlo, CSR env)\")\n\n\n\n\n\nNorth - Anakena\n\nIsotropic edge correction\n\n\nL_iso_north <- Lest(north_corals, rmax = 5.0, correction = \"isotropic\")\n\n\nplot(L_iso_north, . - r ~ r, legend = F, main = \"North: standardzied L (isotropic correction)\")\n\n\n\n\n\nTranslate (toroidal) edge correction\n\n\nL_trans_north <- Lest(north_corals, rmax = 5.0, correction = \"trans\")\n\n\nplot(L_trans_north, . - r ~ r, legend = F, main = \"North: standardzied L (translate correction)\")"
  },
  {
    "objectID": "content/rpn_sppa2.html#monte-carlo-simulations-to-calculate-a-global-and-pointwise-confidence-envelope-under-csr-2",
    "href": "content/rpn_sppa2.html#monte-carlo-simulations-to-calculate-a-global-and-pointwise-confidence-envelope-under-csr-2",
    "title": "SPPA",
    "section": "Monte Carlo simulations to calculate a global and pointwise confidence envelope under CSR",
    "text": "Monte Carlo simulations to calculate a global and pointwise confidence envelope under CSR\n\nL_csr_north <- envelope(north_corals, rmax = 5.0, Lest, nsim = 99, rank = 1, correction = \"trans\", global = F)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nL_csr.g_north <- envelope(north_corals, rmax = 5.0, Lest, nsim = 99, rank = 1, correction = \"trans\", global = T)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nPlot point-wise envelope\n\n\nplot(L_csr_north, . - r ~ r, shade = c(\"hi\", \"lo\"), legend = F, main = \"North: stand. L (Monte Carlo, CSR env)\")\n\n\n\n\n\nPlot global envelope\n\n\nplot(L_csr.g_north, . - r ~ r, shade = c(\"hi\", \"lo\"), legend = F, main = \"North: stand. L (Monte Carlo, CSR env)\")\n\n\n\n\nFor the functions above, two lines are drawn. The \\(L_{pois}\\) line is a dashed line that represents the expected (theoretical) value based on a Poisson process (CSR). The way that spatstat calculates \\(L\\) is to linearize \\(K\\) such that the expected value is \\(r\\) (or the radius). The other solid line represents the estimated \\(L\\) (linearized \\(K\\)), when the edges are ignored.\nWhen comparing the \\(L\\) function that ignores boundaries to those above that account for boundaries, notice that patterns change at larger distances - we expect that the \\(L\\) function at larger distances should potentially be more biased than at smaller distances because larger radii will naturally overlap more with the boundary of the study area.\nWhen edge effects are ignored, the effect in the of counting fewer points within the radius \\(r\\) near the boundary, so the observed value for \\(L\\) or \\(K\\) should have an artifact of decreasing as \\(r\\) increases.\nThe analyses so far are exploratory. While the observed statistics (\\(K\\), \\(L\\)) appear different than the expectation, it is unclear if these are substantially (or significantly) different.\nTo conduct formal inference regarding if the point pattern follows CSR, we can use Monte Carlo simulations ro calculate a confidence envelope under CSR with the envelope function.\nIn the envelope function, rank specifies the alpha for the simulations. For a rank = 1, the max an min are used as the envelopes, such that for 99 simulations, alpha = 0.01 while for 19 simulations, alpha = 0.05.\nAlso not that we used global = FALSE. This means that these are pointwise envelopes.\nThese envelopes work better for \\(L\\) than \\(K\\) because of variance stabilizing properties.\nPlots of pointwise envelopes show the stated upper and lower quantiles of simulated patterns for any distance r. Because such analyses are calculating envelopes for vhuy distances, pointwise envelopes with a specified alpha should not be used to reject a null model at that level (because of the multiple tests). Consequently, there are alternative global tests that can be used in this way. While global tests are under active development (Baddeley et al. 2014; Wiegand et al. 2016), spatstat does provide one option for a global test (using global = T).\nThis approach estimates the maximum deviation from the Poisson point process across all r (i.e., \\(D = max|K_{(r)} - K_{pois(r)}|)\\). This approach is referred to as a simultaneous envelope (or critical band) rather than a pointwise envelope.\nIf the observed line falls outside the simultaneous envelope at any point on \\(r\\), we would reject the null hypothesis.\n\nSoutheast – Vaihu\n\nPtrans_se <- pcf(se_corals, r = NULL, correction = \"translate\")\n\n\nplot(Ptrans_se)\n\n\n\n\n\nplot(Ptrans_se$r, Ptrans_se$pcf, type = \"l\", xlab = \"r\", ylab = \"g(r)\", main = \"pair correlation\")\nabline(h=1, lty=1)\n\n\n\n\n\nPenv_se <- envelope(se_corals, r = NULL, pcf, nsim = 99, rank = 1, correction =\n                       \"translate\", global = FALSE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nplot(Penv_se, shade = c(\"hi\", \"lo\"), legend = FALSE, main = \"Southeast: pair correlation function, g\")\n\n\n\n\n\n\nWest - Manavai\n\nPtrans_west <- pcf(west_corals, r = NULL, correction = \"translate\")\n\n\nplot(Ptrans_west)\n\n\n\n\n\nplot(Ptrans_west$r, Ptrans_west$pcf, type = \"l\", xlab = \"r\", ylab = \"g(r)\", main = \"pair correlation\")\nabline(h=1, lty=1)\n\n\n\n\n\nPenv_west <- envelope(west_corals, r = NULL, pcf, nsim = 99, rank = 1, correction =\n                       \"translate\", global = FALSE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nplot(Penv_west, shade = c(\"hi\", \"lo\"), legend = FALSE, main = \"Southeast: pair correlation function, g\")\n\n\n\n\n\n\nNorth - Anakena\n\nPtrans_north <- pcf(north_corals, r = NULL, correction = \"translate\")\n\n\nplot(Ptrans_north)\n\n\n\n\n\nplot(Ptrans_north$r, Ptrans_north$pcf, type = \"l\", xlab = \"r\", ylab = \"g(r)\", main = \"pair correlation\")\nabline(h = 1, lty = 1)\n\n\n\n\n\nPenv_north <- envelope(north_corals, r = NULL, pcf, nsim = 99, rank = 1, correction =\n                       \"translate\", global = FALSE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nplot(Penv_north, shade = c(\"hi\", \"lo\"), legend = FALSE, main = \"North: pair correlation function, g\")\n\n\n\n\nThe pcf function uses a smoothing kernel such that distance bins are not needed.\nThe default bandwidth coefficient (related to sigma in a Gaussian kernel) for the smoothing kernel is set to 0.15 (Stoyan and Stoyan 1994).\nWe can adjust the smoothing on the pair correlation function using the stoyan comvhud in the pcf function.\nIncreasing the value of the bandwidth coefficient (e.g., stoyan = 0.4) results in a less wiggly g function.\nFinally, we can use similar arguments for the G-function to estimate the probability of finding a nearest neighbor as a function of distance.\nspatstat uses a similar approach as above with the Gest function.\nNote that for Gest, there are subtly different ways to account for edge effects relative to above. Below we use rs, the reduced sample correction.\nWe can check the observed G-function calculated by spatstat to the cumulative distribution function of the empirical data (with the ecdf function):\n\n\nSoutheast – Vaihu\n\nGtrans_se <- Gest(se_corals, r = NULL, correction = \"rs\")\n\n\nplot(Gtrans_se, legend = F)\n\n\n\n\n\nGenv_se <- envelope(se_corals, r = NULL, Gest, nsim = 99, rank = 1, correction = \"rs\", global = FALSE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nplot(Genv_se, shade = c(\"hi\", \"lo\"), legend = FALSE, main = \"Southeast: G-function\")\n\n\n\n\n\n\nWest - Manavai\n\nGtrans_west <- Gest(west_corals, r = NULL, correction = \"rs\")\n\n\nplot(Gtrans_west, legend = F)\n\n\n\n\n\nGenv_west <- envelope(west_corals, r = NULL, Gest, nsim = 99, rank = 1, correction = \"rs\", global = FALSE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nplot(Genv_west, shade = c(\"hi\", \"lo\"), legend = FALSE, main = \"West: G-function\")\n\n\n\n\n\n\nNorth - Anakena\n\nGtrans_north <- Gest(north_corals, r = NULL, correction = \"rs\")\n\n\nplot(Gtrans_north, legend = F)\n\n\n\n\n\nGenv_north <- envelope(north_corals, r = NULL, Gest, nsim = 99, rank = 1, correction = \"rs\", global = FALSE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nplot(Genv_north, shade = c(\"hi\", \"lo\"), legend = FALSE, main = \"North: G-function\")\n\n\n\n\n\n\nAlternative Null Models\nWhile CSR is a useful starting point as a null model, in some situations we may be interested in using alternative null models. Some null models can be derived from a Poisson cluster process. Two common Poisson cluster processes considered in ecology are Matern cluster processes and Thomas cluster processes (Velazquez et al. 2016). In a Matérn cluster process, there are two types of points. The first are parent points, which have a Poisson distribution. Second, for each parent point, there are offspring points, which are independently and uniformly distributed around the parent points within a radius r. Consequently, these offspring points generate an underlying aggregated pattern. Similarly, with a Thomas process, offspring points are generated with parents but with an isotropic Gaussian distribution (similar to a Gaussian kernel). Such a process could reflect biological phenomena such as seed dispersal from parent plants. We can use these alternative null models in spatstat, with the above functions (\\(K\\), \\(L\\), pair correlation g, etc.). For example, a K function with a Thomas process as a null model can be quantified as:\n\n\nSoutheast – Vaihu\n\nKthomas_se <- kppm(se_corals, ~ 1, \"Thomas\")\n\n\nKthomas_se\n\nStationary cluster point process model\nFitted to point pattern dataset 'se_corals'\nFitted by minimum contrast\n    Summary statistic: K-function\n\nUniform intensity:  12.524\n\nCluster model: Thomas process\nFitted cluster parameters:\n     kappa      scale \n0.09841425 9.29489907 \nMean cluster size:  127.258 points\n\nCluster strength: phi =  0.009359\nSibling probability: psib =  0.009273\n\n\n\nsummary(Kthomas_se)\n\nStationary cluster point process model\nFitted to point pattern dataset 'se_corals'\nFitted by minimum contrast\n    Summary statistic: K-function\nMinimum contrast fit (object of class \"minconfit\")\nModel: Thomas process\nFitted by matching theoretical K function to se_corals\n\nInternal parameters fitted by minimum contrast ($par):\n      kappa      sigma2 \n 0.09841425 86.39514879 \n\nFitted cluster parameters:\n     kappa      scale \n0.09841425 9.29489907 \nMean cluster size:  127.258 points\n\nConverged successfully after 271 function evaluations\n\nStarting values of parameters:\n     kappa     sigma2 \n12.5240000  0.1273346 \nDomain of integration: [ 0 , 2.5 ]\nExponents: p= 2, q= 0.25\n\n----------- TREND  -----\nPoint process model\nFitted to data: X\nFitting method: maximum likelihood (Berman-Turner approximation)\nModel was fitted using glm()\nAlgorithm converged\nCall:\nppm.ppp(Q = X, trend = trend, rename.intercept = FALSE, covariates = covariates, \n    covfunargs = covfunargs, use.gam = use.gam, forcefit = TRUE, \n    improve.type = ppm.improve.type, improve.args = ppm.improve.args, \n    nd = nd, eps = eps)\nEdge correction: \"border\"\n    [border correction distance r = 0 ]\n--------------------------------------------------------------------------------\nQuadrature scheme (Berman-Turner) = data + dummy + weights\n\nData pattern:\nPlanar point pattern:  3131 points\nAverage intensity 12.5 points per square unit\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\n\nDummy quadrature points:\n     120 x 120 grid of dummy points, plus 4 corner points\n     dummy spacing: 0.08333333 x 0.20833333 units\n\nOriginal dummy parameters: =\nPlanar point pattern:  14404 points\nAverage intensity 57.6 points per square unit\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\nQuadrature weights:\n     (counting weights based on 120 x 120 array of rectangular tiles)\nAll weights:\n    range: [0.00434, 0.0174]    total: 250\nWeights on data points:\n    range: [0.00434, 0.00868]   total: 26.5\nWeights on dummy points:\n    range: [0.00434, 0.0174]    total: 223\n--------------------------------------------------------------------------------\nFITTED :\n\nStationary Poisson process\n\n---- Intensity: ----\n\n\nUniform intensity:\n[1] 12.524\n\n            Estimate      S.E. CI95.lo  CI95.hi Ztest     Zval\n(Intercept) 2.527647 0.0178714 2.49262 2.562674   *** 141.4353\n\n----------- gory details -----\n\nFitted regular parameters (theta):\n(Intercept) \n   2.527647 \n\nFitted exp(theta):\n(Intercept) \n     12.524 \n\n----------- CLUSTER  -----------\nModel: Thomas process\n\nFitted cluster parameters:\n     kappa      scale \n0.09841425 9.29489907 \nMean cluster size:  127.258 points\n\nFinal standard error and CI\n(allowing for correlation of cluster process):\n            Estimate       S.E.  CI95.lo  CI95.hi Ztest     Zval\n(Intercept) 2.527647 0.08537972 2.360306 2.694988   *** 29.60477\n\n----------- cluster strength indices ----------\nSibling probability 0.009272504\nCount overdispersion index (on original window): 22.87956\nCluster strength: 0.009359288\n\nSpatial persistence index (over window): 0.1227116\n\nBound on distance from Poisson process (over window): 1\n     = min (1, 6262, 490195.4, 93874.58, 302.9036)\n\nBound on distance from MIXED Poisson process (over window): 1\n\nIntensity of parents of nonempty clusters: 0.09841425\nMean number of offspring in a nonempty cluster: 127.258\nIntensity of parents of clusters of more than one offspring point: 0.09841425\nRatio of parents to parents-plus-offspring: 0.007796785 (where 1 = Poisson \nprocess)\nProbability that a typical point belongs to a nontrivial cluster: 1\n\n\n\nKthomas.env_se <- envelope(Kthomas_se, rmax = 5, Lest, nsim = 99, rank = 1, global = F)\n\nGenerating 99 simulated realisations of fitted cluster model  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nplot(Kthomas.env_se, . - r ~ r, shade = c(\"hi\", \"lo\"), legend = F, main = \"Southeast: Thomas Model\")\n\n\n\n\n\n\nWest - Manavai\n\nKthomas_west <- kppm(west_corals, ~ 1, \"Thomas\")\n\n\nKthomas_west\n\nStationary cluster point process model\nFitted to point pattern dataset 'west_corals'\nFitted by minimum contrast\n    Summary statistic: K-function\n\nUniform intensity:  1.488\n\nCluster model: Thomas process\nFitted cluster parameters:\n     kappa      scale \n0.03807945 0.82357753 \nMean cluster size:  39.07619 points\n\nCluster strength: phi =  3.081\nSibling probability: psib =  0.755\n\n\n\nsummary(Kthomas_west)\n\nStationary cluster point process model\nFitted to point pattern dataset 'west_corals'\nFitted by minimum contrast\n    Summary statistic: K-function\nMinimum contrast fit (object of class \"minconfit\")\nModel: Thomas process\nFitted by matching theoretical K function to west_corals\n\nInternal parameters fitted by minimum contrast ($par):\n     kappa     sigma2 \n0.03807945 0.67827996 \n\nFitted cluster parameters:\n     kappa      scale \n0.03807945 0.82357753 \nMean cluster size:  39.07619 points\n\nConverged successfully after 107 function evaluations\n\nStarting values of parameters:\n    kappa    sigma2 \n1.4880000 0.2521775 \nDomain of integration: [ 0 , 2.5 ]\nExponents: p= 2, q= 0.25\n\n----------- TREND  -----\nPoint process model\nFitted to data: X\nFitting method: maximum likelihood (Berman-Turner approximation)\nModel was fitted using glm()\nAlgorithm converged\nCall:\nppm.ppp(Q = X, trend = trend, rename.intercept = FALSE, covariates = covariates, \n    covfunargs = covfunargs, use.gam = use.gam, forcefit = TRUE, \n    improve.type = ppm.improve.type, improve.args = ppm.improve.args, \n    nd = nd, eps = eps)\nEdge correction: \"border\"\n    [border correction distance r = 0 ]\n--------------------------------------------------------------------------------\nQuadrature scheme (Berman-Turner) = data + dummy + weights\n\nData pattern:\nPlanar point pattern:  372 points\nAverage intensity 1.49 points per square unit\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\n\nDummy quadrature points:\n     40 x 40 grid of dummy points, plus 4 corner points\n     dummy spacing: 0.250 x 0.625 units\n\nOriginal dummy parameters: =\nPlanar point pattern:  1604 points\nAverage intensity 6.42 points per square unit\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\nQuadrature weights:\n     (counting weights based on 40 x 40 array of rectangular tiles)\nAll weights:\n    range: [0.0223, 0.156]  total: 250\nWeights on data points:\n    range: [0.0223, 0.0781] total: 21.7\nWeights on dummy points:\n    range: [0.0223, 0.156]  total: 228\n--------------------------------------------------------------------------------\nFITTED :\n\nStationary Poisson process\n\n---- Intensity: ----\n\n\nUniform intensity:\n[1] 1.488\n\n             Estimate       S.E.   CI95.lo   CI95.hi Ztest     Zval\n(Intercept) 0.3974329 0.05184758 0.2958135 0.4990523   *** 7.665409\n\n----------- gory details -----\n\nFitted regular parameters (theta):\n(Intercept) \n  0.3974329 \n\nFitted exp(theta):\n(Intercept) \n      1.488 \n\n----------- CLUSTER  -----------\nModel: Thomas process\n\nFitted cluster parameters:\n     kappa      scale \n0.03807945 0.82357753 \nMean cluster size:  39.07619 points\n\nFinal standard error and CI\n(allowing for correlation of cluster process):\n             Estimate    S.E.    CI95.lo  CI95.hi Ztest     Zval\n(Intercept) 0.3974329 0.30742 -0.2050991 0.999965       1.292801\n\n----------- cluster strength indices ----------\nSibling probability 0.7549615\nCount overdispersion index (on original window): 35.83397\nCluster strength: 3.080991\n\nSpatial persistence index (over window): 0\n\nBound on distance from Poisson process (over window): 1\n     = min (1, 744, 440896.2, 1683540, 652.9624)\n\nBound on distance from MIXED Poisson process (over window): 1\n\nIntensity of parents of nonempty clusters: 0.03807945\nMean number of offspring in a nonempty cluster: 39.07619\nIntensity of parents of clusters of more than one offspring point: 0.03807945\nRatio of parents to parents-plus-offspring: 0.02495247 (where 1 = Poisson \nprocess)\nProbability that a typical point belongs to a nontrivial cluster: 1\n\n\n\nKthomas.env_west <- envelope(Kthomas_west, rmax = 5, Lest, nsim = 99, rank = 1, global = F)\n\nGenerating 99 simulated realisations of fitted cluster model  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nplot(Kthomas.env_west, . - r ~ r, shade = c(\"hi\", \"lo\"), legend = F, main = \"West: Thomas Model\")\n\n\n\n\n\n\nNorth - Anakena\n\nKthomas_north <- kppm(north_corals, ~ 1, \"Thomas\")\n\n\nKthomas_north\n\nStationary cluster point process model\nFitted to point pattern dataset 'north_corals'\nFitted by minimum contrast\n    Summary statistic: K-function\n\nUniform intensity:  0.544\n\nCluster model: Thomas process\nFitted cluster parameters:\n    kappa     scale \n0.2166116 0.5233174 \nMean cluster size:  2.511408 points\n\nCluster strength: phi =  1.341\nSibling probability: psib =  0.5729\n\n\n\nsummary(Kthomas_north)\n\nStationary cluster point process model\nFitted to point pattern dataset 'north_corals'\nFitted by minimum contrast\n    Summary statistic: K-function\nMinimum contrast fit (object of class \"minconfit\")\nModel: Thomas process\nFitted by matching theoretical K function to north_corals\n\nInternal parameters fitted by minimum contrast ($par):\n    kappa    sigma2 \n0.2166116 0.2738611 \n\nFitted cluster parameters:\n    kappa     scale \n0.2166116 0.5233174 \nMean cluster size:  2.511408 points\n\nConverged successfully after 89 function evaluations\n\nStarting values of parameters:\n   kappa   sigma2 \n0.544000 1.095114 \nDomain of integration: [ 0 , 2.5 ]\nExponents: p= 2, q= 0.25\n\n----------- TREND  -----\nPoint process model\nFitted to data: X\nFitting method: maximum likelihood (Berman-Turner approximation)\nModel was fitted using glm()\nAlgorithm converged\nCall:\nppm.ppp(Q = X, trend = trend, rename.intercept = FALSE, covariates = covariates, \n    covfunargs = covfunargs, use.gam = use.gam, forcefit = TRUE, \n    improve.type = ppm.improve.type, improve.args = ppm.improve.args, \n    nd = nd, eps = eps)\nEdge correction: \"border\"\n    [border correction distance r = 0 ]\n--------------------------------------------------------------------------------\nQuadrature scheme (Berman-Turner) = data + dummy + weights\n\nData pattern:\nPlanar point pattern:  136 points\nAverage intensity 0.544 points per square unit\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\n\nDummy quadrature points:\n     32 x 32 grid of dummy points, plus 4 corner points\n     dummy spacing: 0.31250 x 0.78125 units\n\nOriginal dummy parameters: =\nPlanar point pattern:  1028 points\nAverage intensity 4.11 points per square unit\nWindow: rectangle = [0, 10] x [0, 25] units\nWindow area = 250 square units\nQuadrature weights:\n     (counting weights based on 32 x 32 array of rectangular tiles)\nAll weights:\n    range: [0.0488, 0.244]  total: 250\nWeights on data points:\n    range: [0.0488, 0.122]  total: 14.9\nWeights on dummy points:\n    range: [0.0488, 0.244]  total: 235\n--------------------------------------------------------------------------------\nFITTED :\n\nStationary Poisson process\n\n---- Intensity: ----\n\n\nUniform intensity:\n[1] 0.544\n\n             Estimate       S.E.    CI95.lo    CI95.hi Ztest      Zval\n(Intercept) -0.608806 0.08574929 -0.7768716 -0.4407405   *** -7.099837\n\n----------- gory details -----\n\nFitted regular parameters (theta):\n(Intercept) \n  -0.608806 \n\nFitted exp(theta):\n(Intercept) \n      0.544 \n\n----------- CLUSTER  -----------\nModel: Thomas process\n\nFitted cluster parameters:\n    kappa     scale \n0.2166116 0.5233174 \nMean cluster size:  2.511408 points\n\nFinal standard error and CI\n(allowing for correlation of cluster process):\n             Estimate      S.E.    CI95.lo    CI95.hi Ztest      Zval\n(Intercept) -0.608806 0.1560742 -0.9147059 -0.3029062   *** -3.900747\n\n----------- cluster strength indices ----------\nSibling probability 0.5729162\nCount overdispersion index (on original window): 3.372763\nCluster strength: 1.341461\n\nSpatial persistence index (over window): 0\n\nBound on distance from Poisson process (over window): 1\n     = min (1, 249.9261, 25153.21, 206010.5, 157.5172)\n\nBound on distance from MIXED Poisson process (over window): 1\n\nIntensity of parents of nonempty clusters: 0.1990327\nMean number of offspring in a nonempty cluster: 2.733219\nIntensity of parents of clusters of more than one offspring point: 0.154885\nRatio of parents to parents-plus-offspring: 0.2847861 (where 1 = Poisson \nprocess)\nProbability that a typical point belongs to a nontrivial cluster: 0.9188461\n\n\n\nKthomas.env_north <- envelope(Kthomas_north, rmax = 5, Lest, nsim = 99, rank = 1, global = F)\n\nGenerating 99 simulated realisations of fitted cluster model  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nplot(Kthomas.env_north, . - r ~ r, shade = c(\"hi\", \"lo\"), legend = F, main = \"North: Thomas Model\")"
  },
  {
    "objectID": "content/draft.html",
    "href": "content/draft.html",
    "title": "Manuscript Draft",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "content/soil.html",
    "href": "content/soil.html",
    "title": "Soil",
    "section": "",
    "text": "soil_full <- read.csv('soil_full.csv') %>%\n  as_tibble()\n\n\nsoil_full\n\n# A tibble: 71 × 5\n   material range depth range_mean    mbc\n   <chr>    <chr> <dbl> <chr>       <dbl>\n 1 NDM      0-3     1.5 0-10       13323.\n 2 HAHTM    0-5     2.5 0-10        5251.\n 3 NDM      0-5     2.5 0-10        2253.\n 4 HAHTM    0-6     3   0-10        3274.\n 5 HAHTM    0-6     3   0-10        2881.\n 6 HAHTM    0-9     4.5 0-10        1527.\n 7 NDM      6-Mar   4.5 0-10        7314.\n 8 HAHTM    0-13    6.5 0-10         896.\n 9 HAHTM    0-13    6.5 0-10         581.\n10 NDM      0-15    7.5 0-10        1252.\n# ℹ 61 more rows\n\n\n\nsoil <- read.csv('soil_full.csv') %>%\n  as_tibble() %>%\n  mutate_at(vars(material, range, range_mean), factor) %>%\n  group_by(material, range_mean) %>%\n  dplyr::summarize(mean = mean(mbc), \n                   sd = sd(mbc), \n                   n = n(),\n                   se = sd/sqrt(n)\n  ) %>%\n  mutate(se = sd / sqrt(n),\n         lower.ci = mean - qt(1 - (0.05 / 2), n - 1) * se,\n         upper.ci = mean + qt(1 - (0.05 / 2), n - 1) * se)\n\n`summarise()` has grouped output by 'material'. You can override using the\n`.groups` argument.\n\n\n\nsoil\n\n# A tibble: 12 × 8\n# Groups:   material [2]\n   material range_mean  mean      sd     n       se lower.ci upper.ci\n   <fct>    <fct>      <dbl>   <dbl> <int>    <dbl>    <dbl>    <dbl>\n 1 HAHTM    0-10       2244. 1659.       7  627.      710.      3778.\n 2 HAHTM    10-20       302.  141.      11   42.4     207.       396.\n 3 HAHTM    30-50       242.   94.8     11   28.6     178.       306.\n 4 HAHTM    50-70       188.   52.0      7   19.7     140.       237.\n 5 HAHTM    70-90       242.   55.0      6   22.5     185.       300.\n 6 HAHTM    90+         264.  104.       5   46.4     135.       393.\n 7 NDM      0-10       5193. 4823.       6 1969.      131.     10254.\n 8 NDM      10-20       434.  271.       4  136.        2.34     865.\n 9 NDM      30-50       136.   63.5      3   36.6     -21.4      294.\n10 NDM      50-70       234.    1.06     2    0.749   224.       243.\n11 NDM      70-90       220.   98.8      4   49.4      62.4      377.\n12 NDM      90+         164.   84.5      5   37.8      58.6      268.\n\n\n\nx_labels = c(\"0 - 10 cm\", \"10 - 30 cm\", \"30 - 50 cm\", \"50 - 70 cm\", \"70 - 90 cm\", \"90+ cm\")\n\n\nsoil.barplot <- ggplot(soil, aes(x = range_mean, y = mean, fill = material)) +   \n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8), width = 0.75, color = \"#2b2b2b\", linewidth = 0.50, alpha = 0.6) +\n  geom_linerange(aes(ymin = mean, ymax = mean + se), position = position_dodge(width = 0.8), linewidth = 0.75) +\n  scale_x_discrete(expand = c(0, 1), labels = x_labels) + \n  ggtitle(expression(paste(italic(\" Soil\")))) +\n  ylab(expression(paste(\"mbc\"))) +\n  labs(x = NULL) +\n  theme(strip.text = element_text(size = 10, color = \"black\", hjust = 0.50),\n        strip.background = element_rect(fill = \"#FFFFFF\", color = NA),    \n        panel.background = element_rect(fill = \"#FFFFFF\", color = NA),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_line(color = \"#b2b2b2\"),\n        panel.spacing.x = unit(1, \"cm\"),\n        panel.spacing.y = unit(0.5, \"cm\"),\n        panel.spacing = unit(1, \"lines\"),\n        axis.ticks = element_blank(),\n        legend.position = \"right\",\n        plot.title = element_text(size = 11),\n        axis.title.y = element_text(size = 11),\n        legend.title = element_blank())\n\n\nsoil.barplot\n\n\n\n\n\nSoil Main File - Numbered categories for depth\n\nsoil_main <- read.csv('soil_main.csv') %>%\n  as_tibble() %>%\n  mutate_at(vars(material, depth2), factor) %>%\n  group_by(material, depth2) %>%\n  dplyr::summarize(mean = mean(mbc), \n                   sd = sd(mbc), \n                   n = n(),\n                   se = sd/sqrt(n)\n  ) %>%\n  mutate(se = sd / sqrt(n),\n         lower.ci = mean - qt(1 - (0.05 / 2), n - 1) * se,\n         upper.ci = mean + qt(1 - (0.05 / 2), n - 1) * se)\n\n`summarise()` has grouped output by 'material'. You can override using the\n`.groups` argument.\n\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `lower.ci = mean - qt(1 - (0.05/2), n - 1) * se`.\nℹ In group 2: `material = NDM`.\nCaused by warning in `qt()`:\n! NaNs produced\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n\nsoil_main\n\n# A tibble: 17 × 8\n# Groups:   material [2]\n   material depth2  mean     sd     n     se lower.ci upper.ci\n   <fct>    <fct>  <dbl>  <dbl> <int>  <dbl>    <dbl>    <dbl>\n 1 HAHTM    1      2244. 1659.      7  627.     710.     3778.\n 2 HAHTM    2       371.  144.      7   54.6    237.      504.\n 3 HAHTM    3       246.   65.7     7   24.8    185.      307.\n 4 HAHTM    4       214.   72.3     7   27.3    147.      281.\n 5 HAHTM    5       202.   69.5     7   26.3    138.      266.\n 6 HAHTM    6       259.  105.      6   42.7    149.      368.\n 7 HAHTM    7       212.   79.1     4   39.5     86.0     338.\n 8 HAHTM    8       223.   21.4     2   15.1     31.0     415.\n 9 NDM      1      5609. 6699.      3 3868.  -11032.    22250.\n10 NDM      2      2757. 3953.      3 2282.   -7062.    12576.\n11 NDM      3      2249. 3529.      3 2038.   -6518.    11015.\n12 NDM      4       381.  400.      3  231.    -612.     1374.\n13 NDM      5       268.   57.0     3   32.9    126.      409.\n14 NDM      6       136.  101.      3   58.3   -114.      387.\n15 NDM      7       165.   86.0     3   49.7    -48.9     378.\n16 NDM      8       267.   93.7     2   66.3   -575.     1109.\n17 NDM      9       237.   NA       1   NA      NaN       NaN \n\n\n\nx_labels = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\")\n\n\nsoil_main.barplot <- ggplot(soil_main, aes(x = depth2, y = mean, fill = material)) +   \n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.9), width = 0.75, color = \"#2b2b2b\", linewidth = 0.50, alpha = 0.6) +\n  geom_linerange(aes(ymin = mean, ymax = mean + se), position = position_dodge(width = 0.9), linewidth = 0.75) +\n  scale_x_discrete(expand = c(0, 1), labels = x_labels) + \n  ggtitle(expression(paste(italic(\" Soil\")))) +\n  ylab(expression(paste(\"mbc\"))) +\n  labs(x = NULL) +\n  theme(strip.text = element_text(size = 10, color = \"black\", hjust = 0.50),\n        strip.background = element_rect(fill = \"#FFFFFF\", color = NA),    \n        panel.background = element_rect(fill = \"#FFFFFF\", color = NA),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_line(color = \"#b2b2b2\"),\n        panel.spacing.x = unit(1, \"cm\"),\n        panel.spacing.y = unit(0.5, \"cm\"),\n        panel.spacing = unit(1, \"lines\"),\n        axis.ticks = element_blank(),\n        legend.position = \"right\",\n        plot.title = element_text(size = 11),\n        axis.title.y = element_text(size = 11),\n        legend.title = element_blank())\n\n\nsoil_main.barplot\n\nWarning: Removed 1 rows containing missing values (`geom_segment()`).\n\n\n\n\n\n\nsoil_full\n\n# A tibble: 71 × 5\n   material range depth range_mean    mbc\n   <chr>    <chr> <dbl> <chr>       <dbl>\n 1 NDM      0-3     1.5 0-10       13323.\n 2 HAHTM    0-5     2.5 0-10        5251.\n 3 NDM      0-5     2.5 0-10        2253.\n 4 HAHTM    0-6     3   0-10        3274.\n 5 HAHTM    0-6     3   0-10        2881.\n 6 HAHTM    0-9     4.5 0-10        1527.\n 7 NDM      6-Mar   4.5 0-10        7314.\n 8 HAHTM    0-13    6.5 0-10         896.\n 9 HAHTM    0-13    6.5 0-10         581.\n10 NDM      0-15    7.5 0-10        1252.\n# ℹ 61 more rows\n\n\n\nsoil.lm <- lm(mbc ~ depth * material, data = soil_full)\n\n\npar(mfrow = c(2, 2))\nplot(soil.lm)\n\n\n\n\n\nsummary(soil.lm)\n\n\nCall:\nlm(formula = mbc ~ depth * material, data = soil_full)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2461.8  -555.8  -308.3   316.4  9913.9 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)   \n(Intercept)       1174.254    454.177   2.585   0.0119 * \ndepth              -14.064      8.463  -1.662   0.1012   \nmaterialNDM       2293.428    738.969   3.104   0.0028 **\ndepth:materialNDM  -25.148     12.430  -2.023   0.0470 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1741 on 67 degrees of freedom\nMultiple R-squared:  0.2783,    Adjusted R-squared:  0.246 \nF-statistic: 8.612 on 3 and 67 DF,  p-value: 6.493e-05\n\n\n\nggplot(soil_full, aes(x = depth, y = mbc, color = material)) +\n  geom_point(size = 1.5)+\n  xlab(\"depth\") +\n  ylab(\"mbc\") +\n  ggtitle(\"Soil\") +\n  theme(axis.title.x = element_text(face = \"bold\", size = 14), \n        axis.title.y = element_text(face = \"bold\", size = 14), \n        axis.text.y  = element_text(size = 10),\n        axis.text.x  = element_text(size = 10), \n        legend.text = element_text(size = 10),\n        legend.title = element_text(size = 10, face = \"bold\"),\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16))\n\n\n\n\n\nsoil_log.lm <- lm(log(mbc) ~ depth * material, data = soil_full)\n\n\npar(mfrow = c(2, 2))\nplot(soil_log.lm)\n\n\n\n\n\nsummary(soil_log.lm)\n\n\nCall:\nlm(formula = log(mbc) ~ depth * material, data = soil_full)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8928 -0.5593 -0.0242  0.4972  2.2318 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        6.426412   0.219971  29.215  < 2e-16 ***\ndepth             -0.015063   0.004099  -3.675 0.000475 ***\nmaterialNDM        0.877818   0.357905   2.453 0.016791 *  \ndepth:materialNDM -0.010816   0.006020  -1.797 0.076912 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8434 on 67 degrees of freedom\nMultiple R-squared:  0.4233,    Adjusted R-squared:  0.3975 \nF-statistic: 16.39 on 3 and 67 DF,  p-value: 4.295e-08\n\n\n\nggplot(soil_full, aes(x = depth, y = log(mbc), color = material)) +\n  geom_point(size = 1.5)+\n  xlab(\"depth\") +\n  ylab(\"mbc\") +\n  ggtitle(\"Soil\") +\n  theme(axis.title.x = element_text(face = \"bold\", size = 14), \n        axis.title.y = element_text(face = \"bold\", size = 14), \n        axis.text.y  = element_text(size = 10),\n        axis.text.x  = element_text(size = 10), \n        legend.text = element_text(size = 10),\n        legend.title = element_text(size = 10, face = \"bold\"),\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16))\n\n\n\n\n\nsoil_hyp.lm <- lm(log(1/mbc) ~ exp(depth) * material, data = soil_full)\n\n\npar(mfrow = c(2, 2))\nplot(soil_hyp.lm)\n\nWarning in sqrt(crit * p * (1 - hh)/hh): NaNs produced\n\nWarning in sqrt(crit * p * (1 - hh)/hh): NaNs produced\n\n\n\n\n\n\nsummary(soil_hyp.lm)\n\n\nCall:\nlm(formula = log(1/mbc) ~ exp(depth) * material, data = soil_full)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4295 -0.2424  0.2582  0.6092  1.7314 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            -5.762e+00  1.595e-01 -36.126   <2e-16 ***\nexp(depth)              2.965e-49  1.120e-48   0.265    0.792    \nmaterialNDM            -3.053e-01  2.763e-01  -1.105    0.273    \nexp(depth):materialNDM -2.965e-49  1.120e-48  -0.265    0.792    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.082 on 67 degrees of freedom\nMultiple R-squared:  0.05104,   Adjusted R-squared:  0.00855 \nF-statistic: 1.201 on 3 and 67 DF,  p-value: 0.3161\n\n\n\nggplot(soil_full, aes(log(mbc), fill = material)) + geom_histogram(binwidth = 1.0) + facet_grid(material ~ ., margins = TRUE, scales = \"free\")\n\n\n\n\n\nsummary(m1 <- glm.nb(mbc ~ depth + material, data = soil_full))\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 13322.756000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 5251.113000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 2253.029000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 3273.606000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 2881.409000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 1526.776000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 7314.128000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 895.614000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 581.138000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 1251.818000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 692.213000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 1296.439000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 6322.571000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 196.525000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 435.551000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 619.199000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 838.347000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 442.632000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 262.403000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 299.546000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 261.725000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 143.822000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 299.623000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 263.912000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 333.523000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 221.808000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 212.948000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 224.340000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 352.168000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 377.460000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 231.191000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 204.767000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 137.354000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 308.525000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 206.056000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 79.545000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 135.695000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 142.907000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 330.242000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 124.340000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 300.578000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 140.947000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 120.524000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 234.716000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 249.895000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 206.732000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 249.712000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 145.582000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 233.217000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 200.425000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 146.195000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 207.947000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 99.238000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 252.794000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 193.990000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 214.724000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 211.719000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 306.720000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 318.240000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 192.918000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 333.465000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 435.382000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 152.746000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 234.823000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 246.971000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 246.966000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 200.934000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 237.216000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 76.429000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 238.180000\n\n\nWarning in dpois(y, mu, log = TRUE): non-integer x = 68.242000\n\n\n\nCall:\nglm.nb(formula = mbc ~ depth + material, data = soil_full, init.theta = 1.421043058, \n    link = log)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0704  -1.0577  -0.4726   0.2348   2.8623  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  7.190553   0.180869  39.755  < 2e-16 ***\ndepth       -0.025177   0.002995  -8.406  < 2e-16 ***\nmaterialNDM  0.620237   0.211676   2.930  0.00339 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.421) family taken to be 1)\n\n    Null deviance: 186.096  on 70  degrees of freedom\nResidual deviance:  78.835  on 68  degrees of freedom\nAIC: 1029.7\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.421 \n          Std. Err.:  0.217 \n\n 2 x log-likelihood:  -1021.658 \n\n\n\nwith(soil_full, tapply(mbc, material, function(x) {\n    sprintf(\"M (SD) = %1.2f (%1.2f)\", mean(x), sd(x))\n}))\n\n                       HAHTM                          NDM \n  \"M (SD) = 548.56 (939.01)\" \"M (SD) = 1477.69 (3143.08)\""
  },
  {
    "objectID": "content/customizing.html",
    "href": "content/customizing.html",
    "title": "Customization",
    "section": "",
    "text": "Quarto allow many bells and whistles to make nice output. Read the documentation here Quarto documentation."
  },
  {
    "objectID": "content/customizing.html#examples",
    "href": "content/customizing.html#examples",
    "title": "Customization",
    "section": "Examples",
    "text": "Examples\nLooking at other people’s Quarto code is a great way to figure out how to do stuff. Most will have a link to a GitHub repo where you can see the raw code. Look for a link to edit page or see source code. This will usually be on the right. Or look for the GitHub icon somewhere.\n\nQuarto gallery\nnmfs-openscapes\nFaye lab manual\nquarto-titlepages Note the link to edit is broken. Go to repo and look in documentation directory."
  },
  {
    "objectID": "content/coral_size.html",
    "href": "content/coral_size.html",
    "title": "Coral Size",
    "section": "",
    "text": "poci_size <- read.csv('coral_size.csv')\n\n\npoci_size.gg <- read_csv(\"poci_size_main.csv\")\n\nRows: 3639 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): level, Site, cover\ndbl (7): layer, class, id, area, enn, para, size_cm\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nggplot(poci_size.gg, aes(x = size_cm)) +\n  geom_histogram(fill = \"#333399\") + \n  #below here is ylabel, xlabel, and main title\n  ylab(\"Frequency\") +\n  xlab(NULL) +\n  ggtitle(expression(\"Coral size \" (cm**2))) +\n  theme_bw() +\n  facet_wrap(~ Site, ncol = 1) +\n  #theme sets sizes, text, etc\n  theme(axis.title.x = element_text(size = 14), \n        axis.title.y = element_text(size = 14), \n        axis.text.y  = element_text(size= 10),\n        axis.text.x  = element_text(size = 12), \n        legend.text = element_text(size = 12),\n        legend.title = element_text(size = 12),\n        plot.title = element_text(hjust = 0.5, size = 14),\n        # change plot background, grid lines, etc (just examples so you can see)\n        panel.background = element_rect(fill = \"white\"),\n        panel.grid.minor.y = element_blank(),\n        panel.grid.major = element_blank(),\n        plot.background = element_rect(fill = \"white\"),\n        legend.background = element_rect(fill = \"white\"),\n        strip.text.x = element_text(size = 12, colour = \"#FFFFFF\"),\n        strip.background = element_rect(fill = '#000066')\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\npoci_size2 <-\npoci_size %>%\n  as_tibble() %>%\n  mutate(size_cm = area*10000) %>%\n  group_by(Site) %>%\n  dplyr::summarize(mean = mean(size_cm), \n                   sd = sd(size_cm), \n                   n = n(),\n                   se = sd/sqrt(n)\n  ) %>%\n  mutate(se = sd / sqrt(n),\n         lower.ci = mean - qt(1 - (0.05 / 2), n - 1) * se,\n         upper.ci = mean + qt(1 - (0.05 / 2), n - 1) * se) %>%\n  mutate_at(vars(Site), factor) %>%\n  add_column(\n          location = c('Anakena', 'Manavai', 'Southeast')\n          ) %>%\n  mutate_at(vars(location), factor)\n\n\npoci_size.gg <-\n  poci_size %>%\n  mutate(size_cm = area*10000) %>%\n  as_tibble() %>%\n  mutate_at(vars(Site), factor)\n\n\nmodel_1.lm <- lm(size_cm ~ Site, data = poci_size.gg)\n\n\nmodel_1.lm\n\n\nCall:\nlm(formula = size_cm ~ Site, data = poci_size.gg)\n\nCoefficients:\n(Intercept)      Siteman      Sitevhu  \n     110.67        40.18       120.81  \n\n\n\nsummary(model_1.lm)\n\n\nCall:\nlm(formula = size_cm ~ Site, data = poci_size.gg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-231.46  -91.09  -16.44   70.33 1160.47 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   110.67      11.18   9.896  < 2e-16 ***\nSiteman        40.18      13.07   3.074  0.00212 ** \nSitevhu       120.81      11.42  10.575  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 130.4 on 3636 degrees of freedom\nMultiple R-squared:  0.05804,   Adjusted R-squared:  0.05752 \nF-statistic:   112 on 2 and 3636 DF,  p-value: < 2.2e-16\n\n\n\npar(mfrow = c(2, 2))\nplot(model_1.lm)\n\n\n\n\n\nAnova(model_1.lm, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: size_cm\n              Sum Sq   Df F value    Pr(>F)    \n(Intercept)  1665764    1  97.933 < 2.2e-16 ***\nSite         3810826    2 112.022 < 2.2e-16 ***\nResiduals   61845547 3636                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\npost_hoc.model_1.lm <- glht(model_1.lm, linfct = mcp(Site = 'Tukey'))\n\n\nmodel_1.aov <- aov(size_cm ~ Site, data = poci_size.gg)\n\n\n# Tukey's test\ntukey <- TukeyHSD(model_1.aov)\n\n\n# compact letter display\ncld <- multcompLetters4(model_1.aov, tukey)\n\n\ncld\n\n$Site\nvhu man ana \n\"a\" \"b\" \"c\" \n\n\n\npoci_size3 <- \npoci_size2 %>%\n  add_column(\n          cld = c('a', 'b', 'c')\n          ) %>%\nmutate_at(vars(cld), factor)\n\n\npoci_size3\n\n# A tibble: 3 × 9\n  Site   mean    sd     n    se lower.ci upper.ci location  cld  \n  <fct> <dbl> <dbl> <int> <dbl>    <dbl>    <dbl> <fct>     <fct>\n1 ana    111.  73.8   136  6.33     98.2     123. Anakena   a    \n2 man    151.  82.7   372  4.29    142.      159. Manavai   b    \n3 vhu    231. 137.   3131  2.44    227.      236. Southeast c    \n\n\n\nx_labels = c(\"North\", \"West\", \"Southeast\")\n# label_names = c(\"8 m\" = \"8 m\", \"15 m\" = \"15 m\", \"25 m\" = \"25 m\")\n\n\npoci_size.gg.barplot <- ggplot(poci_size3, aes(x = location, y = mean, fill = x_labels)) +   \n  geom_bar(stat = \"identity\", width = 0.75, color = \"black\", size = 0.50, alpha = 0.6) +\n  geom_linerange(aes(ymin = lower.ci, ymax = upper.ci), size = 0.75) +\n  scale_y_continuous(expression(paste(\"Mean Colony Size (\",\" \", cm^2, \")\")), limits = c(0, 300)) + \n  scale_x_discrete(expand = c(0, 1), labels = x_labels) + \n  scale_fill_manual(values = c(\"#FFC74E\", \"#82A5C0\", \"#ABC178\")) +\n# facet_wrap( ~ depth2, labeller = as_labeller(label_names), dir = \"v\", ncol = 1) + \n# ggtitle(expression(paste(italic(\" Pocillopora \"), \"spp.\"))) +\n  geom_text(aes(label = cld, y = upper.ci), vjust = -0.5) +\n  #scale_y_log10(expression(paste(\"Colony Size (\", cm^2, \")\"), limits = c(0, 100000))) +\n  labs(x = NULL) +\n  theme(strip.text = element_text(size = 10, color = \"black\", hjust = 0.50),\n        strip.background = element_rect(fill = \"#FFFFFF\", color = NA),    \n        panel.background = element_rect(fill = \"#FFFFFF\", color = NA),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_line(color = \"#b2b2b2\"),\n        panel.spacing.x = unit(1, \"cm\"),\n        panel.spacing.y = unit(0.5, \"cm\"),\n        panel.spacing = unit(1, \"lines\"),\n        axis.ticks = element_blank(),\n        legend.position = \"none\",\n        plot.title = element_text(size = 11),\n        axis.title.y = element_text(size = 11),\n        legend.title = element_blank())\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\npoci_size.gg.barplot"
  },
  {
    "objectID": "content/bleaching.html",
    "href": "content/bleaching.html",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "",
    "text": "rpn_bleach <- read.csv('rpn_bleach_2015.csv') %>%\n  as_tibble() %>%\n  mutate_at(vars(location, depth, transect, group, status), factor)"
  },
  {
    "objectID": "content/bleaching.html#porites",
    "href": "content/bleaching.html#porites",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "Porites",
    "text": "Porites\n\nGeneralized linear model\n\nplob.glm <- glm(cbind(total_count, failures) ~ location * depth, \n                    family = binomial(link = \"logit\"), \n                    data = rpn_plob)\n\n\npar(mfrow = c(2, 2))\nplot(plob.glm)\n\n\n\n\n\nsummary(plob.glm)\n\n\nCall:\nglm(formula = cbind(total_count, failures) ~ location * depth, \n    family = binomial(link = \"logit\"), data = rpn_plob)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.54165  -1.20892  -0.00388   1.28236   2.71923  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)            1.8245     0.1822  10.017  < 2e-16 ***\nlocationse            -2.8154     0.2352 -11.973  < 2e-16 ***\nlocationwest          -1.2368     0.2246  -5.506 3.68e-08 ***\ndepthsh               -1.0773     0.2267  -4.753 2.00e-06 ***\nlocationse:depthsh     0.6719     0.3138   2.142   0.0322 *  \nlocationwest:depthsh   1.3105     0.2956   4.434 9.25e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 396.020  on 11  degrees of freedom\nResidual deviance:  31.694  on  6  degrees of freedom\nAIC: 102.95\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nhttps://www.rpubs.com/daharo_calpoly/502695\n\nrpn_plob$groups <- interaction(rpn_plob$location, rpn_plob$depth)"
  },
  {
    "objectID": "content/bleaching.html#create-a-post-hoc-model",
    "href": "content/bleaching.html#create-a-post-hoc-model",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "Create a post-hoc model",
    "text": "Create a post-hoc model\n\nmodel_plob <- with(rpn_plob, glm(cbind(total_count, failures) ~ groups, family = binomial))"
  },
  {
    "objectID": "content/bleaching.html#determine-the-post-hoc-comparisons-of-interest",
    "href": "content/bleaching.html#determine-the-post-hoc-comparisons-of-interest",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "Determine the post-hoc comparisons of interest",
    "text": "Determine the post-hoc comparisons of interest\n\nsummary(glht(model_plob, \n             linfct = mcp(groups =\n                     #Is the difference between these groups different from zero?\n                     c(\"(north.sh) - (west.sh) = 0\",\n                       \"(north.sh) - (west.dp) = 0\",\n                       \"(north.sh) - (se.sh) = 0\",\n                       \"(north.sh) - (se.dp) = 0\",\n                       \"(north.dp) - (west.sh) = 0\",\n                       \"(north.dp) - (west.dp) = 0\",\n                       \"(north.dp) - (se.sh) = 0\",\n                       \"(north.dp) - (se.dp) = 0\",\n                       \"(se.sh) - (west.sh) = 0\",\n                       \"(se.sh) - (west.dp) = 0\",\n                       \"(se.dp) - (west.sh) = 0\",\n                       \"(se.dp) - (west.dp) = 0\"))),\n        test = adjusted(\"holm\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: User-defined Contrasts\n\n\nFit: glm(formula = cbind(total_count, failures) ~ groups, family = binomial)\n\nLinear Hypotheses:\n                            Estimate Std. Error z value Pr(>|z|)    \n(north.sh) - (west.sh) == 0 -0.07377    0.19208  -0.384    0.795    \n(north.sh) - (west.dp) == 0  0.15943    0.18835   0.846    0.795    \n(north.sh) - (se.sh) == 0    2.14346    0.20771  10.319  < 2e-16 ***\n(north.sh) - (se.dp) == 0    1.73807    0.20078   8.657  < 2e-16 ***\n(north.dp) - (west.sh) == 0  1.00357    0.22777   4.406 3.16e-05 ***\n(north.dp) - (west.dp) == 0  1.23676    0.22464   5.506 1.47e-07 ***\n(north.dp) - (se.sh) == 0    3.22079    0.24110  13.359  < 2e-16 ***\n(north.dp) - (se.dp) == 0    2.81541    0.23515  11.973  < 2e-16 ***\n(se.sh) - (west.sh) == 0    -2.21723    0.20893 -10.612  < 2e-16 ***\n(se.sh) - (west.dp) == 0    -1.98403    0.20551  -9.654  < 2e-16 ***\n(se.dp) - (west.sh) == 0    -1.81184    0.20204  -8.968  < 2e-16 ***\n(se.dp) - (west.dp) == 0    -1.57865    0.19850  -7.953 8.88e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\n\nrpn_plob_bleach <- rpn_bleach %>%  \n  filter(group == 'PLOB' & status == 'BL') %>%\n  group_by(location, depth, transect) %>%\n  dplyr::summarise(total_count = n())\n\n`summarise()` has grouped output by 'location', 'depth'. You can override using\nthe `.groups` argument.\n\n\n\nrpn_plob_bleach\n\n# A tibble: 0 × 4\n# Groups:   location, depth [0]\n# ℹ 4 variables: location <fct>, depth <fct>, transect <fct>, total_count <int>\n\n\n\nrpn_plob_pb <- rpn_bleach %>%  \n  filter(group == 'PLOB' & status == 'PB') %>%\n  group_by(location, depth, transect) %>%\n  dplyr::summarise(total_count = n())\n\n`summarise()` has grouped output by 'location', 'depth'. You can override using\nthe `.groups` argument.\n\n\n\nrpn_plob_pb\n\n# A tibble: 9 × 4\n# Groups:   location, depth [5]\n  location depth transect total_count\n  <fct>    <fct> <fct>          <int>\n1 north    dp    one                5\n2 north    dp    two               49\n3 north    sh    one               40\n4 north    sh    two               31\n5 se       dp    two                2\n6 west     dp    one               70\n7 west     dp    two               55\n8 west     sh    one               64\n9 west     sh    two               46\n\n\n\nrpn_plob_pale <- rpn_bleach %>%  \n  filter(group == 'PLOB' & status == 'P') %>%\n  group_by(location, depth, transect) %>%\n  dplyr::summarise(total_count = n())\n\n`summarise()` has grouped output by 'location', 'depth'. You can override using\nthe `.groups` argument.\n\n\n\nrpn_plob_pale\n\n# A tibble: 12 × 4\n# Groups:   location, depth [6]\n   location depth transect total_count\n   <fct>    <fct> <fct>          <int>\n 1 north    dp    one               87\n 2 north    dp    two               52\n 3 north    sh    one               28\n 4 north    sh    two               37\n 5 se       dp    one               23\n 6 se       dp    two                2\n 7 se       sh    one               29\n 8 se       sh    two                9\n 9 west     dp    one               17\n10 west     dp    two               16\n11 west     sh    one                5\n12 west     sh    two               55\n\n\n\nrpn_plob_healthy <- rpn_bleach %>%  \n  filter(group == 'PLOB' & status == 'H') %>%\n  group_by(location, depth, transect) %>%\n  dplyr::summarise(total_count = n())\n\n`summarise()` has grouped output by 'location', 'depth'. You can override using\nthe `.groups` argument.\n\n\n\nrpn_plob_healthy\n\n# A tibble: 10 × 4\n# Groups:   location, depth [6]\n   location depth transect total_count\n   <fct>    <fct> <fct>          <int>\n 1 north    dp    one               12\n 2 north    dp    two               12\n 3 north    sh    one               15\n 4 north    sh    two               20\n 5 se       dp    one               11\n 6 se       sh    one                6\n 7 se       sh    two                6\n 8 west     dp    one                2\n 9 west     dp    two                2\n10 west     sh    one                5"
  },
  {
    "objectID": "content/bleaching.html#pocillopora",
    "href": "content/bleaching.html#pocillopora",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "Pocillopora",
    "text": "Pocillopora\n\nGeneralized linear model\n\nrpn_poci\n\n# A tibble: 12 × 8\n   location depth transect group total_count total_points   cover failures\n   <fct>    <fct> <fct>    <fct>       <int>        <int>   <dbl>    <int>\n 1 north    dp    one      POCI            7          126 0.0556       119\n 2 north    dp    two      POCI            2          126 0.0159       124\n 3 north    sh    one      POCI            0          126 0            126\n 4 north    sh    two      POCI            1          126 0.00794      125\n 5 se       dp    one      POCI           57          124 0.460         67\n 6 se       dp    two      POCI           58          105 0.552         47\n 7 se       sh    one      POCI           75          126 0.595         51\n 8 se       sh    two      POCI           60          126 0.476         66\n 9 west     dp    one      POCI           13          126 0.103        113\n10 west     dp    two      POCI           14          126 0.111        112\n11 west     sh    one      POCI           15          126 0.119        111\n12 west     sh    two      POCI           18          126 0.143        108\n\n\n\npoci.glm <- glm(cbind(total_count, failures) ~ location * depth, \n                family = binomial(link = \"logit\"), \n                data = rpn_poci)\n\n\npar(mfrow = c(2, 2))\nplot(poci.glm)\n\n\n\n\n\nsummary(poci.glm)\n\n\nCall:\nglm(formula = cbind(total_count, failures) ~ location * depth, \n    family = binomial(link = \"logit\"), data = rpn_poci)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.34438  -0.96059  -0.00078   0.72479   1.34520  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)           -3.2958     0.3395  -9.709  < 2e-16 ***\nlocationse             3.3046     0.3643   9.072  < 2e-16 ***\nlocationwest           1.1756     0.3959   2.970  0.00298 ** \ndepthsh               -2.2296     1.0579  -2.108  0.03507 *  \nlocationse:depthsh     2.3640     1.0736   2.202  0.02767 *  \nlocationwest:depthsh   2.4573     1.0934   2.247  0.02462 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 436.578  on 11  degrees of freedom\nResidual deviance:  10.347  on  6  degrees of freedom\nAIC: 69.293\n\nNumber of Fisher Scoring iterations: 5\n\n\nAnova function from the car package\n\nAnova(poci.glm, type = \"III\") # Type III because...\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: cbind(total_count, failures)\n               LR Chisq Df Pr(>Chisq)    \nlocation        178.902  2  < 2.2e-16 ***\ndepth             7.491  1   0.006201 ** \nlocation:depth    8.513  2   0.014171 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nhttps://www.rpubs.com/daharo_calpoly/502695\n\nrpn_poci$groups <- interaction(rpn_poci$location, rpn_poci$depth)"
  },
  {
    "objectID": "content/bleaching.html#create-a-post-hoc-model-1",
    "href": "content/bleaching.html#create-a-post-hoc-model-1",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "Create a post-hoc model",
    "text": "Create a post-hoc model\n\nmodel_poci <- with(rpn_poci, glm(cbind(total_count, failures) ~ groups, family = binomial))"
  },
  {
    "objectID": "content/bleaching.html#determine-the-post-hoc-comparisons-of-interest-1",
    "href": "content/bleaching.html#determine-the-post-hoc-comparisons-of-interest-1",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "Determine the post-hoc comparisons of interest",
    "text": "Determine the post-hoc comparisons of interest\n\nsummary(glht(model_poci, \n             linfct = mcp(groups =\n                     #Is the difference between these groups different from zero?\n                     c(\"(north.sh) - (west.sh) = 0\",\n                       \"(north.sh) - (west.dp) = 0\",\n                       \"(north.sh) - (se.sh) = 0\",\n                       \"(north.sh) - (se.dp) = 0\",\n                       \"(north.dp) - (west.sh) = 0\",\n                       \"(north.dp) - (west.dp) = 0\",\n                       \"(north.dp) - (se.sh) = 0\",\n                       \"(north.dp) - (se.dp) = 0\",\n                       \"(se.sh) - (west.sh) = 0\",\n                       \"(se.sh) - (west.dp) = 0\",\n                       \"(se.dp) - (west.sh) = 0\",\n                       \"(se.dp) - (west.dp) = 0\"))),\n        test = adjusted(\"holm\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: User-defined Contrasts\n\n\nFit: glm(formula = cbind(total_count, failures) ~ groups, family = binomial)\n\nLinear Hypotheses:\n                            Estimate Std. Error z value Pr(>|z|)    \n(north.sh) - (west.sh) == 0  -3.6329     1.0192  -3.564  0.00117 ** \n(north.sh) - (west.dp) == 0  -3.4052     1.0225  -3.330  0.00173 ** \n(north.sh) - (se.sh) == 0    -5.6686     1.0099  -5.613 1.19e-07 ***\n(north.sh) - (se.dp) == 0    -5.5342     1.0107  -5.476 2.18e-07 ***\n(north.dp) - (west.sh) == 0  -1.4033     0.3874  -3.622  0.00117 ** \n(north.dp) - (west.dp) == 0  -1.1756     0.3959  -2.970  0.00298 ** \n(north.dp) - (se.sh) == 0    -3.4389     0.3622  -9.495  < 2e-16 ***\n(north.dp) - (se.dp) == 0    -3.3046     0.3643  -9.072  < 2e-16 ***\n(se.sh) - (west.sh) == 0      2.0357     0.2254   9.030  < 2e-16 ***\n(se.sh) - (west.dp) == 0      2.2634     0.2397   9.444  < 2e-16 ***\n(se.dp) - (west.sh) == 0      1.9013     0.2288   8.311  < 2e-16 ***\n(se.dp) - (west.dp) == 0      2.1290     0.2428   8.769  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\n\nrpn_poci_bleach <- rpn_bleach %>%  \n  filter(group == 'POCI' & status == 'BL') %>%\n  group_by(location, depth, transect) %>%\n  dplyr::summarise(total_count = n())\n\n`summarise()` has grouped output by 'location', 'depth'. You can override using\nthe `.groups` argument.\n\n\n\nrpn_poci_bleach\n\n# A tibble: 6 × 4\n# Groups:   location, depth [5]\n  location depth transect total_count\n  <fct>    <fct> <fct>          <int>\n1 north    dp    one                1\n2 se       dp    one                1\n3 se       dp    two                2\n4 se       sh    two                1\n5 west     dp    one                4\n6 west     sh    two                2\n\n\n\nrpn_poci_pb <- rpn_bleach %>%  \n  filter(group == 'POCI' & status == 'PB') %>%\n  group_by(location, depth, transect) %>%\n  dplyr::summarise(total_count = n())\n\n`summarise()` has grouped output by 'location', 'depth'. You can override using\nthe `.groups` argument.\n\n\n\nrpn_poci_pb\n\n# A tibble: 8 × 4\n# Groups:   location, depth [5]\n  location depth transect total_count\n  <fct>    <fct> <fct>          <int>\n1 north    dp    one                2\n2 north    dp    two                1\n3 se       dp    one               27\n4 se       dp    two                9\n5 se       sh    one                2\n6 se       sh    two                5\n7 west     dp    two                2\n8 west     sh    one                1\n\n\n\nrpn_poci_pale <- rpn_bleach %>%  \n  filter(group == 'POCI' & status == 'P') %>%\n  group_by(location, depth, transect) %>%\n  dplyr::summarise(total_count = n())\n\n`summarise()` has grouped output by 'location', 'depth'. You can override using\nthe `.groups` argument.\n\n\n\nrpn_poci_pale\n\n# A tibble: 10 × 4\n# Groups:   location, depth [5]\n   location depth transect total_count\n   <fct>    <fct> <fct>          <int>\n 1 north    dp    one                3\n 2 north    dp    two                1\n 3 se       dp    one               19\n 4 se       dp    two               16\n 5 se       sh    one               59\n 6 se       sh    two               45\n 7 west     dp    one                7\n 8 west     dp    two                8\n 9 west     sh    one               13\n10 west     sh    two                9\n\n\n\nrpn_poci_healthy <- rpn_bleach %>%  \n  filter(group == 'POCI' & status == 'H') %>%\n  group_by(location, depth, transect) %>%\n  dplyr::summarise(total_count = n())\n\n`summarise()` has grouped output by 'location', 'depth'. You can override using\nthe `.groups` argument.\n\n\n\nrpn_poci_healthy\n\n# A tibble: 9 × 4\n# Groups:   location, depth [6]\n  location depth transect total_count\n  <fct>    <fct> <fct>          <int>\n1 north    dp    one                1\n2 north    sh    two                1\n3 se       dp    one               10\n4 se       sh    one               14\n5 se       sh    two                9\n6 west     dp    one                2\n7 west     dp    two                4\n8 west     sh    one                1\n9 west     sh    two                7"
  },
  {
    "objectID": "content/bleaching.html#examine-bleaching-response-of-coral-groups",
    "href": "content/bleaching.html#examine-bleaching-response-of-coral-groups",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "Examine bleaching response of coral groups",
    "text": "Examine bleaching response of coral groups\n\nPorites\n\nrpn_bleach <- read.csv('rpn_coral_bleach.csv') %>%\n  as_tibble() %>%\n  #filter(group == \"PLOB\") %>%\n  mutate_at(vars(location, depth, transect, species), factor) %>%\n  mutate(\n  bl = coral_count - bl_count,\n  pb = coral_count - pb_count,\n  p  = coral_count - p_count,\n  h  = coral_count - h_count,\n    )\n\n\nrpn_bleach\n\n# A tibble: 24 × 13\n   species location depth transect bl_count pb_count p_count h_count coral_count\n   <fct>   <fct>    <fct> <fct>       <int>    <int>   <int>   <int>       <int>\n 1 plob    north    dp    one             0        5      87      12         104\n 2 plob    north    dp    two             0       49      52      12         113\n 3 plob    north    sh    one             0       40      28      15          83\n 4 plob    north    sh    two             0       31      37      20          88\n 5 plob    se       dp    one             0        0      23      11          34\n 6 plob    se       dp    two             0        2       2       0          28\n 7 plob    se       sh    one             0        0      29       6          35\n 8 plob    se       sh    two             0        0       9       6          15\n 9 plob    west     dp    one             0       70      17       2          89\n10 plob    west     dp    two             0       55      16       2          73\n# ℹ 14 more rows\n# ℹ 4 more variables: bl <int>, pb <int>, p <int>, h <int>\n\n\n\nplob_bleach <- rpn_bleach %>%\n  filter(species == \"plob\")\n\n\nplob_bleach\n\n# A tibble: 12 × 13\n   species location depth transect bl_count pb_count p_count h_count coral_count\n   <fct>   <fct>    <fct> <fct>       <int>    <int>   <int>   <int>       <int>\n 1 plob    north    dp    one             0        5      87      12         104\n 2 plob    north    dp    two             0       49      52      12         113\n 3 plob    north    sh    one             0       40      28      15          83\n 4 plob    north    sh    two             0       31      37      20          88\n 5 plob    se       dp    one             0        0      23      11          34\n 6 plob    se       dp    two             0        2       2       0          28\n 7 plob    se       sh    one             0        0      29       6          35\n 8 plob    se       sh    two             0        0       9       6          15\n 9 plob    west     dp    one             0       70      17       2          89\n10 plob    west     dp    two             0       55      16       2          73\n11 plob    west     sh    one             0       64       5       0          71\n12 plob    west     sh    two             0       46      55       5         101\n# ℹ 4 more variables: bl <int>, pb <int>, p <int>, h <int>\n\n\n\nplob_pb.glm <- glm(cbind(pb_count, pb) ~ location * depth, \n                family = binomial(link = \"logit\"), \n                data = plob_bleach)\n\n\npar(mfrow = c(2, 2))\nplot(plob_pb.glm)\n\n\n\n\n\nsummary(plob_pb.glm)\n\n\nCall:\nglm(formula = cbind(pb_count, pb) ~ location * depth, family = binomial(link = \"logit\"), \n    data = plob_bleach)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-5.5192  -1.2798  -0.0003   1.0681   5.0776  \n\nCoefficients:\n                      Estimate Std. Error z value Pr(>|z|)    \n(Intercept)            -1.1048     0.1570  -7.036 1.98e-12 ***\nlocationse             -2.2964     0.7357  -3.121 0.001801 ** \nlocationwest            2.3222     0.2443   9.506  < 2e-16 ***\ndepthsh                 0.7623     0.2208   3.453 0.000555 ***\nlocationse:depthsh    -17.2555  1792.1996  -0.010 0.992318    \nlocationwest:depthsh   -1.4063     0.3301  -4.260 2.04e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 344.520  on 11  degrees of freedom\nResidual deviance:  95.151  on  6  degrees of freedom\nAIC: 145.78\n\nNumber of Fisher Scoring iterations: 16\n\n\nAnova function from the car package\n\nAnova(plob_pb.glm, type = \"III\") # Type III because...\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: cbind(pb_count, pb)\n               LR Chisq Df Pr(>Chisq)    \nlocation        161.859  2  < 2.2e-16 ***\ndepth            12.092  1  0.0005065 ***\nlocation:depth   21.115  2    2.6e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nplob_pb.aov <- Anova(plob_pb.glm, type = \"III\") # Type III because...\n\n\nhttps://www.rpubs.com/daharo_calpoly/502695\n\nplob_bleach$groups <- interaction(plob_bleach$location, plob_bleach$depth)"
  },
  {
    "objectID": "content/bleaching.html#create-a-post-hoc-model-2",
    "href": "content/bleaching.html#create-a-post-hoc-model-2",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "Create a post-hoc model",
    "text": "Create a post-hoc model\n\nmodel_plob_bleach <- with(plob_bleach, glm(cbind(pb_count, pb) ~ groups, family = binomial))"
  },
  {
    "objectID": "content/bleaching.html#determine-the-post-hoc-comparisons-of-interest-2",
    "href": "content/bleaching.html#determine-the-post-hoc-comparisons-of-interest-2",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "Determine the post-hoc comparisons of interest",
    "text": "Determine the post-hoc comparisons of interest\n\nsummary(glht(model_plob_bleach, \n             linfct = mcp(groups =\n                     #Is the difference between these groups different from zero?\n                     c(\"(north.sh) - (west.sh) = 0\",\n                       \"(north.sh) - (west.dp) = 0\",\n                       \"(north.sh) - (se.sh) = 0\",\n                       \"(north.sh) - (se.dp) = 0\",\n                       \"(north.dp) - (west.sh) = 0\",\n                       \"(north.dp) - (west.dp) = 0\",\n                       \"(north.dp) - (se.sh) = 0\",\n                       \"(north.dp) - (se.dp) = 0\",\n                       \"(se.sh) - (west.sh) = 0\",\n                       \"(se.sh) - (west.dp) = 0\",\n                       \"(se.dp) - (west.sh) = 0\",\n                       \"(se.dp) - (west.dp) = 0\"))),\n        test = adjusted(\"holm\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: User-defined Contrasts\n\n\nFit: glm(formula = cbind(pb_count, pb) ~ groups, family = binomial)\n\nLinear Hypotheses:\n                             Estimate Std. Error z value Pr(>|z|)    \n(north.sh) - (west.sh) == 0   -0.9158     0.2220  -4.125 0.000223 ***\n(north.sh) - (west.dp) == 0   -1.5599     0.2431  -6.416 1.40e-09 ***\n(north.sh) - (se.sh) == 0     19.5519  1792.1994   0.011 1.000000    \n(north.sh) - (se.dp) == 0      3.0587     0.7354   4.159 0.000223 ***\n(north.dp) - (west.sh) == 0   -1.6781     0.2233  -7.514 6.30e-13 ***\n(north.dp) - (west.dp) == 0   -2.3222     0.2443  -9.506  < 2e-16 ***\n(north.dp) - (se.sh) == 0     18.7897  1792.1994   0.010 1.000000    \n(north.dp) - (se.dp) == 0      2.2964     0.7357   3.121 0.009005 ** \n(se.sh) - (west.sh) == 0     -20.4678  1792.1994  -0.011 1.000000    \n(se.sh) - (west.dp) == 0     -21.1118  1792.1994  -0.012 1.000000    \n(se.dp) - (west.sh) == 0      -3.9745     0.7361  -5.399 5.35e-07 ***\n(se.dp) - (west.dp) == 0      -4.6186     0.7428  -6.218 4.53e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\nPLOB pale\n\nplob_pale.glm <- glm(cbind(p_count, p) ~ location * depth, \n                family = binomial(link = \"logit\"), \n                data = plob_bleach)\n\n\npar(mfrow = c(2, 2))\nplot(plob_pale.glm)\n\n\n\n\n\nsummary(plob_pale.glm)\n\n\nCall:\nglm(formula = cbind(p_count, p) ~ location * depth, family = binomial(link = \"logit\"), \n    data = plob_bleach)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-5.5666  -2.0023   0.0128   1.5432   4.4335  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)            0.5778     0.1415   4.084 4.43e-05 ***\nlocationse            -0.9698     0.2950  -3.287  0.00101 ** \nlocationwest          -1.9411     0.2410  -8.055 7.95e-16 ***\ndepthsh               -1.0668     0.2117  -5.038 4.70e-07 ***\nlocationse:depthsh     2.6115     0.4706   5.549 2.88e-08 ***\nlocationwest:depthsh   1.8060     0.3294   5.483 4.18e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 217.65  on 11  degrees of freedom\nResidual deviance: 112.66  on  6  degrees of freedom\nAIC: 174.36\n\nNumber of Fisher Scoring iterations: 4\n\n\nAnova function from the car package\n\nAnova(plob_pale.glm, type = \"III\") # Type III because...\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: cbind(p_count, p)\n               LR Chisq Df Pr(>Chisq)    \nlocation         75.499  2  < 2.2e-16 ***\ndepth            26.276  1  2.959e-07 ***\nlocation:depth   49.739  2  1.583e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nplob_pale.aov <- Anova(plob_pale.glm, type = \"III\") # Type III because...\n\n\nhttps://www.rpubs.com/daharo_calpoly/502695\n\nplob_bleach$groups <- interaction(plob_bleach$location, plob_bleach$depth)"
  },
  {
    "objectID": "content/bleaching.html#create-a-post-hoc-model-3",
    "href": "content/bleaching.html#create-a-post-hoc-model-3",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "Create a post-hoc model",
    "text": "Create a post-hoc model\n\nmodel_plob_pale <- with(plob_bleach, glm(cbind(p_count, p) ~ groups, family = binomial))"
  },
  {
    "objectID": "content/bleaching.html#determine-the-post-hoc-comparisons-of-interest-3",
    "href": "content/bleaching.html#determine-the-post-hoc-comparisons-of-interest-3",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "Determine the post-hoc comparisons of interest",
    "text": "Determine the post-hoc comparisons of interest\n\nsummary(glht(model_plob_pale, \n             linfct = mcp(groups =\n                     #Is the difference between these groups different from zero?\n                     c(\"(north.sh) - (west.sh) = 0\",\n                       \"(north.sh) - (west.dp) = 0\",\n                       \"(north.sh) - (se.sh) = 0\",\n                       \"(north.sh) - (se.dp) = 0\",\n                       \"(north.dp) - (west.sh) = 0\",\n                       \"(north.dp) - (west.dp) = 0\",\n                       \"(north.dp) - (se.sh) = 0\",\n                       \"(north.dp) - (se.dp) = 0\",\n                       \"(se.sh) - (west.sh) = 0\",\n                       \"(se.sh) - (west.dp) = 0\",\n                       \"(se.dp) - (west.sh) = 0\",\n                       \"(se.dp) - (west.dp) = 0\"))),\n        test = adjusted(\"holm\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: User-defined Contrasts\n\n\nFit: glm(formula = cbind(p_count, p) ~ groups, family = binomial)\n\nLinear Hypotheses:\n                            Estimate Std. Error z value Pr(>|z|)    \n(north.sh) - (west.sh) == 0  0.13510    0.22453   0.602  1.00000    \n(north.sh) - (west.dp) == 0  0.87425    0.25075   3.487  0.00342 ** \n(north.sh) - (se.sh) == 0   -1.64173    0.36670  -4.477 6.05e-05 ***\n(north.sh) - (se.dp) == 0   -0.09701    0.30306  -0.320  1.00000    \n(north.dp) - (west.sh) == 0  1.20192    0.21356   5.628 1.82e-07 ***\n(north.dp) - (west.dp) == 0  1.94107    0.24098   8.055 1.07e-14 ***\n(north.dp) - (se.sh) == 0   -0.57491    0.36009  -1.597  0.44143    \n(north.dp) - (se.dp) == 0    0.96981    0.29503   3.287  0.00607 ** \n(se.sh) - (west.sh) == 0     1.77683    0.36776   4.832 1.22e-05 ***\n(se.sh) - (west.dp) == 0     2.51598    0.38432   6.547 6.48e-10 ***\n(se.dp) - (west.sh) == 0     0.23211    0.30434   0.763  1.00000    \n(se.dp) - (west.dp) == 0     0.97126    0.32416   2.996  0.01367 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\n\nPLOB healthy\nInteraction\n\nplob_h.glm <- glm(cbind(h_count, h) ~ location * depth, \n                family = binomial(link = \"logit\"), \n                data = plob_bleach)\n\n\npar(mfrow = c(2, 2))\nplot(plob_h.glm)\n\n\n\n\n\nsummary(plob_h.glm)\n\n\nCall:\nglm(formula = cbind(h_count, h) ~ location * depth, family = binomial(link = \"logit\"), \n    data = plob_bleach)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.3072  -0.6585   0.0047   0.6672   2.0546  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)           -2.0846     0.2164  -9.631  < 2e-16 ***\nlocationse             0.5507     0.3967   1.388  0.16506    \nlocationwest          -1.5917     0.5506  -2.891  0.00384 ** \ndepthsh                0.7273     0.2877   2.528  0.01147 *  \nlocationse:depthsh    -0.3461     0.5504  -0.629  0.52949    \nlocationwest:depthsh  -0.5596     0.7383  -0.758  0.44849    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 78.898  on 11  degrees of freedom\nResidual deviance: 24.092  on  6  degrees of freedom\nAIC: 72.567\n\nNumber of Fisher Scoring iterations: 5\n\n\nAnova function from the car package\n\nAnova(plob_h.glm, type = \"III\") # Type III because...\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: cbind(h_count, h)\n               LR Chisq Df Pr(>Chisq)    \nlocation        17.2263  2  0.0001817 ***\ndepth            6.5218  1  0.0106558 *  \nlocation:depth   0.8028  2  0.6693953    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nplob_pale.aov <- Anova(plob_h.glm, type = \"III\") # Type III because...\n\nNo interaction\n\nplob_h.glm2 <- glm(cbind(h_count, h) ~ location + depth, \n                family = binomial(link = \"logit\"), \n                data = plob_bleach)\n\n\npar(mfrow = c(2, 2))\nplot(plob_h.glm2)\n\n\n\n\n\nsummary(plob_h.glm2)\n\n\nCall:\nglm(formula = cbind(h_count, h) ~ location + depth, family = binomial(link = \"logit\"), \n    data = plob_bleach)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-3.15440  -0.62092   0.05079   0.77454   2.30294  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -2.0042     0.1892 -10.592  < 2e-16 ***\nlocationse     0.3666     0.2751   1.332   0.1827    \nlocationwest  -1.9215     0.3678  -5.225 1.74e-07 ***\ndepthsh        0.5819     0.2308   2.521   0.0117 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 78.898  on 11  degrees of freedom\nResidual deviance: 24.895  on  8  degrees of freedom\nAIC: 69.37\n\nNumber of Fisher Scoring iterations: 5\n\n\nAnova function from the car package\n\nAnova(plob_h.glm2, type = \"III\") # Type III because...\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: cbind(h_count, h)\n         LR Chisq Df Pr(>Chisq)    \nlocation   49.890  2  1.467e-11 ***\ndepth       6.441  1    0.01115 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "content/bleaching.html#determine-the-post-hoc-comparisons-of-interest-4",
    "href": "content/bleaching.html#determine-the-post-hoc-comparisons-of-interest-4",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "Determine the post-hoc comparisons of interest",
    "text": "Determine the post-hoc comparisons of interest\n\nsummary(glht(plob_h.glm2, linfct = mcp(location = \"Tukey\")))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: glm(formula = cbind(h_count, h) ~ location + depth, family = binomial(link = \"logit\"), \n    data = plob_bleach)\n\nLinear Hypotheses:\n                  Estimate Std. Error z value Pr(>|z|)    \nse - north == 0     0.3666     0.2751   1.332     0.37    \nwest - north == 0  -1.9215     0.3678  -5.225   <1e-04 ***\nwest - se == 0     -2.2880     0.4131  -5.539   <1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\n\nPocillopora\n\npoci_bleach <- rpn_bleach %>%\n  filter(species == \"poci\")\n\n\npoci_bleach\n\n# A tibble: 12 × 13\n   species location depth transect bl_count pb_count p_count h_count coral_count\n   <fct>   <fct>    <fct> <fct>       <int>    <int>   <int>   <int>       <int>\n 1 poci    north    dp    one             1        2       3       1           7\n 2 poci    north    dp    two             0        1       1       0           2\n 3 poci    north    sh    one             0        0       0       0           0\n 4 poci    north    sh    two             0        0       0       1           1\n 5 poci    se       dp    one             1       27      19      10          57\n 6 poci    se       dp    two             2        9      16       0          58\n 7 poci    se       sh    one             0        2      59      14          75\n 8 poci    se       sh    two             1        5      45       9          60\n 9 poci    west     dp    one             4        0       7       2          13\n10 poci    west     dp    two             0        2       8       4          14\n11 poci    west     sh    one             0        1      13       1          15\n12 poci    west     sh    two             2        0       9       7          18\n# ℹ 4 more variables: bl <int>, pb <int>, p <int>, h <int>\n\n\n\nPOCI Partially Bleached\nInteraction\n\npoci_pb.glm <- glm(cbind(pb_count, pb) ~ location * depth, \n                family = binomial(link = \"logit\"), \n                data = poci_bleach)\n\n\npar(mfrow = c(2, 2))\nplot(poci_pb.glm)\n\nWarning: not plotting observations with leverage one:\n  3\n\n\n\n\n\n\nsummary(poci_pb.glm)\n\n\nCall:\nglm(formula = cbind(pb_count, pb) ~ location * depth, family = binomial(link = \"logit\"), \n    data = poci_bleach)\n\nDeviance Residuals: \n       1         2         3         4         5         6         7         8  \n-0.27079   0.48535   0.00000  -0.00022   2.52677  -2.76404  -1.08056   1.01500  \n       9        10        11        12  \n-1.41456   0.88009   0.71184  -1.05251  \n\nCoefficients:\n                       Estimate Std. Error z value Pr(>|z|)  \n(Intercept)            -0.69315    0.70711  -0.980   0.3270  \nlocationse             -0.09278    0.73514  -0.126   0.8996  \nlocationwest           -1.83258    1.01980  -1.797   0.0723 .\ndepthsh               -16.87292 3956.18039  -0.004   0.9966  \nlocationse:depthsh     14.75273 3956.18041   0.004   0.9970  \nlocationwest:depthsh   15.93291 3956.18059   0.004   0.9968  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 62.211  on 10  degrees of freedom\nResidual deviance: 20.921  on  5  degrees of freedom\nAIC: 55.354\n\nNumber of Fisher Scoring iterations: 16\n\n\nAnova function from the car package\n\nAnova(poci_pb.glm, type = \"III\") # Type III because...\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: cbind(pb_count, pb)\n               LR Chisq Df Pr(>Chisq)  \nlocation         7.9356  2    0.01892 *\ndepth            0.7600  1    0.38332  \nlocation:depth   0.8257  2    0.66178  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNo interaction\n\npoci_pb.glm <- glm(cbind(pb_count, pb) ~ location + depth, \n                family = binomial(link = \"logit\"), \n                data = poci_bleach)\n\n\npar(mfrow = c(2, 2))\nplot(poci_pb.glm)\n\n\n\n\n\nsummary(poci_pb.glm)\n\n\nCall:\nglm(formula = cbind(pb_count, pb) ~ location + depth, family = binomial(link = \"logit\"), \n    data = poci_bleach)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.6979  -0.8276  -0.1169   0.6541   2.5992  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -0.72330    0.70092  -1.032   0.3021    \nlocationse   -0.08263    0.72752  -0.114   0.9096    \nlocationwest -1.53602    0.92597  -1.659   0.0972 .  \ndepthsh      -2.02830    0.41105  -4.934 8.04e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 62.211  on 10  degrees of freedom\nResidual deviance: 21.747  on  7  degrees of freedom\nAIC: 52.18\n\nNumber of Fisher Scoring iterations: 5\n\n\nAnova function from the car package\n\nAnova(poci_pb.glm, type = \"III\") # Type III because...\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: cbind(pb_count, pb)\n         LR Chisq Df Pr(>Chisq)    \nlocation    7.508  2    0.02342 *  \ndepth      32.047  1  1.505e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "content/bleaching.html#determine-the-post-hoc-comparisons-of-interest-5",
    "href": "content/bleaching.html#determine-the-post-hoc-comparisons-of-interest-5",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "Determine the post-hoc comparisons of interest",
    "text": "Determine the post-hoc comparisons of interest\n\nsummary(glht(poci_pb.glm, linfct = mcp(location = \"Tukey\")))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: glm(formula = cbind(pb_count, pb) ~ location + depth, family = binomial(link = \"logit\"), \n    data = poci_bleach)\n\nLinear Hypotheses:\n                  Estimate Std. Error z value Pr(>|z|)  \nse - north == 0   -0.08263    0.72752  -0.114   0.9926  \nwest - north == 0 -1.53602    0.92597  -1.659   0.2126  \nwest - se == 0    -1.45339    0.62897  -2.311   0.0515 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\n\nsummary(glht(poci_pb.glm, linfct = mcp(depth = \"Tukey\")))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: glm(formula = cbind(pb_count, pb) ~ location + depth, family = binomial(link = \"logit\"), \n    data = poci_bleach)\n\nLinear Hypotheses:\n             Estimate Std. Error z value Pr(>|z|)    \nsh - dp == 0  -2.0283     0.4111  -4.934 8.04e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\n\nPOCI Pale\n\npoci_pale.glm <- glm(cbind(p_count, p) ~ location * depth, \n                family = binomial(link = \"logit\"), \n                data = poci_bleach)\n\n\npar(mfrow = c(2, 2))\nplot(poci_pale.glm)\n\nWarning: not plotting observations with leverage one:\n  3\n\n\n\n\n\n\nsummary(poci_pale.glm)\n\n\nCall:\nglm(formula = cbind(p_count, p) ~ location * depth, family = binomial(link = \"logit\"), \n    data = poci_bleach)\n\nDeviance Residuals: \n       1         2         3         4         5         6         7         8  \n-0.08462   0.15762   0.00000  -0.00013   0.47159  -0.47587   0.33844  -0.37143  \n       9        10        11        12  \n-0.12389   0.11968   1.77660  -1.45605  \n\nCoefficients:\n                      Estimate Std. Error z value Pr(>|z|)\n(Intercept)            -0.2231     0.6708  -0.333    0.739\nlocationse             -0.6035     0.7008  -0.861    0.389\nlocationwest            0.4463     0.7746   0.576    0.565\ndepthsh               -18.3429  6522.6386  -0.003    0.998\nlocationse:depthsh     20.3800  6522.6386   0.003    0.998\nlocationwest:depthsh   18.8129  6522.6386   0.003    0.998\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 66.3499  on 10  degrees of freedom\nResidual deviance:  6.0394  on  5  degrees of freedom\nAIC: 51.212\n\nNumber of Fisher Scoring iterations: 17\n\n\nAnova function from the car package\n\nAnova(poci_pale.glm, type = \"III\") # Type III because...\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: cbind(p_count, p)\n               LR Chisq Df Pr(>Chisq)   \nlocation         6.1173  2   0.046951 * \ndepth            1.0949  1   0.295383   \nlocation:depth   9.5663  2   0.008369 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\npoci_pale.aov <- Anova(poci_pale.glm, type = \"III\") # Type III because...\n\n\n\nhttps://www.rpubs.com/daharo_calpoly/502695\n\npoci_bleach$groups <- interaction(poci_bleach$location, poci_bleach$depth)"
  },
  {
    "objectID": "content/bleaching.html#create-a-post-hoc-model-4",
    "href": "content/bleaching.html#create-a-post-hoc-model-4",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "Create a post-hoc model",
    "text": "Create a post-hoc model\n\nmodel_poci_pale <- with(poci_bleach, glm(cbind(p_count, p) ~ groups, family = binomial))"
  },
  {
    "objectID": "content/bleaching.html#determine-the-post-hoc-comparisons-of-interest-6",
    "href": "content/bleaching.html#determine-the-post-hoc-comparisons-of-interest-6",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "Determine the post-hoc comparisons of interest",
    "text": "Determine the post-hoc comparisons of interest\n\nsummary(glht(model_poci_pale, \n             linfct = mcp(groups =\n                     #Is the difference between these groups different from zero?\n                     c(\"(north.sh) - (west.sh) = 0\",\n                       \"(north.sh) - (west.dp) = 0\",\n                       \"(north.sh) - (se.sh) = 0\",\n                       \"(north.sh) - (se.dp) = 0\",\n                       \"(north.dp) - (west.sh) = 0\",\n                       \"(north.dp) - (west.dp) = 0\",\n                       \"(north.dp) - (se.sh) = 0\",\n                       \"(north.dp) - (se.dp) = 0\",\n                       \"(se.sh) - (west.sh) = 0\",\n                       \"(se.sh) - (west.dp) = 0\",\n                       \"(se.dp) - (west.sh) = 0\",\n                       \"(se.dp) - (west.dp) = 0\"))),\n        test = adjusted(\"holm\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: User-defined Contrasts\n\n\nFit: glm(formula = cbind(p_count, p) ~ groups, family = binomial)\n\nLinear Hypotheses:\n                             Estimate Std. Error z value Pr(>|z|)   \n(north.sh) - (west.sh) == 0  -19.2592  6522.6386  -0.003   1.0000   \n(north.sh) - (west.dp) == 0  -18.7892  6522.6386  -0.003   1.0000   \n(north.sh) - (se.sh) == 0    -19.7765  6522.6386  -0.003   1.0000   \n(north.sh) - (se.dp) == 0    -17.7394  6522.6386  -0.003   1.0000   \n(north.dp) - (west.sh) == 0   -0.9163     0.7657  -1.197   1.0000   \n(north.dp) - (west.dp) == 0   -0.4463     0.7746  -0.576   1.0000   \n(north.dp) - (se.sh) == 0     -1.4335     0.7013  -2.044   0.3686   \n(north.dp) - (se.dp) == 0      0.6035     0.7008   0.861   1.0000   \n(se.sh) - (west.sh) == 0       0.5173     0.4222   1.225   1.0000   \n(se.sh) - (west.dp) == 0       0.9873     0.4380   2.254   0.2421   \n(se.dp) - (west.sh) == 0      -1.5198     0.4212  -3.608   0.0037 **\n(se.dp) - (west.dp) == 0      -1.0498     0.4371  -2.402   0.1795   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\n\nPOCI Healthy\nInteraction\n\npoci_h.glm <- glm(cbind(h_count, h) ~ location * depth, \n                family = binomial(link = \"logit\"), \n                data = poci_bleach)\n\n\npar(mfrow = c(2, 2))\nplot(poci_h.glm)\n\nWarning: not plotting observations with leverage one:\n  3\n\n\n\n\n\n\nsummary(poci_h.glm)\n\n\nCall:\nglm(formula = cbind(h_count, h) ~ location * depth, family = binomial(link = \"logit\"), \n    data = poci_bleach)\n\nDeviance Residuals: \n      1        2        3        4        5        6        7        8  \n 0.2573  -0.6864   0.0000   0.0002   2.1109  -3.2485   0.3708  -0.4267  \n      9       10       11       12  \n-0.6187   0.5544  -1.8055   1.3747  \n\nCoefficients:\n                      Estimate Std. Error z value Pr(>|z|)  \n(Intercept)            -2.0794     1.0607  -1.961   0.0499 *\nlocationse             -0.2719     1.1111  -0.245   0.8067  \nlocationwest            0.8267     1.1573   0.714   0.4750  \ndepthsh                19.6455  3956.1805   0.005   0.9960  \nlocationse:depthsh    -18.8771  3956.1805  -0.005   0.9962  \nlocationwest:depthsh  -19.5322  3956.1805  -0.005   0.9961  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 33.048  on 10  degrees of freedom\nResidual deviance: 21.706  on  5  degrees of freedom\nAIC: 58.33\n\nNumber of Fisher Scoring iterations: 16\n\n\nAnova function from the car package\n\nAnova(poci_h.glm, type = \"III\") # Type III because...\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: cbind(h_count, h)\n               LR Chisq Df Pr(>Chisq)  \nlocation         3.4341  2    0.17960  \ndepth            3.7291  1    0.05347 .\nlocation:depth   3.5875  2    0.16634  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\npoci_pale.aov <- Anova(poci_h.glm, type = \"III\") # Type III because...\n\nNo interaction\n\npoci_h.glm2 <- glm(cbind(h_count, h) ~ location + depth, \n                family = binomial(link = \"logit\"), \n                data = poci_bleach)\n\n\npar(mfrow = c(2, 2))\nplot(poci_h.glm2)\n\n\n\n\n\nsummary(poci_h.glm2)\n\n\nCall:\nglm(formula = cbind(h_count, h) ~ location + depth, family = binomial(link = \"logit\"), \n    data = poci_bleach)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.3652  -0.4691  -0.0824   0.9802   1.9326  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)  \n(Intercept)   -1.4636     0.7951  -1.841   0.0657 .\nlocationse    -0.8138     0.8342  -0.976   0.3293  \nlocationwest  -0.1162     0.8678  -0.134   0.8935  \ndepthsh        0.6574     0.3333   1.972   0.0486 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 33.048  on 10  degrees of freedom\nResidual deviance: 25.293  on  7  degrees of freedom\nAIC: 57.918\n\nNumber of Fisher Scoring iterations: 5\n\n\nAnova function from the car package\n\nAnova(poci_h.glm2, type = \"III\") # Type III because...\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: cbind(h_count, h)\n         LR Chisq Df Pr(>Chisq)  \nlocation   4.0681  2    0.13080  \ndepth      4.0623  1    0.04385 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "content/bleaching.html#determine-the-post-hoc-comparisons-of-interest-7",
    "href": "content/bleaching.html#determine-the-post-hoc-comparisons-of-interest-7",
    "title": "Percent Cover and Bleaching in 2015",
    "section": "Determine the post-hoc comparisons of interest",
    "text": "Determine the post-hoc comparisons of interest\n\nsummary(glht(poci_h.glm2, linfct = mcp(location = \"Tukey\")))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: glm(formula = cbind(h_count, h) ~ location + depth, family = binomial(link = \"logit\"), \n    data = poci_bleach)\n\nLinear Hypotheses:\n                  Estimate Std. Error z value Pr(>|z|)\nse - north == 0    -0.8138     0.8342  -0.976    0.576\nwest - north == 0  -0.1162     0.8678  -0.134    0.990\nwest - se == 0      0.6976     0.3609   1.933    0.119\n(Adjusted p values reported -- single-step method)\n\n\n\nsummary(glht(poci_h.glm2, linfct = mcp(depth = \"Tukey\")))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: glm(formula = cbind(h_count, h) ~ location + depth, family = binomial(link = \"logit\"), \n    data = poci_bleach)\n\nLinear Hypotheses:\n             Estimate Std. Error z value Pr(>|z|)  \nsh - dp == 0   0.6574     0.3333   1.972   0.0486 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)"
  },
  {
    "objectID": "content/add-content.html",
    "href": "content/add-content.html",
    "title": "Customize",
    "section": "",
    "text": "Edit the qmd or md files in the content folder. qmd files can include code (R, Python, Julia) and lots of Quarto markdown bells and whistles (like call-outs, cross-references, auto-citations and much more).\nEach page should start with\n---\ntitle: your title\n---\nand the first header will be the 2nd level, so ##. Note, there are situations where you leave off\n---\ntitle: your title\n---\nand start the qmd file with a level header #, but if using the default title yaml (in the --- fence) is a good habit since it makes it easy for Quarto convert your qmd file to other formats (like into a presentation)."
  },
  {
    "objectID": "content/add-content.html#add-your-pages-the-project",
    "href": "content/add-content.html#add-your-pages-the-project",
    "title": "Customize",
    "section": "Add your pages the project",
    "text": "Add your pages the project\n\nAdd the files to _quarto.yml"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NOAA quarto simple with R",
    "section": "",
    "text": "This is a template for a simple Quarto website that looks like a “book”. This is a common format for documentation. It includes a GitHub Action that will build the website automatically when you make changes to the files. The NOAA palette and fonts has been added to theme.scss. The webpage will be on the gh-pages branch. Serving the website files from this branch is a common way to keep all the website files from cluttering your main branch.\nThe GitHub Action installs R so you can have R code in your qmd or Rmd files. Note, you do not need to make changes to your Rmd files unless your need Quarto features like cross-references."
  },
  {
    "objectID": "index.html#github-set-up",
    "href": "index.html#github-set-up",
    "title": "NOAA quarto simple with R",
    "section": "GitHub Set-up",
    "text": "GitHub Set-up\n\nClick the green “use template” button to make a repository with this content. Make sure to make your repo public (since GitHub Pages doesn’t work on private repos unless you have a paid account) and check box to include all the branches (so that you get the gh-pages branch). \nTurn on GitHub Pages under Settings > Pages . You will set pages to be made from the gh-pages branch and root directory. \nTurn on GitHub Actions under Settings > Actions > General \nEdit the repo description and Readme to add a link to the webpage. When you edit the description, you will see the link url in the url box or you can click on the Actions tab or the Settings > Pages page to find the url."
  },
  {
    "objectID": "soil2.html",
    "href": "soil2.html",
    "title": "Soil",
    "section": "",
    "text": "soil_full <- read.csv('soil_full.csv') %>%\n  as_tibble()\n\n\nsoil_full\n\n# A tibble: 71 × 5\n   material range depth range_mean    mbc\n   <chr>    <chr> <dbl> <chr>       <dbl>\n 1 NDM      0-3     1.5 0-10       13323.\n 2 HAHTM    0-5     2.5 0-10        5251.\n 3 NDM      0-5     2.5 0-10        2253.\n 4 HAHTM    0-6     3   0-10        3274.\n 5 HAHTM    0-6     3   0-10        2881.\n 6 HAHTM    0-9     4.5 0-10        1527.\n 7 NDM      3-6     4.5 0-10        7314.\n 8 HAHTM    0-13    6.5 0-10         896.\n 9 HAHTM    0-13    6.5 0-10         581.\n10 NDM      0-15    7.5 0-10        1252.\n# ℹ 61 more rows\n\n\n\nsoil <- read.csv('soil_full.csv') %>%\n  as_tibble() %>%\n  mutate_at(vars(material, range, range_mean), factor) %>%\n  group_by(material, range_mean) %>%\n  dplyr::summarize(mean = mean(mbc), \n                   sd = sd(mbc), \n                   n = n(),\n                   se = sd/sqrt(n)\n  ) %>%\n  mutate(se = sd / sqrt(n),\n         lower.ci = mean - qt(1 - (0.05 / 2), n - 1) * se,\n         upper.ci = mean + qt(1 - (0.05 / 2), n - 1) * se)\n\n`summarise()` has grouped output by 'material'. You can override using the\n`.groups` argument.\n\n\n\nsoil\n\n# A tibble: 12 × 8\n# Groups:   material [2]\n   material range_mean  mean      sd     n       se lower.ci upper.ci\n   <fct>    <fct>      <dbl>   <dbl> <int>    <dbl>    <dbl>    <dbl>\n 1 HAHTM    0-10       2244. 1659.       7  627.      710.      3778.\n 2 HAHTM    10-30       302.  141.      11   42.4     207.       396.\n 3 HAHTM    30-50       242.   94.8     11   28.6     178.       306.\n 4 HAHTM    50-70       188.   52.0      7   19.7     140.       237.\n 5 HAHTM    70-90       242.   55.0      6   22.5     185.       300.\n 6 HAHTM    90+         264.  104.       5   46.4     135.       393.\n 7 NDM      0-10       5193. 4823.       6 1969.      131.     10254.\n 8 NDM      10-30       434.  271.       4  136.        2.34     865.\n 9 NDM      30-50       136.   63.5      3   36.6     -21.4      294.\n10 NDM      50-70       234.    1.06     2    0.749   224.       243.\n11 NDM      70-90       220.   98.8      4   49.4      62.4      377.\n12 NDM      90+         164.   84.5      5   37.8      58.6      268.\n\n\n\nx_labels = c(\"0 - 10 cm\", \"10 - 30 cm\", \"30 - 50 cm\", \"50 - 70 cm\", \"70 - 90 cm\", \"90+ cm\")\n\n\nsoil.barplot <- ggplot(soil, aes(x = range_mean, y = mean, fill = material)) +   \n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8), width = 0.75, color = \"#2b2b2b\", linewidth = 0.50, alpha = 0.6) +\n  geom_linerange(aes(ymin = mean, ymax = mean + se), position = position_dodge(width = 0.8), linewidth = 0.75) +\n  scale_x_discrete(expand = c(0, 1), labels = x_labels) + \n  ggtitle(expression(paste(italic(\" Soil\")))) +\n  ylab(expression(paste(\"mbc\"))) +\n  labs(x = NULL) +\n  theme(strip.text = element_text(size = 10, color = \"black\", hjust = 0.50),\n        strip.background = element_rect(fill = \"#FFFFFF\", color = NA),    \n        panel.background = element_rect(fill = \"#FFFFFF\", color = NA),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_line(color = \"#b2b2b2\"),\n        panel.spacing.x = unit(1, \"cm\"),\n        panel.spacing.y = unit(0.5, \"cm\"),\n        panel.spacing = unit(1, \"lines\"),\n        axis.ticks = element_blank(),\n        legend.position = \"right\",\n        plot.title = element_text(size = 11),\n        axis.title.y = element_text(size = 11),\n        legend.title = element_blank())\n\n\nsoil.barplot\n\n\n\n\n\nsoil_full <- read.csv('soil_full.csv') %>%\n  as_tibble() \n#%>%\n#  group_by(ParentMaterials, Depth) %>%\n#  mutate_at(vars(ParentMaterials, Depth), factor)\n\n\nsoil_full\n\n# A tibble: 71 × 5\n   material range depth range_mean    mbc\n   <chr>    <chr> <dbl> <chr>       <dbl>\n 1 NDM      0-3     1.5 0-10       13323.\n 2 HAHTM    0-5     2.5 0-10        5251.\n 3 NDM      0-5     2.5 0-10        2253.\n 4 HAHTM    0-6     3   0-10        3274.\n 5 HAHTM    0-6     3   0-10        2881.\n 6 HAHTM    0-9     4.5 0-10        1527.\n 7 NDM      3-6     4.5 0-10        7314.\n 8 HAHTM    0-13    6.5 0-10         896.\n 9 HAHTM    0-13    6.5 0-10         581.\n10 NDM      0-15    7.5 0-10        1252.\n# ℹ 61 more rows\n\n\n\nsoil.lm <- lm(mbc ~ depth * material, data = soil_full)\n\n\npar(mfrow = c(2, 2))\nplot(soil.lm)\n\n\n\n\n\nsummary(soil.lm)\n\n\nCall:\nlm(formula = mbc ~ depth * material, data = soil_full)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2461.8  -555.8  -308.3   316.4  9913.9 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)   \n(Intercept)       1174.254    454.177   2.585   0.0119 * \ndepth              -14.064      8.463  -1.662   0.1012   \nmaterialNDM       2293.428    738.969   3.104   0.0028 **\ndepth:materialNDM  -25.148     12.430  -2.023   0.0470 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1741 on 67 degrees of freedom\nMultiple R-squared:  0.2783,    Adjusted R-squared:  0.246 \nF-statistic: 8.612 on 3 and 67 DF,  p-value: 6.493e-05\n\n\n\nggplot(soil_full, aes(x = depth, y = mbc, color = material)) +\n  geom_point(size = 1.5)+\n  xlab(\"depth\") +\n  ylab(\"mbc\") +\n  ggtitle(\"Soil\") +\n  theme(axis.title.x = element_text(face = \"bold\", size = 14), \n        axis.title.y = element_text(face = \"bold\", size = 14), \n        axis.text.y  = element_text(size = 10),\n        axis.text.x  = element_text(size = 10), \n        legend.text = element_text(size = 10),\n        legend.title = element_text(size = 10, face = \"bold\"),\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16))"
  },
  {
    "objectID": "soil.html",
    "href": "soil.html",
    "title": "Soil",
    "section": "",
    "text": "soil_full <- read.csv('soil_full.csv') %>%\n  as_tibble()\n\n\nsoil_full\n\n# A tibble: 71 × 5\n   material range depth range_mean    mbc\n   <chr>    <chr> <dbl> <chr>       <dbl>\n 1 NDM      0-3     1.5 0-10       13323.\n 2 HAHTM    0-5     2.5 0-10        5251.\n 3 NDM      0-5     2.5 0-10        2253.\n 4 HAHTM    0-6     3   0-10        3274.\n 5 HAHTM    0-6     3   0-10        2881.\n 6 HAHTM    0-9     4.5 0-10        1527.\n 7 NDM      3-6     4.5 0-10        7314.\n 8 HAHTM    0-13    6.5 0-10         896.\n 9 HAHTM    0-13    6.5 0-10         581.\n10 NDM      0-15    7.5 0-10        1252.\n# ℹ 61 more rows\n\n\n\nsoil <- read.csv('soil_full.csv') %>%\n  as_tibble() %>%\n  mutate_at(vars(material, range, range_mean), factor) %>%\n  group_by(material, range_mean) %>%\n  dplyr::summarize(mean = mean(mbc), \n                   sd = sd(mbc), \n                   n = n(),\n                   se = sd/sqrt(n)\n  ) %>%\n  mutate(se = sd / sqrt(n),\n         lower.ci = mean - qt(1 - (0.05 / 2), n - 1) * se,\n         upper.ci = mean + qt(1 - (0.05 / 2), n - 1) * se)\n\n`summarise()` has grouped output by 'material'. You can override using the\n`.groups` argument.\n\n\n\nsoil\n\n# A tibble: 12 × 8\n# Groups:   material [2]\n   material range_mean  mean      sd     n       se lower.ci upper.ci\n   <fct>    <fct>      <dbl>   <dbl> <int>    <dbl>    <dbl>    <dbl>\n 1 HAHTM    0-10       2244. 1659.       7  627.      710.      3778.\n 2 HAHTM    10-30       302.  141.      11   42.4     207.       396.\n 3 HAHTM    30-50       242.   94.8     11   28.6     178.       306.\n 4 HAHTM    50-70       188.   52.0      7   19.7     140.       237.\n 5 HAHTM    70-90       242.   55.0      6   22.5     185.       300.\n 6 HAHTM    90+         264.  104.       5   46.4     135.       393.\n 7 NDM      0-10       5193. 4823.       6 1969.      131.     10254.\n 8 NDM      10-30       434.  271.       4  136.        2.34     865.\n 9 NDM      30-50       136.   63.5      3   36.6     -21.4      294.\n10 NDM      50-70       234.    1.06     2    0.749   224.       243.\n11 NDM      70-90       220.   98.8      4   49.4      62.4      377.\n12 NDM      90+         164.   84.5      5   37.8      58.6      268.\n\n\n\nx_labels = c(\"0 - 10 cm\", \"10 - 30 cm\", \"30 - 50 cm\", \"50 - 70 cm\", \"70 - 90 cm\", \"90+ cm\")\n\n\nsoil.barplot <- ggplot(soil, aes(x = range_mean, y = mean, fill = material)) +   \n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8), width = 0.75, color = \"#2b2b2b\", linewidth = 0.50, alpha = 0.6) +\n  geom_linerange(aes(ymin = mean, ymax = mean + se), position = position_dodge(width = 0.8), linewidth = 0.75) +\n  scale_x_discrete(expand = c(0, 1), labels = x_labels) + \n  ggtitle(expression(paste(italic(\" Soil\")))) +\n  ylab(expression(paste(\"mbc\"))) +\n  labs(x = NULL) +\n  theme(strip.text = element_text(size = 10, color = \"black\", hjust = 0.50),\n        strip.background = element_rect(fill = \"#FFFFFF\", color = NA),    \n        panel.background = element_rect(fill = \"#FFFFFF\", color = NA),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_line(color = \"#b2b2b2\"),\n        panel.spacing.x = unit(1, \"cm\"),\n        panel.spacing.y = unit(0.5, \"cm\"),\n        panel.spacing = unit(1, \"lines\"),\n        axis.ticks = element_blank(),\n        legend.position = \"right\",\n        plot.title = element_text(size = 11),\n        axis.title.y = element_text(size = 11),\n        legend.title = element_blank())\n\n\nsoil.barplot\n\n\n\n\n\nsoil_full\n\n# A tibble: 71 × 5\n   material range depth range_mean    mbc\n   <chr>    <chr> <dbl> <chr>       <dbl>\n 1 NDM      0-3     1.5 0-10       13323.\n 2 HAHTM    0-5     2.5 0-10        5251.\n 3 NDM      0-5     2.5 0-10        2253.\n 4 HAHTM    0-6     3   0-10        3274.\n 5 HAHTM    0-6     3   0-10        2881.\n 6 HAHTM    0-9     4.5 0-10        1527.\n 7 NDM      3-6     4.5 0-10        7314.\n 8 HAHTM    0-13    6.5 0-10         896.\n 9 HAHTM    0-13    6.5 0-10         581.\n10 NDM      0-15    7.5 0-10        1252.\n# ℹ 61 more rows\n\n\n\nsoil.lm <- lm(mbc ~ depth * material, data = soil_full)\n\n\npar(mfrow = c(2, 2))\nplot(soil.lm)\n\n\n\n\n\nsummary(soil.lm)\n\n\nCall:\nlm(formula = mbc ~ depth * material, data = soil_full)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2461.8  -555.8  -308.3   316.4  9913.9 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)   \n(Intercept)       1174.254    454.177   2.585   0.0119 * \ndepth              -14.064      8.463  -1.662   0.1012   \nmaterialNDM       2293.428    738.969   3.104   0.0028 **\ndepth:materialNDM  -25.148     12.430  -2.023   0.0470 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1741 on 67 degrees of freedom\nMultiple R-squared:  0.2783,    Adjusted R-squared:  0.246 \nF-statistic: 8.612 on 3 and 67 DF,  p-value: 6.493e-05\n\n\n\nggplot(soil_full, aes(x = depth, y = mbc, color = material)) +\n  geom_point(size = 1.5)+\n  xlab(\"depth\") +\n  ylab(\"mbc\") +\n  ggtitle(\"Soil\") +\n  theme(axis.title.x = element_text(face = \"bold\", size = 14), \n        axis.title.y = element_text(face = \"bold\", size = 14), \n        axis.text.y  = element_text(size = 10),\n        axis.text.x  = element_text(size = 10), \n        legend.text = element_text(size = 10),\n        legend.title = element_text(size = 10, face = \"bold\"),\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16))"
  },
  {
    "objectID": "fuck_this.html",
    "href": "fuck_this.html",
    "title": "SPPA",
    "section": "",
    "text": "vhu_csv <- read.csv(\"vhu_centroids.csv\")\n\n\nvhu_csv2 <-as_tibble(vhu_csv)\n\n\nvhu_csv2 \n\n# A tibble: 3,131 × 6\n   layer level class    id       x     y\n   <int> <chr> <int> <int>   <dbl> <dbl>\n 1     1 patch     0     1 0.0775   24.7\n 2     1 patch     0     2 0.0588   24.0\n 3     1 patch     0     3 0.0368   23.7\n 4     1 patch     0     4 0.0318   23.4\n 5     1 patch     0     5 0.0501   23.1\n 6     1 patch     0     6 0.0125   22.8\n 7     1 patch     0     7 0.0794   21.7\n 8     1 patch     0     8 0.00269  21.0\n 9     1 patch     0     9 0.0485   20.8\n10     1 patch     0    10 0.0824   16.7\n# ℹ 3,121 more rows\n\n\nDefine window for spatial pattern analyses based on the extent defined above\n\nvhu_window <- owin(c(0, 10), c(0, 25))\n\n\nvhu_window\n\nwindow: rectangle = [0, 10] x [0, 25] units"
  }
]